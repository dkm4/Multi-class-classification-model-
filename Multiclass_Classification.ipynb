{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dkm4/Multi-class-classification-model-/blob/main/Multiclass_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# imports"
      ],
      "metadata": {
        "id": "SXA2TGir6FAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8J9ur7kj6ISD",
        "outputId": "a382f6db-670a-4d33-e458-7e618662ff09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assign the device"
      ],
      "metadata": {
        "id": "cUYxcErF6Xqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"My device is {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMsuVv-m6aTz",
        "outputId": "35800c9e-dcfd-4c5d-9fd7-f91cd345e462"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My device is cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build our dataset"
      ],
      "metadata": {
        "id": "b5JKn0ib61xM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "NUM_CLASSES  = 4\n",
        "NUM_FEATURES = 2\n",
        "RANDOM_SEED = 42\n",
        "N_SAMPLES   = 100\n",
        "x_blob, y_blob = make_blobs(n_samples = N_SAMPLES, n_features = NUM_FEATURES, centers = NUM_CLASSES, cluster_std=1.5, random_state=RANDOM_SEED)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tfKhREqk63z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"x blob is {x_blob}\")\n",
        "print(f\"y blob ias {y_blob}\")\n",
        "print(f\"x blob type is {type(x_blob)}\")\n",
        "print(f\"y blob type is {type(y_blob)}\")\n",
        "print(f\"x blob shape is {x_blob.shape}\")\n",
        "print(f\"y blob shape is {y_blob.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSA7dqrc7sA2",
        "outputId": "372092fe-67f9-4a94-b161-d5ac253c1549"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x blob is [[-10.10851833   5.05125208]\n",
            " [ -5.65935137  -8.72640607]\n",
            " [ -3.2134092    9.82812619]\n",
            " [ -4.77543572  -8.98288619]\n",
            " [ -8.30164672   8.16469971]\n",
            " [ -8.36543168  -7.72955619]\n",
            " [  5.13677398   3.43648737]\n",
            " [  4.310371     2.50883854]\n",
            " [  4.78549516   3.42613717]\n",
            " [ -4.23568799   9.57783316]\n",
            " [ -3.32577171   9.18067001]\n",
            " [ -6.53843729  -4.91939546]\n",
            " [ -6.09671484  -6.43463258]\n",
            " [  2.98037638   0.17885975]\n",
            " [ -1.40149775   9.27133855]\n",
            " [-10.4446665    8.04723154]\n",
            " [ -8.42329156   8.56429779]\n",
            " [  4.53186365   3.47846903]\n",
            " [-10.16411391   7.55411057]\n",
            " [ -8.12847132   7.21427955]\n",
            " [ -3.20432416   8.3156915 ]\n",
            " [ -8.95398032   7.83525088]\n",
            " [ -9.50810018   8.60812111]\n",
            " [  5.18197224   4.28022453]\n",
            " [  4.58613978   4.32013517]\n",
            " [ -3.20015578  10.59996947]\n",
            " [  2.51682272   1.3422017 ]\n",
            " [ -7.89982673   6.03778658]\n",
            " [-10.30035026   8.50414982]\n",
            " [ -8.73505326  -8.86079451]\n",
            " [ -7.89966427  -6.53172905]\n",
            " [  5.08405925   2.36475259]\n",
            " [ -7.16816864  -6.42778858]\n",
            " [ -7.10043439   6.09249944]\n",
            " [ -0.1403784   10.16543822]\n",
            " [ -3.41015566   8.5767455 ]\n",
            " [ -2.40790532   6.87716385]\n",
            " [ -2.02307167   8.43666271]\n",
            " [  6.01298201   2.46629635]\n",
            " [  4.77752     -1.00818369]\n",
            " [-10.90483181   5.91678536]\n",
            " [  3.92111748   1.69468122]\n",
            " [ -6.93169485  -8.63312665]\n",
            " [ -4.5014767    9.30957798]\n",
            " [ -2.68267005   8.56263058]\n",
            " [ -8.47308276  -6.16972095]\n",
            " [ -9.29085204  -6.60315881]\n",
            " [ -8.57855637   7.90149899]\n",
            " [ -8.81882492   9.50382403]\n",
            " [  3.42713843   1.22053412]\n",
            " [  0.71026118   3.20602344]\n",
            " [  4.64754902   1.62128898]\n",
            " [  5.85866757   4.00752973]\n",
            " [ -6.489803    -5.70737529]\n",
            " [ -7.60523752  10.16871239]\n",
            " [ -9.17352193   8.39452366]\n",
            " [ -4.0284443    9.48565713]\n",
            " [ -2.14625422   6.14436576]\n",
            " [  6.8567199    1.19576436]\n",
            " [ -5.09657437   8.17085483]\n",
            " [ -6.44001848  -7.95163672]\n",
            " [  3.5867992    1.48167646]\n",
            " [ -9.20640993   6.19291867]\n",
            " [ -9.75778401  -6.91988041]\n",
            " [ -9.23531301  11.40377667]\n",
            " [ -8.51718714   5.45491475]\n",
            " [ -6.78928188  -3.18524642]\n",
            " [-10.1725994    6.09980749]\n",
            " [  3.84523853   2.74307083]\n",
            " [ -6.49330161  -6.99177847]\n",
            " [  4.12580706   0.76975378]\n",
            " [ -6.50388792  -6.36043728]\n",
            " [-10.625283     8.30835333]\n",
            " [ -1.99377019   6.3697259 ]\n",
            " [ -6.73015014  -7.63532307]\n",
            " [ -7.39326356   7.94269431]\n",
            " [ -5.69307927  -8.24419078]\n",
            " [ -8.75101468   5.60906747]\n",
            " [ -4.05034834  -6.61824287]\n",
            " [ -9.20562234  -6.77726513]\n",
            " [  5.18233287   1.00549005]\n",
            " [ -8.05450713  -7.36320187]\n",
            " [ -1.27538025   7.18302065]\n",
            " [ -6.03966599   8.0342723 ]\n",
            " [ -2.52944346   7.42771973]\n",
            " [ -2.19590223   6.07478094]\n",
            " [ -0.31072447   8.67562168]\n",
            " [ -8.25876354  -4.55520799]\n",
            " [ -4.72698061   7.93451982]\n",
            " [  4.05171661  -0.22210274]\n",
            " [  4.77044944   1.52465866]\n",
            " [ -5.16539297  -5.75221004]\n",
            " [ -3.87123374   6.89583058]\n",
            " [ -5.99934155  -3.59442615]\n",
            " [ -3.41175754  11.7927034 ]\n",
            " [ -3.52458062   9.93180056]\n",
            " [ -0.96269834  10.41120631]\n",
            " [  4.39795027   2.57924597]\n",
            " [ -7.21375089   8.90422599]\n",
            " [  3.38105255   1.50935112]]\n",
            "y blob ias [3 2 0 2 3 2 1 1 1 0 0 2 2 1 0 3 3 1 3 3 0 3 3 1 1 0 1 3 3 2 2 1 2 3 0 0 0\n",
            " 0 1 1 3 1 2 0 0 2 2 3 3 1 1 1 1 2 3 3 0 0 1 0 2 1 3 2 3 3 2 3 1 2 1 2 3 0\n",
            " 2 3 2 3 2 2 1 2 0 3 0 0 0 2 0 1 1 2 0 2 0 0 0 1 3 1]\n",
            "x blob type is <class 'numpy.ndarray'>\n",
            "y blob type is <class 'numpy.ndarray'>\n",
            "x blob shape is (100, 2)\n",
            "y blob shape is (100,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot my dataset"
      ],
      "metadata": {
        "id": "4bJ47Ztm8Ry-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x_blob[:, 0], x_blob[:, 1], c=y_blob)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "VYssbRvl8TjF",
        "outputId": "a714dc92-d552-4f09-e950-bd1771124bbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7e1e7e4e8190>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert these numpy to tensors"
      ],
      "metadata": {
        "id": "YTSIG9Xo81nO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_tensor = torch.from_numpy(x_blob).type(torch.float).to(device)\n",
        "# ytensor should be long\n",
        "y_tensor = torch.from_numpy(y_blob).type(torch.long).to(device)\n",
        "\n",
        "print(f\"{X_tensor.shape} and {X_tensor.device} and {X_tensor.dtype}\")\n",
        "print(f\"{y_tensor.shape} and {y_tensor.device} and {y_tensor.dtype}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkHk163k83cV",
        "outputId": "4a367b05-9b43-4668-b267-8155ac296f8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 2]) and cuda:0 and torch.float32\n",
            "torch.Size([100]) and cuda:0 and torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train/test split"
      ],
      "metadata": {
        "id": "hBUvd2_b9WC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.20, random_state=42)"
      ],
      "metadata": {
        "id": "DfQ2cXd19X-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot train/test split"
      ],
      "metadata": {
        "id": "s0mSW2YJ99z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Information about training dataset {X_train.shape}\")\n",
        "plt.scatter(X_train[:, 0].cpu(), X_train[:, 1].cpu(), c=y_train.cpu())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "BzOItHeG9_2b",
        "outputId": "a501cdfa-a1dc-4040-e303-21f24ee0ba75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Information about training dataset torch.Size([80, 2])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7e1e7856ff70>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Information about test dataset {X_test.shape}\")\n",
        "plt.scatter(X_test[:, 0].cpu(), X_test[:, 1].cpu(), c=y_test.cpu())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "5FtV6CJ5-TBr",
        "outputId": "a5bfbbe2-8785-4615-edee-9f277c596639"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Information about test dataset torch.Size([20, 2])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7e1e78609bd0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Design a Neural Network"
      ],
      "metadata": {
        "id": "SFTJh-39-kbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#in_features=2, out_features=4\n",
        "#NN:-(1000000, 2)->(1000000, 4)\n",
        "class MulticlassNN(nn.Module):\n",
        "  def __init__(self, in_features, out_features, hidden_units=8):\n",
        "    super().__init__()\n",
        "    self.layer = nn.Sequential(\n",
        "        nn.Linear(in_features=in_features, out_features=hidden_units),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=hidden_units, out_features=hidden_units*2),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=hidden_units*2, out_features=out_features),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layer(x)"
      ],
      "metadata": {
        "id": "8jOVdnVD-nI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MulticlassNN(NUM_FEATURES, NUM_CLASSES).to(device)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYg1QHfYAM-k",
        "outputId": "8881ed32-9e59-4359-f735-87fa7b9a45cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MulticlassNN(\n",
              "  (layer): Sequential(\n",
              "    (0): Linear(in_features=2, out_features=8, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=8, out_features=16, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=16, out_features=4, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nk0l0ti3Alyb",
        "outputId": "f1c4c8d6-fced-432d-8fdf-d76e8e7cf433"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('layer.0.weight',\n",
              "              tensor([[ 0.3557,  0.4390],\n",
              "                      [-0.5228,  0.4906],\n",
              "                      [-0.0387,  0.1586],\n",
              "                      [ 0.1255,  0.6633],\n",
              "                      [-0.4566, -0.1498],\n",
              "                      [-0.2810, -0.0255],\n",
              "                      [ 0.2925,  0.2505],\n",
              "                      [ 0.0439,  0.5723]], device='cuda:0')),\n",
              "             ('layer.0.bias',\n",
              "              tensor([ 0.0891, -0.1746,  0.5786, -0.5920, -0.5005,  0.5527, -0.1274,  0.3768],\n",
              "                     device='cuda:0')),\n",
              "             ('layer.2.weight',\n",
              "              tensor([[ 3.3839e-03, -1.1903e-01, -1.2421e-01,  3.0998e-01,  6.5029e-02,\n",
              "                        8.2599e-02,  1.5911e-01,  2.5043e-01],\n",
              "                      [-1.7657e-01,  2.0233e-01,  1.0458e-01,  3.2103e-01, -1.3738e-01,\n",
              "                        1.2552e-01, -1.9862e-01, -8.4499e-02],\n",
              "                      [-3.4046e-01, -1.5564e-01, -3.4598e-01, -2.8151e-01,  4.1948e-02,\n",
              "                        2.3703e-01,  1.7148e-01, -6.4988e-02],\n",
              "                      [-6.1773e-02,  1.8533e-01,  2.3968e-02, -3.4709e-01, -4.7405e-02,\n",
              "                       -2.1942e-02,  1.9949e-02,  2.4810e-01],\n",
              "                      [-3.1826e-01,  1.4653e-01,  3.0024e-01,  1.0337e-01, -3.2273e-01,\n",
              "                        1.8488e-01,  2.5697e-02, -1.5104e-02],\n",
              "                      [-5.7193e-05,  6.9068e-02, -2.7955e-01, -2.8950e-01,  3.1570e-01,\n",
              "                        3.7101e-03,  5.8670e-02,  9.7319e-02],\n",
              "                      [-2.5298e-01,  1.9564e-01,  2.4212e-02,  1.3132e-01, -2.6490e-02,\n",
              "                       -2.1601e-01, -2.7997e-01, -1.7275e-02],\n",
              "                      [ 2.5650e-01,  3.3599e-01, -3.0668e-03,  2.6131e-01, -1.3562e-01,\n",
              "                       -9.1416e-02,  3.4967e-01, -2.4206e-01],\n",
              "                      [ 1.2146e-01,  3.3353e-01, -4.3740e-03, -2.1495e-02,  3.3068e-01,\n",
              "                       -3.0375e-01,  1.2260e-01,  2.8859e-01],\n",
              "                      [-5.9811e-02,  2.2617e-01,  3.4909e-01, -3.2066e-01, -1.3926e-01,\n",
              "                        1.5640e-01, -1.2729e-01, -7.1894e-02],\n",
              "                      [ 1.0577e-01,  1.0631e-01, -1.4597e-01,  1.0255e-01, -2.2072e-01,\n",
              "                       -1.6778e-01, -1.9707e-01,  3.5341e-01],\n",
              "                      [-2.5313e-01, -1.0391e-01,  2.6594e-01,  1.9506e-01, -2.3000e-02,\n",
              "                        6.9273e-02, -3.3210e-02,  6.3565e-02],\n",
              "                      [-1.9493e-01,  4.2315e-02,  1.7657e-01,  2.8095e-01,  2.0287e-01,\n",
              "                        2.9298e-01, -1.4717e-01,  5.7083e-02],\n",
              "                      [ 7.3251e-02,  1.6804e-01,  1.6426e-01,  3.4402e-01, -2.9182e-01,\n",
              "                       -2.1568e-01,  6.4604e-02,  3.5351e-01],\n",
              "                      [-1.3454e-01,  2.2640e-01,  2.9686e-01,  3.3316e-01,  3.2696e-01,\n",
              "                       -8.2650e-03, -3.5297e-02, -1.4478e-01],\n",
              "                      [-3.0341e-01,  1.1233e-01, -9.8588e-02, -6.3394e-02,  6.4008e-02,\n",
              "                        7.8642e-02, -2.8667e-01,  3.3995e-01]], device='cuda:0')),\n",
              "             ('layer.2.bias',\n",
              "              tensor([ 0.0189, -0.1392, -0.3423, -0.0157, -0.2533, -0.1163,  0.2515, -0.3409,\n",
              "                       0.0775, -0.2190, -0.2915, -0.2191,  0.3095,  0.0794,  0.2437,  0.3456],\n",
              "                     device='cuda:0')),\n",
              "             ('layer.4.weight',\n",
              "              tensor([[-0.0249,  0.1636,  0.1343, -0.1671,  0.1488,  0.0115,  0.0597,  0.0997,\n",
              "                        0.1702, -0.1539, -0.1940, -0.2469, -0.1579,  0.0049, -0.2258,  0.1850],\n",
              "                      [-0.0447,  0.1181,  0.0584, -0.2395,  0.0425,  0.0946, -0.1122, -0.2242,\n",
              "                       -0.0449, -0.2420, -0.2399, -0.0857,  0.1322, -0.1349,  0.2269,  0.2480],\n",
              "                      [-0.2405, -0.1422,  0.0270, -0.0919,  0.1619, -0.0576, -0.2249, -0.1993,\n",
              "                        0.1844,  0.0763, -0.2171, -0.0018,  0.0073,  0.2025, -0.0346, -0.0300],\n",
              "                      [ 0.1244, -0.0367,  0.2185,  0.0790,  0.2099,  0.0204,  0.1493, -0.2155,\n",
              "                        0.2107,  0.1219, -0.1236,  0.2460, -0.1227, -0.2470,  0.1780, -0.1075]],\n",
              "                     device='cuda:0')),\n",
              "             ('layer.4.bias',\n",
              "              tensor([-0.0192,  0.0813, -0.1051,  0.2279], device='cuda:0'))])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Send some random input and get the output back"
      ],
      "metadata": {
        "id": "pP0VLFyzBhO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#untrained model\n",
        "random_output = model(X_tensor)\n",
        "print(f\"My input shape is {X_tensor.shape} and dtype {X_tensor.dtype}\")\n",
        "print(f\"My output shape is {random_output.shape} and dtype {random_output.dtype}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgBjjOu-BkKc",
        "outputId": "dc602c19-48f5-4286-a692-6c61ab4f6904"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My input shape is torch.Size([100, 2]) and dtype torch.float32\n",
            "My output shape is torch.Size([100, 4]) and dtype torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to get the output back from model"
      ],
      "metadata": {
        "id": "NX74l3CHCVpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#untrained model\n",
        "logits = model(torch.rand(3, 2).type(torch.float32).to(device))\n",
        "print(f\"My input shape is {X_tensor.shape} and dtype {X_tensor.dtype}\")\n",
        "print(f\"My logits shape is {logits.shape} and dtype {logits.dtype}\")\n",
        "print(f\"My logits are {logits}\")\n",
        "softmax_values = torch.softmax(logits, dim=1)\n",
        "print(softmax_values.shape)\n",
        "print(f\"My softmax is {softmax_values} and its shape is {softmax_values.shape}\")\n",
        "y_preds = torch.argmax(softmax_values, dim=1)\n",
        "print(f\"my ypred are {y_preds} and its shape is {y_preds.shape}\")\n",
        "print(torch.softmax(logits, dim=1).argmax(dim=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3M34hZa3Ca6s",
        "outputId": "c8d58441-0d02-4f4d-ca68-387f70ad8bec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My input shape is torch.Size([100, 2]) and dtype torch.float32\n",
            "My logits shape is torch.Size([3, 4]) and dtype torch.float32\n",
            "My logits are tensor([[ 0.1960, -0.1446, -0.2707,  0.2285],\n",
            "        [ 0.1423, -0.1133, -0.2800,  0.2498],\n",
            "        [ 0.1759, -0.1345, -0.2848,  0.2189]], device='cuda:0',\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "torch.Size([3, 4])\n",
            "My softmax is tensor([[0.2966, 0.2110, 0.1860, 0.3064],\n",
            "        [0.2822, 0.2186, 0.1850, 0.3142],\n",
            "        [0.2934, 0.2151, 0.1851, 0.3063]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>) and its shape is torch.Size([3, 4])\n",
            "my ypred are tensor([3, 3, 3], device='cuda:0') and its shape is torch.Size([3])\n",
            "tensor([3, 3, 3], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a loss function and optimizer"
      ],
      "metadata": {
        "id": "lxLvy60oEvzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.06)"
      ],
      "metadata": {
        "id": "Xn6jEQ0nExmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Please check if the loss function is giving some number"
      ],
      "metadata": {
        "id": "KRWC7JkhDi9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#untrained model\n",
        "random_output = model(X_tensor)\n",
        "#the bug is your y_tensor is expected to be Long\n",
        "print(f\"dtype 1 {random_output.dtype} and dtype2 {y_tensor.dtype}\")\n",
        "loss_fn(random_output, y_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VREBr10eD2XE",
        "outputId": "6000533d-2fbb-43ff-9dfe-29ab727ed17a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dtype 1 torch.float32 and dtype2 torch.int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.6534, device='cuda:0', grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start training the model"
      ],
      "metadata": {
        "id": "A5qz2lIpCClV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def acc_fn(y_true, y_pred):\n",
        "  correct = torch.eq(y_true, y_pred).sum().item()\n",
        "  acc = (correct/ len(y_true))*100.0\n",
        "  return acc"
      ],
      "metadata": {
        "id": "lM4-J4UHFRrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50000\n",
        "for epoch in range(epochs):\n",
        "  #put my model in training model\n",
        "  model.train()\n",
        "  #make predictions for my training dataset\n",
        "  logits = model(X_train).squeeze()\n",
        "  #I calculate the loss\n",
        "  training_loss = loss_fn(logits, y_train)\n",
        "  y_preds = torch.softmax(logits, dim=1).argmax(dim=1)\n",
        "  training_acc = acc_fn(y_train, y_preds)\n",
        "\n",
        "  #make all the previous gradients to zero\n",
        "  optimizer.zero_grad()\n",
        "  #compute the gradients\n",
        "  training_loss.backward()\n",
        "  #take an optimization step\n",
        "  optimizer.step()\n",
        "\n",
        "  model.eval() #eval stage\n",
        "  with torch.no_grad():\n",
        "    test_logits = model(X_test).squeeze()\n",
        "    test_loss = loss_fn(test_logits, y_test)\n",
        "    y_preds = torch.softmax(test_logits, dim=1).argmax(dim=1)\n",
        "    testing_acc = acc_fn(y_test, y_preds)\n",
        "\n",
        "  if epoch%1==0:\n",
        "    print(f\"Epoch: {epoch} | Training Loss: {training_loss} | Training Acc: {training_acc} | Testing Loss: {test_loss} | Testing Acc: {testing_acc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "enKLCNdMCEjD",
        "outputId": "2aa233d1-b8f9-4571-f348-ee3219f95cf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Training Loss: 1.6890232563018799 | Training Acc: 22.5 | Testing Loss: 1.355880618095398 | Testing Acc: 35.0\n",
            "Epoch: 1 | Training Loss: 1.468564748764038 | Training Acc: 22.5 | Testing Loss: 1.232939600944519 | Testing Acc: 60.0\n",
            "Epoch: 2 | Training Loss: 1.3076642751693726 | Training Acc: 46.25 | Testing Loss: 1.1298115253448486 | Testing Acc: 60.0\n",
            "Epoch: 3 | Training Loss: 1.1785430908203125 | Training Acc: 47.5 | Testing Loss: 1.0402412414550781 | Testing Acc: 70.0\n",
            "Epoch: 4 | Training Loss: 1.0713337659835815 | Training Acc: 61.25000000000001 | Testing Loss: 0.9617916345596313 | Testing Acc: 75.0\n",
            "Epoch: 5 | Training Loss: 0.9815413355827332 | Training Acc: 71.25 | Testing Loss: 0.8923784494400024 | Testing Acc: 75.0\n",
            "Epoch: 6 | Training Loss: 0.9048677682876587 | Training Acc: 73.75 | Testing Loss: 0.8296078443527222 | Testing Acc: 85.0\n",
            "Epoch: 7 | Training Loss: 0.8378434181213379 | Training Acc: 77.5 | Testing Loss: 0.7723069787025452 | Testing Acc: 95.0\n",
            "Epoch: 8 | Training Loss: 0.7786799669265747 | Training Acc: 78.75 | Testing Loss: 0.7190696001052856 | Testing Acc: 100.0\n",
            "Epoch: 9 | Training Loss: 0.7254865765571594 | Training Acc: 82.5 | Testing Loss: 0.6682826280593872 | Testing Acc: 100.0\n",
            "Epoch: 10 | Training Loss: 0.6765052080154419 | Training Acc: 86.25 | Testing Loss: 0.6196980476379395 | Testing Acc: 100.0\n",
            "Epoch: 11 | Training Loss: 0.6309545040130615 | Training Acc: 95.0 | Testing Loss: 0.5733963847160339 | Testing Acc: 100.0\n",
            "Epoch: 12 | Training Loss: 0.5883589386940002 | Training Acc: 97.5 | Testing Loss: 0.5296422243118286 | Testing Acc: 100.0\n",
            "Epoch: 13 | Training Loss: 0.5486302375793457 | Training Acc: 98.75 | Testing Loss: 0.4885452687740326 | Testing Acc: 100.0\n",
            "Epoch: 14 | Training Loss: 0.5114274024963379 | Training Acc: 98.75 | Testing Loss: 0.45041078329086304 | Testing Acc: 100.0\n",
            "Epoch: 15 | Training Loss: 0.4768269658088684 | Training Acc: 98.75 | Testing Loss: 0.4151827394962311 | Testing Acc: 100.0\n",
            "Epoch: 16 | Training Loss: 0.444771945476532 | Training Acc: 98.75 | Testing Loss: 0.3828819990158081 | Testing Acc: 100.0\n",
            "Epoch: 17 | Training Loss: 0.4152401387691498 | Training Acc: 98.75 | Testing Loss: 0.3533691167831421 | Testing Acc: 100.0\n",
            "Epoch: 18 | Training Loss: 0.3879773020744324 | Training Acc: 98.75 | Testing Loss: 0.3266279995441437 | Testing Acc: 100.0\n",
            "Epoch: 19 | Training Loss: 0.3628302216529846 | Training Acc: 98.75 | Testing Loss: 0.3024946451187134 | Testing Acc: 100.0\n",
            "Epoch: 20 | Training Loss: 0.33968883752822876 | Training Acc: 98.75 | Testing Loss: 0.2806970477104187 | Testing Acc: 100.0\n",
            "Epoch: 21 | Training Loss: 0.3184434771537781 | Training Acc: 98.75 | Testing Loss: 0.26092761754989624 | Testing Acc: 100.0\n",
            "Epoch: 22 | Training Loss: 0.29898545145988464 | Training Acc: 98.75 | Testing Loss: 0.24300098419189453 | Testing Acc: 100.0\n",
            "Epoch: 23 | Training Loss: 0.2811306416988373 | Training Acc: 98.75 | Testing Loss: 0.226726695895195 | Testing Acc: 100.0\n",
            "Epoch: 24 | Training Loss: 0.2647487223148346 | Training Acc: 98.75 | Testing Loss: 0.21192291378974915 | Testing Acc: 100.0\n",
            "Epoch: 25 | Training Loss: 0.2497013807296753 | Training Acc: 98.75 | Testing Loss: 0.1984999179840088 | Testing Acc: 100.0\n",
            "Epoch: 26 | Training Loss: 0.23588168621063232 | Training Acc: 98.75 | Testing Loss: 0.18630903959274292 | Testing Acc: 100.0\n",
            "Epoch: 27 | Training Loss: 0.22320406138896942 | Training Acc: 98.75 | Testing Loss: 0.17522060871124268 | Testing Acc: 100.0\n",
            "Epoch: 28 | Training Loss: 0.21156418323516846 | Training Acc: 98.75 | Testing Loss: 0.16511616110801697 | Testing Acc: 100.0\n",
            "Epoch: 29 | Training Loss: 0.20087583363056183 | Training Acc: 98.75 | Testing Loss: 0.15597110986709595 | Testing Acc: 100.0\n",
            "Epoch: 30 | Training Loss: 0.19108974933624268 | Training Acc: 98.75 | Testing Loss: 0.1475796103477478 | Testing Acc: 100.0\n",
            "Epoch: 31 | Training Loss: 0.18209365010261536 | Training Acc: 98.75 | Testing Loss: 0.13989855349063873 | Testing Acc: 100.0\n",
            "Epoch: 32 | Training Loss: 0.173800989985466 | Training Acc: 98.75 | Testing Loss: 0.132840096950531 | Testing Acc: 100.0\n",
            "Epoch: 33 | Training Loss: 0.1661374419927597 | Training Acc: 98.75 | Testing Loss: 0.126370370388031 | Testing Acc: 100.0\n",
            "Epoch: 34 | Training Loss: 0.15905360877513885 | Training Acc: 98.75 | Testing Loss: 0.12041698396205902 | Testing Acc: 100.0\n",
            "Epoch: 35 | Training Loss: 0.15249420702457428 | Training Acc: 98.75 | Testing Loss: 0.11492513120174408 | Testing Acc: 100.0\n",
            "Epoch: 36 | Training Loss: 0.1464051753282547 | Training Acc: 98.75 | Testing Loss: 0.10985829681158066 | Testing Acc: 100.0\n",
            "Epoch: 37 | Training Loss: 0.14074884355068207 | Training Acc: 98.75 | Testing Loss: 0.10517378151416779 | Testing Acc: 100.0\n",
            "Epoch: 38 | Training Loss: 0.1354876607656479 | Training Acc: 98.75 | Testing Loss: 0.1008376032114029 | Testing Acc: 100.0\n",
            "Epoch: 39 | Training Loss: 0.13058724999427795 | Training Acc: 98.75 | Testing Loss: 0.09682260453701019 | Testing Acc: 100.0\n",
            "Epoch: 40 | Training Loss: 0.12601527571678162 | Training Acc: 98.75 | Testing Loss: 0.09308812767267227 | Testing Acc: 100.0\n",
            "Epoch: 41 | Training Loss: 0.12174234539270401 | Training Acc: 98.75 | Testing Loss: 0.08961265534162521 | Testing Acc: 100.0\n",
            "Epoch: 42 | Training Loss: 0.11774283647537231 | Training Acc: 98.75 | Testing Loss: 0.08637057989835739 | Testing Acc: 100.0\n",
            "Epoch: 43 | Training Loss: 0.11399338394403458 | Training Acc: 98.75 | Testing Loss: 0.08334426581859589 | Testing Acc: 100.0\n",
            "Epoch: 44 | Training Loss: 0.11047323793172836 | Training Acc: 98.75 | Testing Loss: 0.08051423728466034 | Testing Acc: 100.0\n",
            "Epoch: 45 | Training Loss: 0.10716452449560165 | Training Acc: 98.75 | Testing Loss: 0.0778658539056778 | Testing Acc: 100.0\n",
            "Epoch: 46 | Training Loss: 0.10404958575963974 | Training Acc: 98.75 | Testing Loss: 0.07538031041622162 | Testing Acc: 100.0\n",
            "Epoch: 47 | Training Loss: 0.10111100971698761 | Training Acc: 98.75 | Testing Loss: 0.0730472058057785 | Testing Acc: 100.0\n",
            "Epoch: 48 | Training Loss: 0.09833505749702454 | Training Acc: 98.75 | Testing Loss: 0.07085184752941132 | Testing Acc: 100.0\n",
            "Epoch: 49 | Training Loss: 0.09571393579244614 | Training Acc: 98.75 | Testing Loss: 0.06880451738834381 | Testing Acc: 100.0\n",
            "Epoch: 50 | Training Loss: 0.09323523938655853 | Training Acc: 98.75 | Testing Loss: 0.06685362011194229 | Testing Acc: 100.0\n",
            "Epoch: 51 | Training Loss: 0.09089149534702301 | Training Acc: 98.75 | Testing Loss: 0.06501670181751251 | Testing Acc: 100.0\n",
            "Epoch: 52 | Training Loss: 0.08866807073354721 | Training Acc: 98.75 | Testing Loss: 0.06328289210796356 | Testing Acc: 100.0\n",
            "Epoch: 53 | Training Loss: 0.08655864745378494 | Training Acc: 98.75 | Testing Loss: 0.06164207309484482 | Testing Acc: 100.0\n",
            "Epoch: 54 | Training Loss: 0.08455308526754379 | Training Acc: 98.75 | Testing Loss: 0.060091424733400345 | Testing Acc: 100.0\n",
            "Epoch: 55 | Training Loss: 0.08264543116092682 | Training Acc: 98.75 | Testing Loss: 0.05862114578485489 | Testing Acc: 100.0\n",
            "Epoch: 56 | Training Loss: 0.08082789182662964 | Training Acc: 98.75 | Testing Loss: 0.05722497031092644 | Testing Acc: 100.0\n",
            "Epoch: 57 | Training Loss: 0.07909505069255829 | Training Acc: 98.75 | Testing Loss: 0.05589687079191208 | Testing Acc: 100.0\n",
            "Epoch: 58 | Training Loss: 0.07744160294532776 | Training Acc: 98.75 | Testing Loss: 0.05464407801628113 | Testing Acc: 100.0\n",
            "Epoch: 59 | Training Loss: 0.07586203515529633 | Training Acc: 98.75 | Testing Loss: 0.05344460532069206 | Testing Acc: 100.0\n",
            "Epoch: 60 | Training Loss: 0.07435160875320435 | Training Acc: 98.75 | Testing Loss: 0.05230005830526352 | Testing Acc: 100.0\n",
            "Epoch: 61 | Training Loss: 0.07290579378604889 | Training Acc: 98.75 | Testing Loss: 0.05120856687426567 | Testing Acc: 100.0\n",
            "Epoch: 62 | Training Loss: 0.07152070850133896 | Training Acc: 98.75 | Testing Loss: 0.05016520619392395 | Testing Acc: 100.0\n",
            "Epoch: 63 | Training Loss: 0.07019255310297012 | Training Acc: 98.75 | Testing Loss: 0.0491587296128273 | Testing Acc: 100.0\n",
            "Epoch: 64 | Training Loss: 0.0689178928732872 | Training Acc: 98.75 | Testing Loss: 0.04820263385772705 | Testing Acc: 100.0\n",
            "Epoch: 65 | Training Loss: 0.06769388914108276 | Training Acc: 98.75 | Testing Loss: 0.04729112237691879 | Testing Acc: 100.0\n",
            "Epoch: 66 | Training Loss: 0.06651758402585983 | Training Acc: 98.75 | Testing Loss: 0.04641812667250633 | Testing Acc: 100.0\n",
            "Epoch: 67 | Training Loss: 0.06538625806570053 | Training Acc: 98.75 | Testing Loss: 0.045581988990306854 | Testing Acc: 100.0\n",
            "Epoch: 68 | Training Loss: 0.06429747492074966 | Training Acc: 98.75 | Testing Loss: 0.044781044125556946 | Testing Acc: 100.0\n",
            "Epoch: 69 | Training Loss: 0.06324882805347443 | Training Acc: 98.75 | Testing Loss: 0.044011957943439484 | Testing Acc: 100.0\n",
            "Epoch: 70 | Training Loss: 0.06223803758621216 | Training Acc: 98.75 | Testing Loss: 0.04327332228422165 | Testing Acc: 100.0\n",
            "Epoch: 71 | Training Loss: 0.061263252049684525 | Training Acc: 98.75 | Testing Loss: 0.04256356135010719 | Testing Acc: 100.0\n",
            "Epoch: 72 | Training Loss: 0.06032262369990349 | Training Acc: 98.75 | Testing Loss: 0.041881076991558075 | Testing Acc: 100.0\n",
            "Epoch: 73 | Training Loss: 0.059414368122816086 | Training Acc: 98.75 | Testing Loss: 0.04122447222471237 | Testing Acc: 100.0\n",
            "Epoch: 74 | Training Loss: 0.0585368387401104 | Training Acc: 98.75 | Testing Loss: 0.040592294186353683 | Testing Acc: 100.0\n",
            "Epoch: 75 | Training Loss: 0.05768854543566704 | Training Acc: 100.0 | Testing Loss: 0.03998333960771561 | Testing Acc: 100.0\n",
            "Epoch: 76 | Training Loss: 0.05686797574162483 | Training Acc: 100.0 | Testing Loss: 0.039396364241838455 | Testing Acc: 100.0\n",
            "Epoch: 77 | Training Loss: 0.05607382580637932 | Training Acc: 100.0 | Testing Loss: 0.03883030638098717 | Testing Acc: 100.0\n",
            "Epoch: 78 | Training Loss: 0.055304817855358124 | Training Acc: 100.0 | Testing Loss: 0.03828404098749161 | Testing Acc: 100.0\n",
            "Epoch: 79 | Training Loss: 0.05455975979566574 | Training Acc: 100.0 | Testing Loss: 0.03775663673877716 | Testing Acc: 100.0\n",
            "Epoch: 80 | Training Loss: 0.05383756756782532 | Training Acc: 100.0 | Testing Loss: 0.037247177213430405 | Testing Acc: 100.0\n",
            "Epoch: 81 | Training Loss: 0.0531371533870697 | Training Acc: 100.0 | Testing Loss: 0.03675472363829613 | Testing Acc: 100.0\n",
            "Epoch: 82 | Training Loss: 0.052457522600889206 | Training Acc: 100.0 | Testing Loss: 0.036278583109378815 | Testing Acc: 100.0\n",
            "Epoch: 83 | Training Loss: 0.0517977774143219 | Training Acc: 100.0 | Testing Loss: 0.035817913711071014 | Testing Acc: 100.0\n",
            "Epoch: 84 | Training Loss: 0.05115702748298645 | Training Acc: 100.0 | Testing Loss: 0.035372015088796616 | Testing Acc: 100.0\n",
            "Epoch: 85 | Training Loss: 0.05053446441888809 | Training Acc: 100.0 | Testing Loss: 0.03494006395339966 | Testing Acc: 100.0\n",
            "Epoch: 86 | Training Loss: 0.04992926865816116 | Training Acc: 100.0 | Testing Loss: 0.03452117741107941 | Testing Acc: 100.0\n",
            "Epoch: 87 | Training Loss: 0.04934071749448776 | Training Acc: 100.0 | Testing Loss: 0.03411512076854706 | Testing Acc: 100.0\n",
            "Epoch: 88 | Training Loss: 0.04876812547445297 | Training Acc: 100.0 | Testing Loss: 0.0337214395403862 | Testing Acc: 100.0\n",
            "Epoch: 89 | Training Loss: 0.048210807144641876 | Training Acc: 100.0 | Testing Loss: 0.0333394929766655 | Testing Acc: 100.0\n",
            "Epoch: 90 | Training Loss: 0.047668181359767914 | Training Acc: 100.0 | Testing Loss: 0.0329686775803566 | Testing Acc: 100.0\n",
            "Epoch: 91 | Training Loss: 0.04713963344693184 | Training Acc: 100.0 | Testing Loss: 0.03260874003171921 | Testing Acc: 100.0\n",
            "Epoch: 92 | Training Loss: 0.04662454500794411 | Training Acc: 100.0 | Testing Loss: 0.03225909173488617 | Testing Acc: 100.0\n",
            "Epoch: 93 | Training Loss: 0.046122364699840546 | Training Acc: 100.0 | Testing Loss: 0.03191935271024704 | Testing Acc: 100.0\n",
            "Epoch: 94 | Training Loss: 0.045632749795913696 | Training Acc: 100.0 | Testing Loss: 0.03159468621015549 | Testing Acc: 100.0\n",
            "Epoch: 95 | Training Loss: 0.045155368745326996 | Training Acc: 100.0 | Testing Loss: 0.03127530962228775 | Testing Acc: 100.0\n",
            "Epoch: 96 | Training Loss: 0.044689539819955826 | Training Acc: 100.0 | Testing Loss: 0.03096335008740425 | Testing Acc: 100.0\n",
            "Epoch: 97 | Training Loss: 0.044234778732061386 | Training Acc: 100.0 | Testing Loss: 0.030659284442663193 | Testing Acc: 100.0\n",
            "Epoch: 98 | Training Loss: 0.04379076510667801 | Training Acc: 100.0 | Testing Loss: 0.030363205820322037 | Testing Acc: 100.0\n",
            "Epoch: 99 | Training Loss: 0.043357014656066895 | Training Acc: 100.0 | Testing Loss: 0.03007480874657631 | Testing Acc: 100.0\n",
            "Epoch: 100 | Training Loss: 0.042933277785778046 | Training Acc: 100.0 | Testing Loss: 0.029794031754136086 | Testing Acc: 100.0\n",
            "Epoch: 101 | Training Loss: 0.04251911863684654 | Training Acc: 100.0 | Testing Loss: 0.029520317912101746 | Testing Acc: 100.0\n",
            "Epoch: 102 | Training Loss: 0.042114146053791046 | Training Acc: 100.0 | Testing Loss: 0.029253458604216576 | Testing Acc: 100.0\n",
            "Epoch: 103 | Training Loss: 0.04171811416745186 | Training Acc: 100.0 | Testing Loss: 0.02899426594376564 | Testing Acc: 100.0\n",
            "Epoch: 104 | Training Loss: 0.0413307249546051 | Training Acc: 100.0 | Testing Loss: 0.028742607682943344 | Testing Acc: 100.0\n",
            "Epoch: 105 | Training Loss: 0.040951747447252274 | Training Acc: 100.0 | Testing Loss: 0.02849709428846836 | Testing Acc: 100.0\n",
            "Epoch: 106 | Training Loss: 0.04058079048991203 | Training Acc: 100.0 | Testing Loss: 0.02825750783085823 | Testing Acc: 100.0\n",
            "Epoch: 107 | Training Loss: 0.04021754860877991 | Training Acc: 100.0 | Testing Loss: 0.02802358940243721 | Testing Acc: 100.0\n",
            "Epoch: 108 | Training Loss: 0.039861779659986496 | Training Acc: 100.0 | Testing Loss: 0.027795031666755676 | Testing Acc: 100.0\n",
            "Epoch: 109 | Training Loss: 0.03951326012611389 | Training Acc: 100.0 | Testing Loss: 0.02757168374955654 | Testing Acc: 100.0\n",
            "Epoch: 110 | Training Loss: 0.039171747863292694 | Training Acc: 100.0 | Testing Loss: 0.02735334262251854 | Testing Acc: 100.0\n",
            "Epoch: 111 | Training Loss: 0.03883703425526619 | Training Acc: 100.0 | Testing Loss: 0.02713984251022339 | Testing Acc: 100.0\n",
            "Epoch: 112 | Training Loss: 0.03850887715816498 | Training Acc: 100.0 | Testing Loss: 0.026931118220090866 | Testing Acc: 100.0\n",
            "Epoch: 113 | Training Loss: 0.03818707540631294 | Training Acc: 100.0 | Testing Loss: 0.026732509955763817 | Testing Acc: 100.0\n",
            "Epoch: 114 | Training Loss: 0.03787136450409889 | Training Acc: 100.0 | Testing Loss: 0.02653469517827034 | Testing Acc: 100.0\n",
            "Epoch: 115 | Training Loss: 0.037561602890491486 | Training Acc: 100.0 | Testing Loss: 0.026339858770370483 | Testing Acc: 100.0\n",
            "Epoch: 116 | Training Loss: 0.03725761920213699 | Training Acc: 100.0 | Testing Loss: 0.026148581877350807 | Testing Acc: 100.0\n",
            "Epoch: 117 | Training Loss: 0.036959294229745865 | Training Acc: 100.0 | Testing Loss: 0.02596111223101616 | Testing Acc: 100.0\n",
            "Epoch: 118 | Training Loss: 0.036666400730609894 | Training Acc: 100.0 | Testing Loss: 0.025777405127882957 | Testing Acc: 100.0\n",
            "Epoch: 119 | Training Loss: 0.03637883439660072 | Training Acc: 100.0 | Testing Loss: 0.025597210973501205 | Testing Acc: 100.0\n",
            "Epoch: 120 | Training Loss: 0.03609640523791313 | Training Acc: 100.0 | Testing Loss: 0.0254207756370306 | Testing Acc: 100.0\n",
            "Epoch: 121 | Training Loss: 0.035819001495838165 | Training Acc: 100.0 | Testing Loss: 0.025247832760214806 | Testing Acc: 100.0\n",
            "Epoch: 122 | Training Loss: 0.03554642200469971 | Training Acc: 100.0 | Testing Loss: 0.02507835626602173 | Testing Acc: 100.0\n",
            "Epoch: 123 | Training Loss: 0.035278551280498505 | Training Acc: 100.0 | Testing Loss: 0.024912100285291672 | Testing Acc: 100.0\n",
            "Epoch: 124 | Training Loss: 0.0350152887403965 | Training Acc: 100.0 | Testing Loss: 0.024749020114541054 | Testing Acc: 100.0\n",
            "Epoch: 125 | Training Loss: 0.03475644811987877 | Training Acc: 100.0 | Testing Loss: 0.02458905056118965 | Testing Acc: 100.0\n",
            "Epoch: 126 | Training Loss: 0.034501928836107254 | Training Acc: 100.0 | Testing Loss: 0.024432022124528885 | Testing Acc: 100.0\n",
            "Epoch: 127 | Training Loss: 0.0342516154050827 | Training Acc: 100.0 | Testing Loss: 0.024277906864881516 | Testing Acc: 100.0\n",
            "Epoch: 128 | Training Loss: 0.03400542214512825 | Training Acc: 100.0 | Testing Loss: 0.02412664145231247 | Testing Acc: 100.0\n",
            "Epoch: 129 | Training Loss: 0.033763181418180466 | Training Acc: 100.0 | Testing Loss: 0.02397804707288742 | Testing Acc: 100.0\n",
            "Epoch: 130 | Training Loss: 0.033524852246046066 | Training Acc: 100.0 | Testing Loss: 0.023832114413380623 | Testing Acc: 100.0\n",
            "Epoch: 131 | Training Loss: 0.03329028934240341 | Training Acc: 100.0 | Testing Loss: 0.023688767105340958 | Testing Acc: 100.0\n",
            "Epoch: 132 | Training Loss: 0.03305940330028534 | Training Acc: 100.0 | Testing Loss: 0.02354794554412365 | Testing Acc: 100.0\n",
            "Epoch: 133 | Training Loss: 0.03283209726214409 | Training Acc: 100.0 | Testing Loss: 0.023408913984894753 | Testing Acc: 100.0\n",
            "Epoch: 134 | Training Loss: 0.03260831534862518 | Training Acc: 100.0 | Testing Loss: 0.023272676393389702 | Testing Acc: 100.0\n",
            "Epoch: 135 | Training Loss: 0.03238794207572937 | Training Acc: 100.0 | Testing Loss: 0.02313889190554619 | Testing Acc: 100.0\n",
            "Epoch: 136 | Training Loss: 0.032170895487070084 | Training Acc: 100.0 | Testing Loss: 0.023007402196526527 | Testing Acc: 100.0\n",
            "Epoch: 137 | Training Loss: 0.03195703402161598 | Training Acc: 100.0 | Testing Loss: 0.02287808619439602 | Testing Acc: 100.0\n",
            "Epoch: 138 | Training Loss: 0.03174636885523796 | Training Acc: 100.0 | Testing Loss: 0.02275090664625168 | Testing Acc: 100.0\n",
            "Epoch: 139 | Training Loss: 0.031538765877485275 | Training Acc: 100.0 | Testing Loss: 0.022625792771577835 | Testing Acc: 100.0\n",
            "Epoch: 140 | Training Loss: 0.031334180384874344 | Training Acc: 100.0 | Testing Loss: 0.022502677515149117 | Testing Acc: 100.0\n",
            "Epoch: 141 | Training Loss: 0.03113250993192196 | Training Acc: 100.0 | Testing Loss: 0.022381510585546494 | Testing Acc: 100.0\n",
            "Epoch: 142 | Training Loss: 0.030933696776628494 | Training Acc: 100.0 | Testing Loss: 0.022262098267674446 | Testing Acc: 100.0\n",
            "Epoch: 143 | Training Loss: 0.030737722292542458 | Training Acc: 100.0 | Testing Loss: 0.022144628688693047 | Testing Acc: 100.0\n",
            "Epoch: 144 | Training Loss: 0.030544444918632507 | Training Acc: 100.0 | Testing Loss: 0.022028999403119087 | Testing Acc: 100.0\n",
            "Epoch: 145 | Training Loss: 0.03035384975373745 | Training Acc: 100.0 | Testing Loss: 0.021915163844823837 | Testing Acc: 100.0\n",
            "Epoch: 146 | Training Loss: 0.03016585484147072 | Training Acc: 100.0 | Testing Loss: 0.021803002804517746 | Testing Acc: 100.0\n",
            "Epoch: 147 | Training Loss: 0.029980411753058434 | Training Acc: 100.0 | Testing Loss: 0.021692542359232903 | Testing Acc: 100.0\n",
            "Epoch: 148 | Training Loss: 0.029797542840242386 | Training Acc: 100.0 | Testing Loss: 0.021588677540421486 | Testing Acc: 100.0\n",
            "Epoch: 149 | Training Loss: 0.02961719036102295 | Training Acc: 100.0 | Testing Loss: 0.021483298391103745 | Testing Acc: 100.0\n",
            "Epoch: 150 | Training Loss: 0.029439246281981468 | Training Acc: 100.0 | Testing Loss: 0.021378207951784134 | Testing Acc: 100.0\n",
            "Epoch: 151 | Training Loss: 0.029263606294989586 | Training Acc: 100.0 | Testing Loss: 0.021274041384458542 | Testing Acc: 100.0\n",
            "Epoch: 152 | Training Loss: 0.029090221971273422 | Training Acc: 100.0 | Testing Loss: 0.02117116190493107 | Testing Acc: 100.0\n",
            "Epoch: 153 | Training Loss: 0.02891908958554268 | Training Acc: 100.0 | Testing Loss: 0.021069712936878204 | Testing Acc: 100.0\n",
            "Epoch: 154 | Training Loss: 0.028750116005539894 | Training Acc: 100.0 | Testing Loss: 0.020969616249203682 | Testing Acc: 100.0\n",
            "Epoch: 155 | Training Loss: 0.028583243489265442 | Training Acc: 100.0 | Testing Loss: 0.020870903506875038 | Testing Acc: 100.0\n",
            "Epoch: 156 | Training Loss: 0.028418445959687233 | Training Acc: 100.0 | Testing Loss: 0.020773472264409065 | Testing Acc: 100.0\n",
            "Epoch: 157 | Training Loss: 0.02825568988919258 | Training Acc: 100.0 | Testing Loss: 0.020677343010902405 | Testing Acc: 100.0\n",
            "Epoch: 158 | Training Loss: 0.02809494361281395 | Training Acc: 100.0 | Testing Loss: 0.02058250829577446 | Testing Acc: 100.0\n",
            "Epoch: 159 | Training Loss: 0.02793613076210022 | Training Acc: 100.0 | Testing Loss: 0.020488927140831947 | Testing Acc: 100.0\n",
            "Epoch: 160 | Training Loss: 0.027779320254921913 | Training Acc: 100.0 | Testing Loss: 0.020403865724802017 | Testing Acc: 100.0\n",
            "Epoch: 161 | Training Loss: 0.027624526992440224 | Training Acc: 100.0 | Testing Loss: 0.020315662026405334 | Testing Acc: 100.0\n",
            "Epoch: 162 | Training Loss: 0.027471572160720825 | Training Acc: 100.0 | Testing Loss: 0.02022675797343254 | Testing Acc: 100.0\n",
            "Epoch: 163 | Training Loss: 0.027320418506860733 | Training Acc: 100.0 | Testing Loss: 0.02013823390007019 | Testing Acc: 100.0\n",
            "Epoch: 164 | Training Loss: 0.027170995250344276 | Training Acc: 100.0 | Testing Loss: 0.020050469785928726 | Testing Acc: 100.0\n",
            "Epoch: 165 | Training Loss: 0.027023280039429665 | Training Acc: 100.0 | Testing Loss: 0.01996367983520031 | Testing Acc: 100.0\n",
            "Epoch: 166 | Training Loss: 0.026877274736762047 | Training Acc: 100.0 | Testing Loss: 0.01987788826227188 | Testing Acc: 100.0\n",
            "Epoch: 167 | Training Loss: 0.0267329178750515 | Training Acc: 100.0 | Testing Loss: 0.01979309320449829 | Testing Acc: 100.0\n",
            "Epoch: 168 | Training Loss: 0.026590202003717422 | Training Acc: 100.0 | Testing Loss: 0.019709307700395584 | Testing Acc: 100.0\n",
            "Epoch: 169 | Training Loss: 0.026449060067534447 | Training Acc: 100.0 | Testing Loss: 0.019626539200544357 | Testing Acc: 100.0\n",
            "Epoch: 170 | Training Loss: 0.026309484615921974 | Training Acc: 100.0 | Testing Loss: 0.019544774666428566 | Testing Acc: 100.0\n",
            "Epoch: 171 | Training Loss: 0.026171451434493065 | Training Acc: 100.0 | Testing Loss: 0.019463922828435898 | Testing Acc: 100.0\n",
            "Epoch: 172 | Training Loss: 0.026034926995635033 | Training Acc: 100.0 | Testing Loss: 0.019384007900953293 | Testing Acc: 100.0\n",
            "Epoch: 173 | Training Loss: 0.025899868458509445 | Training Acc: 100.0 | Testing Loss: 0.01930505409836769 | Testing Acc: 100.0\n",
            "Epoch: 174 | Training Loss: 0.025766273960471153 | Training Acc: 100.0 | Testing Loss: 0.019226986914873123 | Testing Acc: 100.0\n",
            "Epoch: 175 | Training Loss: 0.02563411369919777 | Training Acc: 100.0 | Testing Loss: 0.019149813801050186 | Testing Acc: 100.0\n",
            "Epoch: 176 | Training Loss: 0.025503328070044518 | Training Acc: 100.0 | Testing Loss: 0.01907353103160858 | Testing Acc: 100.0\n",
            "Epoch: 177 | Training Loss: 0.025373926386237144 | Training Acc: 100.0 | Testing Loss: 0.018998082727193832 | Testing Acc: 100.0\n",
            "Epoch: 178 | Training Loss: 0.025245875120162964 | Training Acc: 100.0 | Testing Loss: 0.018923453986644745 | Testing Acc: 100.0\n",
            "Epoch: 179 | Training Loss: 0.02511913701891899 | Training Acc: 100.0 | Testing Loss: 0.018849657848477364 | Testing Acc: 100.0\n",
            "Epoch: 180 | Training Loss: 0.02499369904398918 | Training Acc: 100.0 | Testing Loss: 0.018776588141918182 | Testing Acc: 100.0\n",
            "Epoch: 181 | Training Loss: 0.024869557470083237 | Training Acc: 100.0 | Testing Loss: 0.018704364076256752 | Testing Acc: 100.0\n",
            "Epoch: 182 | Training Loss: 0.024746645241975784 | Training Acc: 100.0 | Testing Loss: 0.018632905557751656 | Testing Acc: 100.0\n",
            "Epoch: 183 | Training Loss: 0.02462499588727951 | Training Acc: 100.0 | Testing Loss: 0.018562225624918938 | Testing Acc: 100.0\n",
            "Epoch: 184 | Training Loss: 0.024504568427801132 | Training Acc: 100.0 | Testing Loss: 0.018492316827178 | Testing Acc: 100.0\n",
            "Epoch: 185 | Training Loss: 0.024385318160057068 | Training Acc: 100.0 | Testing Loss: 0.01842312701046467 | Testing Acc: 100.0\n",
            "Epoch: 186 | Training Loss: 0.02426723763346672 | Training Acc: 100.0 | Testing Loss: 0.018354644998908043 | Testing Acc: 100.0\n",
            "Epoch: 187 | Training Loss: 0.02415032684803009 | Training Acc: 100.0 | Testing Loss: 0.018286967650055885 | Testing Acc: 100.0\n",
            "Epoch: 188 | Training Loss: 0.024034541100263596 | Training Acc: 100.0 | Testing Loss: 0.01821982115507126 | Testing Acc: 100.0\n",
            "Epoch: 189 | Training Loss: 0.02391987480223179 | Training Acc: 100.0 | Testing Loss: 0.018153494223952293 | Testing Acc: 100.0\n",
            "Epoch: 190 | Training Loss: 0.023806296288967133 | Training Acc: 100.0 | Testing Loss: 0.018087737262248993 | Testing Acc: 100.0\n",
            "Epoch: 191 | Training Loss: 0.023693814873695374 | Training Acc: 100.0 | Testing Loss: 0.018022671341896057 | Testing Acc: 100.0\n",
            "Epoch: 192 | Training Loss: 0.023582391440868378 | Training Acc: 100.0 | Testing Loss: 0.01795819215476513 | Testing Acc: 100.0\n",
            "Epoch: 193 | Training Loss: 0.023472005501389503 | Training Acc: 100.0 | Testing Loss: 0.017894433811306953 | Testing Acc: 100.0\n",
            "Epoch: 194 | Training Loss: 0.02336263097822666 | Training Acc: 100.0 | Testing Loss: 0.01783119887113571 | Testing Acc: 100.0\n",
            "Epoch: 195 | Training Loss: 0.023254264146089554 | Training Acc: 100.0 | Testing Loss: 0.017768682911992073 | Testing Acc: 100.0\n",
            "Epoch: 196 | Training Loss: 0.023146893829107285 | Training Acc: 100.0 | Testing Loss: 0.017706632614135742 | Testing Acc: 100.0\n",
            "Epoch: 197 | Training Loss: 0.023040499538183212 | Training Acc: 100.0 | Testing Loss: 0.017645293846726418 | Testing Acc: 100.0\n",
            "Epoch: 198 | Training Loss: 0.02293507009744644 | Training Acc: 100.0 | Testing Loss: 0.017584392800927162 | Testing Acc: 100.0\n",
            "Epoch: 199 | Training Loss: 0.022830592468380928 | Training Acc: 100.0 | Testing Loss: 0.017524177208542824 | Testing Acc: 100.0\n",
            "Epoch: 200 | Training Loss: 0.022727059200406075 | Training Acc: 100.0 | Testing Loss: 0.01746436394751072 | Testing Acc: 100.0\n",
            "Epoch: 201 | Training Loss: 0.022624421864748 | Training Acc: 100.0 | Testing Loss: 0.017405234277248383 | Testing Acc: 100.0\n",
            "Epoch: 202 | Training Loss: 0.02252267673611641 | Training Acc: 100.0 | Testing Loss: 0.017346521839499474 | Testing Acc: 100.0\n",
            "Epoch: 203 | Training Loss: 0.022421855479478836 | Training Acc: 100.0 | Testing Loss: 0.017288465052843094 | Testing Acc: 100.0\n",
            "Epoch: 204 | Training Loss: 0.02232189290225506 | Training Acc: 100.0 | Testing Loss: 0.01723083294928074 | Testing Acc: 100.0\n",
            "Epoch: 205 | Training Loss: 0.02222280018031597 | Training Acc: 100.0 | Testing Loss: 0.017173785716295242 | Testing Acc: 100.0\n",
            "Epoch: 206 | Training Loss: 0.022124575451016426 | Training Acc: 100.0 | Testing Loss: 0.01711713895201683 | Testing Acc: 100.0\n",
            "Epoch: 207 | Training Loss: 0.022027181461453438 | Training Acc: 100.0 | Testing Loss: 0.017061077058315277 | Testing Acc: 100.0\n",
            "Epoch: 208 | Training Loss: 0.02193060889840126 | Training Acc: 100.0 | Testing Loss: 0.017005428671836853 | Testing Acc: 100.0\n",
            "Epoch: 209 | Training Loss: 0.02183486893773079 | Training Acc: 100.0 | Testing Loss: 0.01695031300187111 | Testing Acc: 100.0\n",
            "Epoch: 210 | Training Loss: 0.02173994481563568 | Training Acc: 100.0 | Testing Loss: 0.016895607113838196 | Testing Acc: 100.0\n",
            "Epoch: 211 | Training Loss: 0.021645832806825638 | Training Acc: 100.0 | Testing Loss: 0.016840972006320953 | Testing Acc: 100.0\n",
            "Epoch: 212 | Training Loss: 0.021552536636590958 | Training Acc: 100.0 | Testing Loss: 0.016787027940154076 | Testing Acc: 100.0\n",
            "Epoch: 213 | Training Loss: 0.021460002288222313 | Training Acc: 100.0 | Testing Loss: 0.016733698546886444 | Testing Acc: 100.0\n",
            "Epoch: 214 | Training Loss: 0.021368252113461494 | Training Acc: 100.0 | Testing Loss: 0.01668083295226097 | Testing Acc: 100.0\n",
            "Epoch: 215 | Training Loss: 0.02127723954617977 | Training Acc: 100.0 | Testing Loss: 0.016628535464406013 | Testing Acc: 100.0\n",
            "Epoch: 216 | Training Loss: 0.02118697203695774 | Training Acc: 100.0 | Testing Loss: 0.01657651923596859 | Testing Acc: 100.0\n",
            "Epoch: 217 | Training Loss: 0.021097436547279358 | Training Acc: 100.0 | Testing Loss: 0.016525045037269592 | Testing Acc: 100.0\n",
            "Epoch: 218 | Training Loss: 0.021008631214499474 | Training Acc: 100.0 | Testing Loss: 0.016473878175020218 | Testing Acc: 100.0\n",
            "Epoch: 219 | Training Loss: 0.0209205262362957 | Training Acc: 100.0 | Testing Loss: 0.016423234716057777 | Testing Acc: 100.0\n",
            "Epoch: 220 | Training Loss: 0.020833121612668037 | Training Acc: 100.0 | Testing Loss: 0.016372887417674065 | Testing Acc: 100.0\n",
            "Epoch: 221 | Training Loss: 0.02074640989303589 | Training Acc: 100.0 | Testing Loss: 0.016323048621416092 | Testing Acc: 100.0\n",
            "Epoch: 222 | Training Loss: 0.020660383626818657 | Training Acc: 100.0 | Testing Loss: 0.016273682937026024 | Testing Acc: 100.0\n",
            "Epoch: 223 | Training Loss: 0.02057502046227455 | Training Acc: 100.0 | Testing Loss: 0.01622452773153782 | Testing Acc: 100.0\n",
            "Epoch: 224 | Training Loss: 0.020490329712629318 | Training Acc: 100.0 | Testing Loss: 0.016175881028175354 | Testing Acc: 100.0\n",
            "Epoch: 225 | Training Loss: 0.02040630206465721 | Training Acc: 100.0 | Testing Loss: 0.016127530485391617 | Testing Acc: 100.0\n",
            "Epoch: 226 | Training Loss: 0.020322909578680992 | Training Acc: 100.0 | Testing Loss: 0.016079626977443695 | Testing Acc: 100.0\n",
            "Epoch: 227 | Training Loss: 0.0202401801943779 | Training Acc: 100.0 | Testing Loss: 0.016032006591558456 | Testing Acc: 100.0\n",
            "Epoch: 228 | Training Loss: 0.020158078521490097 | Training Acc: 100.0 | Testing Loss: 0.015984803438186646 | Testing Acc: 100.0\n",
            "Epoch: 229 | Training Loss: 0.0200766883790493 | Training Acc: 100.0 | Testing Loss: 0.01593647338449955 | Testing Acc: 100.0\n",
            "Epoch: 230 | Training Loss: 0.019996026530861855 | Training Acc: 100.0 | Testing Loss: 0.015889279544353485 | Testing Acc: 100.0\n",
            "Epoch: 231 | Training Loss: 0.019915979355573654 | Training Acc: 100.0 | Testing Loss: 0.01584271714091301 | Testing Acc: 100.0\n",
            "Epoch: 232 | Training Loss: 0.019836507737636566 | Training Acc: 100.0 | Testing Loss: 0.015796754509210587 | Testing Acc: 100.0\n",
            "Epoch: 233 | Training Loss: 0.019757654517889023 | Training Acc: 100.0 | Testing Loss: 0.015749972313642502 | Testing Acc: 100.0\n",
            "Epoch: 234 | Training Loss: 0.01967952772974968 | Training Acc: 100.0 | Testing Loss: 0.015704257413744926 | Testing Acc: 100.0\n",
            "Epoch: 235 | Training Loss: 0.019601987674832344 | Training Acc: 100.0 | Testing Loss: 0.01565907709300518 | Testing Acc: 100.0\n",
            "Epoch: 236 | Training Loss: 0.01952500268816948 | Training Acc: 100.0 | Testing Loss: 0.01561436615884304 | Testing Acc: 100.0\n",
            "Epoch: 237 | Training Loss: 0.019448580220341682 | Training Acc: 100.0 | Testing Loss: 0.015570161864161491 | Testing Acc: 100.0\n",
            "Epoch: 238 | Training Loss: 0.019372714683413506 | Training Acc: 100.0 | Testing Loss: 0.015526163391768932 | Testing Acc: 100.0\n",
            "Epoch: 239 | Training Loss: 0.0192974004894495 | Training Acc: 100.0 | Testing Loss: 0.015482606366276741 | Testing Acc: 100.0\n",
            "Epoch: 240 | Training Loss: 0.019222628325223923 | Training Acc: 100.0 | Testing Loss: 0.015439258888363838 | Testing Acc: 100.0\n",
            "Epoch: 241 | Training Loss: 0.01914839632809162 | Training Acc: 100.0 | Testing Loss: 0.015396349132061005 | Testing Acc: 100.0\n",
            "Epoch: 242 | Training Loss: 0.019074702635407448 | Training Acc: 100.0 | Testing Loss: 0.015353587456047535 | Testing Acc: 100.0\n",
            "Epoch: 243 | Training Loss: 0.019001519307494164 | Training Acc: 100.0 | Testing Loss: 0.015311224386096 | Testing Acc: 100.0\n",
            "Epoch: 244 | Training Loss: 0.018928878009319305 | Training Acc: 100.0 | Testing Loss: 0.015270857140421867 | Testing Acc: 100.0\n",
            "Epoch: 245 | Training Loss: 0.01885676011443138 | Training Acc: 100.0 | Testing Loss: 0.015229838900268078 | Testing Acc: 100.0\n",
            "Epoch: 246 | Training Loss: 0.018785152584314346 | Training Acc: 100.0 | Testing Loss: 0.015188696794211864 | Testing Acc: 100.0\n",
            "Epoch: 247 | Training Loss: 0.018714042380452156 | Training Acc: 100.0 | Testing Loss: 0.015147572383284569 | Testing Acc: 100.0\n",
            "Epoch: 248 | Training Loss: 0.018643436953425407 | Training Acc: 100.0 | Testing Loss: 0.015106694772839546 | Testing Acc: 100.0\n",
            "Epoch: 249 | Training Loss: 0.018573299050331116 | Training Acc: 100.0 | Testing Loss: 0.015065902844071388 | Testing Acc: 100.0\n",
            "Epoch: 250 | Training Loss: 0.01850365288555622 | Training Acc: 100.0 | Testing Loss: 0.015025442466139793 | Testing Acc: 100.0\n",
            "Epoch: 251 | Training Loss: 0.018434489145874977 | Training Acc: 100.0 | Testing Loss: 0.014985153451561928 | Testing Acc: 100.0\n",
            "Epoch: 252 | Training Loss: 0.01836579293012619 | Training Acc: 100.0 | Testing Loss: 0.01494518667459488 | Testing Acc: 100.0\n",
            "Epoch: 253 | Training Loss: 0.018297575414180756 | Training Acc: 100.0 | Testing Loss: 0.014908039942383766 | Testing Acc: 100.0\n",
            "Epoch: 254 | Training Loss: 0.01822984591126442 | Training Acc: 100.0 | Testing Loss: 0.014869767241179943 | Testing Acc: 100.0\n",
            "Epoch: 255 | Training Loss: 0.018162580206990242 | Training Acc: 100.0 | Testing Loss: 0.014831137843430042 | Testing Acc: 100.0\n",
            "Epoch: 256 | Training Loss: 0.018095767125487328 | Training Acc: 100.0 | Testing Loss: 0.01479237712919712 | Testing Acc: 100.0\n",
            "Epoch: 257 | Training Loss: 0.018029386177659035 | Training Acc: 100.0 | Testing Loss: 0.014753783121705055 | Testing Acc: 100.0\n",
            "Epoch: 258 | Training Loss: 0.017963454127311707 | Training Acc: 100.0 | Testing Loss: 0.014715257100760937 | Testing Acc: 100.0\n",
            "Epoch: 259 | Training Loss: 0.017897943034768105 | Training Acc: 100.0 | Testing Loss: 0.014677001163363457 | Testing Acc: 100.0\n",
            "Epoch: 260 | Training Loss: 0.017832880839705467 | Training Acc: 100.0 | Testing Loss: 0.014638910070061684 | Testing Acc: 100.0\n",
            "Epoch: 261 | Training Loss: 0.017768245190382004 | Training Acc: 100.0 | Testing Loss: 0.014601029455661774 | Testing Acc: 100.0\n",
            "Epoch: 262 | Training Loss: 0.017704011872410774 | Training Acc: 100.0 | Testing Loss: 0.014563438482582569 | Testing Acc: 100.0\n",
            "Epoch: 263 | Training Loss: 0.017640208825469017 | Training Acc: 100.0 | Testing Loss: 0.014526007696986198 | Testing Acc: 100.0\n",
            "Epoch: 264 | Training Loss: 0.01757682114839554 | Training Acc: 100.0 | Testing Loss: 0.014488883316516876 | Testing Acc: 100.0\n",
            "Epoch: 265 | Training Loss: 0.017513830214738846 | Training Acc: 100.0 | Testing Loss: 0.014451895840466022 | Testing Acc: 100.0\n",
            "Epoch: 266 | Training Loss: 0.01745123788714409 | Training Acc: 100.0 | Testing Loss: 0.014415127225220203 | Testing Acc: 100.0\n",
            "Epoch: 267 | Training Loss: 0.017389077693223953 | Training Acc: 100.0 | Testing Loss: 0.01437864638864994 | Testing Acc: 100.0\n",
            "Epoch: 268 | Training Loss: 0.01732727698981762 | Training Acc: 100.0 | Testing Loss: 0.014342340640723705 | Testing Acc: 100.0\n",
            "Epoch: 269 | Training Loss: 0.01726585440337658 | Training Acc: 100.0 | Testing Loss: 0.014306291937828064 | Testing Acc: 100.0\n",
            "Epoch: 270 | Training Loss: 0.01720486208796501 | Training Acc: 100.0 | Testing Loss: 0.014270377345383167 | Testing Acc: 100.0\n",
            "Epoch: 271 | Training Loss: 0.0171442199498415 | Training Acc: 100.0 | Testing Loss: 0.014234739355742931 | Testing Acc: 100.0\n",
            "Epoch: 272 | Training Loss: 0.01708398014307022 | Training Acc: 100.0 | Testing Loss: 0.014199276454746723 | Testing Acc: 100.0\n",
            "Epoch: 273 | Training Loss: 0.017024099826812744 | Training Acc: 100.0 | Testing Loss: 0.01416400820016861 | Testing Acc: 100.0\n",
            "Epoch: 274 | Training Loss: 0.01696460321545601 | Training Acc: 100.0 | Testing Loss: 0.01412899512797594 | Testing Acc: 100.0\n",
            "Epoch: 275 | Training Loss: 0.01690545305609703 | Training Acc: 100.0 | Testing Loss: 0.014093881472945213 | Testing Acc: 100.0\n",
            "Epoch: 276 | Training Loss: 0.016846682876348495 | Training Acc: 100.0 | Testing Loss: 0.014059126377105713 | Testing Acc: 100.0\n",
            "Epoch: 277 | Training Loss: 0.01678827591240406 | Training Acc: 100.0 | Testing Loss: 0.014024622738361359 | Testing Acc: 100.0\n",
            "Epoch: 278 | Training Loss: 0.016730202361941338 | Training Acc: 100.0 | Testing Loss: 0.013990283012390137 | Testing Acc: 100.0\n",
            "Epoch: 279 | Training Loss: 0.016672495752573013 | Training Acc: 100.0 | Testing Loss: 0.013956224545836449 | Testing Acc: 100.0\n",
            "Epoch: 280 | Training Loss: 0.01661529392004013 | Training Acc: 100.0 | Testing Loss: 0.013922047801315784 | Testing Acc: 100.0\n",
            "Epoch: 281 | Training Loss: 0.016558490693569183 | Training Acc: 100.0 | Testing Loss: 0.01388840563595295 | Testing Acc: 100.0\n",
            "Epoch: 282 | Training Loss: 0.016502046957612038 | Training Acc: 100.0 | Testing Loss: 0.01385525893419981 | Testing Acc: 100.0\n",
            "Epoch: 283 | Training Loss: 0.01644599810242653 | Training Acc: 100.0 | Testing Loss: 0.013821849599480629 | Testing Acc: 100.0\n",
            "Epoch: 284 | Training Loss: 0.016390234231948853 | Training Acc: 100.0 | Testing Loss: 0.013788813725113869 | Testing Acc: 100.0\n",
            "Epoch: 285 | Training Loss: 0.016334790736436844 | Training Acc: 100.0 | Testing Loss: 0.013755994848906994 | Testing Acc: 100.0\n",
            "Epoch: 286 | Training Loss: 0.016279680654406548 | Training Acc: 100.0 | Testing Loss: 0.013723427429795265 | Testing Acc: 100.0\n",
            "Epoch: 287 | Training Loss: 0.01622493378818035 | Training Acc: 100.0 | Testing Loss: 0.013691241852939129 | Testing Acc: 100.0\n",
            "Epoch: 288 | Training Loss: 0.016170518472790718 | Training Acc: 100.0 | Testing Loss: 0.013658711686730385 | Testing Acc: 100.0\n",
            "Epoch: 289 | Training Loss: 0.016116388142108917 | Training Acc: 100.0 | Testing Loss: 0.013626573607325554 | Testing Acc: 100.0\n",
            "Epoch: 290 | Training Loss: 0.016062572598457336 | Training Acc: 100.0 | Testing Loss: 0.013594609685242176 | Testing Acc: 100.0\n",
            "Epoch: 291 | Training Loss: 0.016009071841835976 | Training Acc: 100.0 | Testing Loss: 0.013562889769673347 | Testing Acc: 100.0\n",
            "Epoch: 292 | Training Loss: 0.015955926850438118 | Training Acc: 100.0 | Testing Loss: 0.013531552627682686 | Testing Acc: 100.0\n",
            "Epoch: 293 | Training Loss: 0.015903089195489883 | Training Acc: 100.0 | Testing Loss: 0.013499828055500984 | Testing Acc: 100.0\n",
            "Epoch: 294 | Training Loss: 0.015850508585572243 | Training Acc: 100.0 | Testing Loss: 0.01346849650144577 | Testing Acc: 100.0\n",
            "Epoch: 295 | Training Loss: 0.01579824648797512 | Training Acc: 100.0 | Testing Loss: 0.013437328860163689 | Testing Acc: 100.0\n",
            "Epoch: 296 | Training Loss: 0.015746276825666428 | Training Acc: 100.0 | Testing Loss: 0.013406378217041492 | Testing Acc: 100.0\n",
            "Epoch: 297 | Training Loss: 0.015694623813033104 | Training Acc: 100.0 | Testing Loss: 0.013375433161854744 | Testing Acc: 100.0\n",
            "Epoch: 298 | Training Loss: 0.015643347054719925 | Training Acc: 100.0 | Testing Loss: 0.013344256207346916 | Testing Acc: 100.0\n",
            "Epoch: 299 | Training Loss: 0.015592304989695549 | Training Acc: 100.0 | Testing Loss: 0.013313611038029194 | Testing Acc: 100.0\n",
            "Epoch: 300 | Training Loss: 0.015541535802185535 | Training Acc: 100.0 | Testing Loss: 0.0132831409573555 | Testing Acc: 100.0\n",
            "Epoch: 301 | Training Loss: 0.015491071157157421 | Training Acc: 100.0 | Testing Loss: 0.013252894394099712 | Testing Acc: 100.0\n",
            "Epoch: 302 | Training Loss: 0.015440883114933968 | Training Acc: 100.0 | Testing Loss: 0.013222900219261646 | Testing Acc: 100.0\n",
            "Epoch: 303 | Training Loss: 0.01539105735719204 | Training Acc: 100.0 | Testing Loss: 0.013193192891776562 | Testing Acc: 100.0\n",
            "Epoch: 304 | Training Loss: 0.015341460704803467 | Training Acc: 100.0 | Testing Loss: 0.013163186609745026 | Testing Acc: 100.0\n",
            "Epoch: 305 | Training Loss: 0.015292135067284107 | Training Acc: 100.0 | Testing Loss: 0.013133451342582703 | Testing Acc: 100.0\n",
            "Epoch: 306 | Training Loss: 0.015243090689182281 | Training Acc: 100.0 | Testing Loss: 0.013103924691677094 | Testing Acc: 100.0\n",
            "Epoch: 307 | Training Loss: 0.015194309875369072 | Training Acc: 100.0 | Testing Loss: 0.013074638321995735 | Testing Acc: 100.0\n",
            "Epoch: 308 | Training Loss: 0.015145811252295971 | Training Acc: 100.0 | Testing Loss: 0.013045698404312134 | Testing Acc: 100.0\n",
            "Epoch: 309 | Training Loss: 0.015097605995833874 | Training Acc: 100.0 | Testing Loss: 0.013016385026276112 | Testing Acc: 100.0\n",
            "Epoch: 310 | Training Loss: 0.015049616806209087 | Training Acc: 100.0 | Testing Loss: 0.012987370602786541 | Testing Acc: 100.0\n",
            "Epoch: 311 | Training Loss: 0.015001928433775902 | Training Acc: 100.0 | Testing Loss: 0.012958529405295849 | Testing Acc: 100.0\n",
            "Epoch: 312 | Training Loss: 0.014954489655792713 | Training Acc: 100.0 | Testing Loss: 0.01292991079390049 | Testing Acc: 100.0\n",
            "Epoch: 313 | Training Loss: 0.014907312579452991 | Training Acc: 100.0 | Testing Loss: 0.012901355512440205 | Testing Acc: 100.0\n",
            "Epoch: 314 | Training Loss: 0.014860434457659721 | Training Acc: 100.0 | Testing Loss: 0.012873224914073944 | Testing Acc: 100.0\n",
            "Epoch: 315 | Training Loss: 0.01481376402080059 | Training Acc: 100.0 | Testing Loss: 0.012844616547226906 | Testing Acc: 100.0\n",
            "Epoch: 316 | Training Loss: 0.014767350628972054 | Training Acc: 100.0 | Testing Loss: 0.012816289439797401 | Testing Acc: 100.0\n",
            "Epoch: 317 | Training Loss: 0.014721168205142021 | Training Acc: 100.0 | Testing Loss: 0.012788240797817707 | Testing Acc: 100.0\n",
            "Epoch: 318 | Training Loss: 0.014675619080662727 | Training Acc: 100.0 | Testing Loss: 0.012780291959643364 | Testing Acc: 100.0\n",
            "Epoch: 319 | Training Loss: 0.014630241319537163 | Training Acc: 100.0 | Testing Loss: 0.012764242477715015 | Testing Acc: 100.0\n",
            "Epoch: 320 | Training Loss: 0.014585179276764393 | Training Acc: 100.0 | Testing Loss: 0.012722881510853767 | Testing Acc: 100.0\n",
            "Epoch: 321 | Training Loss: 0.014540420845150948 | Training Acc: 100.0 | Testing Loss: 0.012707710266113281 | Testing Acc: 100.0\n",
            "Epoch: 322 | Training Loss: 0.014495755545794964 | Training Acc: 100.0 | Testing Loss: 0.012687923386693 | Testing Acc: 100.0\n",
            "Epoch: 323 | Training Loss: 0.014451439492404461 | Training Acc: 100.0 | Testing Loss: 0.012664863839745522 | Testing Acc: 100.0\n",
            "Epoch: 324 | Training Loss: 0.014407324604690075 | Training Acc: 100.0 | Testing Loss: 0.012620759196579456 | Testing Acc: 100.0\n",
            "Epoch: 325 | Training Loss: 0.01436352264136076 | Training Acc: 100.0 | Testing Loss: 0.012603780254721642 | Testing Acc: 100.0\n",
            "Epoch: 326 | Training Loss: 0.01431984268128872 | Training Acc: 100.0 | Testing Loss: 0.012583081610500813 | Testing Acc: 100.0\n",
            "Epoch: 327 | Training Loss: 0.01427643932402134 | Training Acc: 100.0 | Testing Loss: 0.012559756636619568 | Testing Acc: 100.0\n",
            "Epoch: 328 | Training Loss: 0.014233206398785114 | Training Acc: 100.0 | Testing Loss: 0.01253539603203535 | Testing Acc: 100.0\n",
            "Epoch: 329 | Training Loss: 0.014190293848514557 | Training Acc: 100.0 | Testing Loss: 0.012491399422287941 | Testing Acc: 100.0\n",
            "Epoch: 330 | Training Loss: 0.014147678390145302 | Training Acc: 100.0 | Testing Loss: 0.0124738160520792 | Testing Acc: 100.0\n",
            "Epoch: 331 | Training Loss: 0.014105096459388733 | Training Acc: 100.0 | Testing Loss: 0.012452712282538414 | Testing Acc: 100.0\n",
            "Epoch: 332 | Training Loss: 0.014062765054404736 | Training Acc: 100.0 | Testing Loss: 0.0124296760186553 | Testing Acc: 100.0\n",
            "Epoch: 333 | Training Loss: 0.014020659029483795 | Training Acc: 100.0 | Testing Loss: 0.012405894696712494 | Testing Acc: 100.0\n",
            "Epoch: 334 | Training Loss: 0.01397885661572218 | Training Acc: 100.0 | Testing Loss: 0.01238110288977623 | Testing Acc: 100.0\n",
            "Epoch: 335 | Training Loss: 0.01393726747483015 | Training Acc: 100.0 | Testing Loss: 0.012337345629930496 | Testing Acc: 100.0\n",
            "Epoch: 336 | Training Loss: 0.01389584131538868 | Training Acc: 100.0 | Testing Loss: 0.0123200174421072 | Testing Acc: 100.0\n",
            "Epoch: 337 | Training Loss: 0.01385458279401064 | Training Acc: 100.0 | Testing Loss: 0.012299573048949242 | Testing Acc: 100.0\n",
            "Epoch: 338 | Training Loss: 0.013813639059662819 | Training Acc: 100.0 | Testing Loss: 0.012276758439838886 | Testing Acc: 100.0\n",
            "Epoch: 339 | Training Loss: 0.013772852718830109 | Training Acc: 100.0 | Testing Loss: 0.012253180146217346 | Testing Acc: 100.0\n",
            "Epoch: 340 | Training Loss: 0.013732271268963814 | Training Acc: 100.0 | Testing Loss: 0.012228935025632381 | Testing Acc: 100.0\n",
            "Epoch: 341 | Training Loss: 0.013691996224224567 | Training Acc: 100.0 | Testing Loss: 0.012186076492071152 | Testing Acc: 100.0\n",
            "Epoch: 342 | Training Loss: 0.013652116060256958 | Training Acc: 100.0 | Testing Loss: 0.012168423272669315 | Testing Acc: 100.0\n",
            "Epoch: 343 | Training Loss: 0.013612302951514721 | Training Acc: 100.0 | Testing Loss: 0.012147730216383934 | Testing Acc: 100.0\n",
            "Epoch: 344 | Training Loss: 0.01357270311564207 | Training Acc: 100.0 | Testing Loss: 0.012125415727496147 | Testing Acc: 100.0\n",
            "Epoch: 345 | Training Loss: 0.013533344492316246 | Training Acc: 100.0 | Testing Loss: 0.012101325206458569 | Testing Acc: 100.0\n",
            "Epoch: 346 | Training Loss: 0.013494242914021015 | Training Acc: 100.0 | Testing Loss: 0.012076852843165398 | Testing Acc: 100.0\n",
            "Epoch: 347 | Training Loss: 0.01345529593527317 | Training Acc: 100.0 | Testing Loss: 0.012052561156451702 | Testing Acc: 100.0\n",
            "Epoch: 348 | Training Loss: 0.013416538946330547 | Training Acc: 100.0 | Testing Loss: 0.012028415687382221 | Testing Acc: 100.0\n",
            "Epoch: 349 | Training Loss: 0.013378019444644451 | Training Acc: 100.0 | Testing Loss: 0.012004665099084377 | Testing Acc: 100.0\n",
            "Epoch: 350 | Training Loss: 0.013339693658053875 | Training Acc: 100.0 | Testing Loss: 0.01198048796504736 | Testing Acc: 100.0\n",
            "Epoch: 351 | Training Loss: 0.013301514089107513 | Training Acc: 100.0 | Testing Loss: 0.011956480331718922 | Testing Acc: 100.0\n",
            "Epoch: 352 | Training Loss: 0.013263517990708351 | Training Acc: 100.0 | Testing Loss: 0.011932668276131153 | Testing Acc: 100.0\n",
            "Epoch: 353 | Training Loss: 0.01322578638792038 | Training Acc: 100.0 | Testing Loss: 0.011891934089362621 | Testing Acc: 100.0\n",
            "Epoch: 354 | Training Loss: 0.013188211247324944 | Training Acc: 100.0 | Testing Loss: 0.011874882504343987 | Testing Acc: 100.0\n",
            "Epoch: 355 | Training Loss: 0.013150771148502827 | Training Acc: 100.0 | Testing Loss: 0.011855041608214378 | Testing Acc: 100.0\n",
            "Epoch: 356 | Training Loss: 0.013113526627421379 | Training Acc: 100.0 | Testing Loss: 0.011833768337965012 | Testing Acc: 100.0\n",
            "Epoch: 357 | Training Loss: 0.013076463714241982 | Training Acc: 100.0 | Testing Loss: 0.011811850592494011 | Testing Acc: 100.0\n",
            "Epoch: 358 | Training Loss: 0.01303962804377079 | Training Acc: 100.0 | Testing Loss: 0.011789077892899513 | Testing Acc: 100.0\n",
            "Epoch: 359 | Training Loss: 0.013002919033169746 | Training Acc: 100.0 | Testing Loss: 0.011766267940402031 | Testing Acc: 100.0\n",
            "Epoch: 360 | Training Loss: 0.012966392561793327 | Training Acc: 100.0 | Testing Loss: 0.011743390001356602 | Testing Acc: 100.0\n",
            "Epoch: 361 | Training Loss: 0.012930040247738361 | Training Acc: 100.0 | Testing Loss: 0.011720795184373856 | Testing Acc: 100.0\n",
            "Epoch: 362 | Training Loss: 0.012893930077552795 | Training Acc: 100.0 | Testing Loss: 0.011697769165039062 | Testing Acc: 100.0\n",
            "Epoch: 363 | Training Loss: 0.012857923284173012 | Training Acc: 100.0 | Testing Loss: 0.01167491264641285 | Testing Acc: 100.0\n",
            "Epoch: 364 | Training Loss: 0.012822093442082405 | Training Acc: 100.0 | Testing Loss: 0.011652186512947083 | Testing Acc: 100.0\n",
            "Epoch: 365 | Training Loss: 0.012786418199539185 | Training Acc: 100.0 | Testing Loss: 0.011629804968833923 | Testing Acc: 100.0\n",
            "Epoch: 366 | Training Loss: 0.012751033529639244 | Training Acc: 100.0 | Testing Loss: 0.011606520041823387 | Testing Acc: 100.0\n",
            "Epoch: 367 | Training Loss: 0.01271575503051281 | Training Acc: 100.0 | Testing Loss: 0.011583558283746243 | Testing Acc: 100.0\n",
            "Epoch: 368 | Training Loss: 0.012680662795901299 | Training Acc: 100.0 | Testing Loss: 0.011560792103409767 | Testing Acc: 100.0\n",
            "Epoch: 369 | Training Loss: 0.012645711190998554 | Training Acc: 100.0 | Testing Loss: 0.011538724415004253 | Testing Acc: 100.0\n",
            "Epoch: 370 | Training Loss: 0.012611018493771553 | Training Acc: 100.0 | Testing Loss: 0.011516359634697437 | Testing Acc: 100.0\n",
            "Epoch: 371 | Training Loss: 0.012576403096318245 | Training Acc: 100.0 | Testing Loss: 0.011493735946714878 | Testing Acc: 100.0\n",
            "Epoch: 372 | Training Loss: 0.01254195161163807 | Training Acc: 100.0 | Testing Loss: 0.011471342295408249 | Testing Acc: 100.0\n",
            "Epoch: 373 | Training Loss: 0.012507679872214794 | Training Acc: 100.0 | Testing Loss: 0.011449361220002174 | Testing Acc: 100.0\n",
            "Epoch: 374 | Training Loss: 0.012473593465983868 | Training Acc: 100.0 | Testing Loss: 0.01142763439565897 | Testing Acc: 100.0\n",
            "Epoch: 375 | Training Loss: 0.01243964210152626 | Training Acc: 100.0 | Testing Loss: 0.011405534110963345 | Testing Acc: 100.0\n",
            "Epoch: 376 | Training Loss: 0.012405850924551487 | Training Acc: 100.0 | Testing Loss: 0.011384100653231144 | Testing Acc: 100.0\n",
            "Epoch: 377 | Training Loss: 0.012372201308608055 | Training Acc: 100.0 | Testing Loss: 0.011362146586179733 | Testing Acc: 100.0\n",
            "Epoch: 378 | Training Loss: 0.01233871839940548 | Training Acc: 100.0 | Testing Loss: 0.011340571567416191 | Testing Acc: 100.0\n",
            "Epoch: 379 | Training Loss: 0.01230542827397585 | Training Acc: 100.0 | Testing Loss: 0.011318715289235115 | Testing Acc: 100.0\n",
            "Epoch: 380 | Training Loss: 0.012272228486835957 | Training Acc: 100.0 | Testing Loss: 0.011296993121504784 | Testing Acc: 100.0\n",
            "Epoch: 381 | Training Loss: 0.012239200994372368 | Training Acc: 100.0 | Testing Loss: 0.011275451630353928 | Testing Acc: 100.0\n",
            "Epoch: 382 | Training Loss: 0.012206315994262695 | Training Acc: 100.0 | Testing Loss: 0.011254046112298965 | Testing Acc: 100.0\n",
            "Epoch: 383 | Training Loss: 0.01217365637421608 | Training Acc: 100.0 | Testing Loss: 0.011233462020754814 | Testing Acc: 100.0\n",
            "Epoch: 384 | Training Loss: 0.012141083367168903 | Training Acc: 100.0 | Testing Loss: 0.011211909353733063 | Testing Acc: 100.0\n",
            "Epoch: 385 | Training Loss: 0.012108632363379002 | Training Acc: 100.0 | Testing Loss: 0.01119057647883892 | Testing Acc: 100.0\n",
            "Epoch: 386 | Training Loss: 0.012076335959136486 | Training Acc: 100.0 | Testing Loss: 0.011169394478201866 | Testing Acc: 100.0\n",
            "Epoch: 387 | Training Loss: 0.012044223956763744 | Training Acc: 100.0 | Testing Loss: 0.011148558929562569 | Testing Acc: 100.0\n",
            "Epoch: 388 | Training Loss: 0.012012260966002941 | Training Acc: 100.0 | Testing Loss: 0.01112742256373167 | Testing Acc: 100.0\n",
            "Epoch: 389 | Training Loss: 0.011980411596596241 | Training Acc: 100.0 | Testing Loss: 0.011106457561254501 | Testing Acc: 100.0\n",
            "Epoch: 390 | Training Loss: 0.011948722414672375 | Training Acc: 100.0 | Testing Loss: 0.01108614169061184 | Testing Acc: 100.0\n",
            "Epoch: 391 | Training Loss: 0.0119171729311347 | Training Acc: 100.0 | Testing Loss: 0.011065555736422539 | Testing Acc: 100.0\n",
            "Epoch: 392 | Training Loss: 0.011885817162692547 | Training Acc: 100.0 | Testing Loss: 0.011044655926525593 | Testing Acc: 100.0\n",
            "Epoch: 393 | Training Loss: 0.011854549869894981 | Training Acc: 100.0 | Testing Loss: 0.011024001985788345 | Testing Acc: 100.0\n",
            "Epoch: 394 | Training Loss: 0.011823415756225586 | Training Acc: 100.0 | Testing Loss: 0.0110034653916955 | Testing Acc: 100.0\n",
            "Epoch: 395 | Training Loss: 0.01179240457713604 | Training Acc: 100.0 | Testing Loss: 0.010983014479279518 | Testing Acc: 100.0\n",
            "Epoch: 396 | Training Loss: 0.011761569418013096 | Training Acc: 100.0 | Testing Loss: 0.010962913744151592 | Testing Acc: 100.0\n",
            "Epoch: 397 | Training Loss: 0.011730887927114964 | Training Acc: 100.0 | Testing Loss: 0.010942958295345306 | Testing Acc: 100.0\n",
            "Epoch: 398 | Training Loss: 0.011700311675667763 | Training Acc: 100.0 | Testing Loss: 0.010922560468316078 | Testing Acc: 100.0\n",
            "Epoch: 399 | Training Loss: 0.011669843457639217 | Training Acc: 100.0 | Testing Loss: 0.010902343317866325 | Testing Acc: 100.0\n",
            "Epoch: 400 | Training Loss: 0.011639533564448357 | Training Acc: 100.0 | Testing Loss: 0.010882233269512653 | Testing Acc: 100.0\n",
            "Epoch: 401 | Training Loss: 0.011609377339482307 | Training Acc: 100.0 | Testing Loss: 0.010862487368285656 | Testing Acc: 100.0\n",
            "Epoch: 402 | Training Loss: 0.011579347774386406 | Training Acc: 100.0 | Testing Loss: 0.010842368006706238 | Testing Acc: 100.0\n",
            "Epoch: 403 | Training Loss: 0.011549401096999645 | Training Acc: 100.0 | Testing Loss: 0.010822420008480549 | Testing Acc: 100.0\n",
            "Epoch: 404 | Training Loss: 0.011519618332386017 | Training Acc: 100.0 | Testing Loss: 0.010802635923027992 | Testing Acc: 100.0\n",
            "Epoch: 405 | Training Loss: 0.011489949189126492 | Training Acc: 100.0 | Testing Loss: 0.010783403180539608 | Testing Acc: 100.0\n",
            "Epoch: 406 | Training Loss: 0.011460459791123867 | Training Acc: 100.0 | Testing Loss: 0.01076388917863369 | Testing Acc: 100.0\n",
            "Epoch: 407 | Training Loss: 0.011431061662733555 | Training Acc: 100.0 | Testing Loss: 0.010744069702923298 | Testing Acc: 100.0\n",
            "Epoch: 408 | Training Loss: 0.011401766911149025 | Training Acc: 100.0 | Testing Loss: 0.010724414139986038 | Testing Acc: 100.0\n",
            "Epoch: 409 | Training Loss: 0.011372605338692665 | Training Acc: 100.0 | Testing Loss: 0.010704928077757359 | Testing Acc: 100.0\n",
            "Epoch: 410 | Training Loss: 0.011343562975525856 | Training Acc: 100.0 | Testing Loss: 0.010685580782592297 | Testing Acc: 100.0\n",
            "Epoch: 411 | Training Loss: 0.01131468452513218 | Training Acc: 100.0 | Testing Loss: 0.010666498914361 | Testing Acc: 100.0\n",
            "Epoch: 412 | Training Loss: 0.011285927146673203 | Training Acc: 100.0 | Testing Loss: 0.010647034272551537 | Testing Acc: 100.0\n",
            "Epoch: 413 | Training Loss: 0.011257237754762173 | Training Acc: 100.0 | Testing Loss: 0.010628258809447289 | Testing Acc: 100.0\n",
            "Epoch: 414 | Training Loss: 0.011228693649172783 | Training Acc: 100.0 | Testing Loss: 0.010609032586216927 | Testing Acc: 100.0\n",
            "Epoch: 415 | Training Loss: 0.01120026595890522 | Training Acc: 100.0 | Testing Loss: 0.010589919053018093 | Testing Acc: 100.0\n",
            "Epoch: 416 | Training Loss: 0.011172017082571983 | Training Acc: 100.0 | Testing Loss: 0.010571170598268509 | Testing Acc: 100.0\n",
            "Epoch: 417 | Training Loss: 0.011143838986754417 | Training Acc: 100.0 | Testing Loss: 0.010552030988037586 | Testing Acc: 100.0\n",
            "Epoch: 418 | Training Loss: 0.011115750297904015 | Training Acc: 100.0 | Testing Loss: 0.010533083230257034 | Testing Acc: 100.0\n",
            "Epoch: 419 | Training Loss: 0.011087799444794655 | Training Acc: 100.0 | Testing Loss: 0.010514264926314354 | Testing Acc: 100.0\n",
            "Epoch: 420 | Training Loss: 0.011059961281716824 | Training Acc: 100.0 | Testing Loss: 0.010495544411242008 | Testing Acc: 100.0\n",
            "Epoch: 421 | Training Loss: 0.01103230006992817 | Training Acc: 100.0 | Testing Loss: 0.010477116331458092 | Testing Acc: 100.0\n",
            "Epoch: 422 | Training Loss: 0.011004707776010036 | Training Acc: 100.0 | Testing Loss: 0.010458828881382942 | Testing Acc: 100.0\n",
            "Epoch: 423 | Training Loss: 0.010977220721542835 | Training Acc: 100.0 | Testing Loss: 0.010440129786729813 | Testing Acc: 100.0\n",
            "Epoch: 424 | Training Loss: 0.010949834249913692 | Training Acc: 100.0 | Testing Loss: 0.010421577841043472 | Testing Acc: 100.0\n",
            "Epoch: 425 | Training Loss: 0.01092256885021925 | Training Acc: 100.0 | Testing Loss: 0.010403111577033997 | Testing Acc: 100.0\n",
            "Epoch: 426 | Training Loss: 0.010895480401813984 | Training Acc: 100.0 | Testing Loss: 0.01038497593253851 | Testing Acc: 100.0\n",
            "Epoch: 427 | Training Loss: 0.010868440382182598 | Training Acc: 100.0 | Testing Loss: 0.010366509668529034 | Testing Acc: 100.0\n",
            "Epoch: 428 | Training Loss: 0.010841509327292442 | Training Acc: 100.0 | Testing Loss: 0.010348208248615265 | Testing Acc: 100.0\n",
            "Epoch: 429 | Training Loss: 0.010814697481691837 | Training Acc: 100.0 | Testing Loss: 0.01033000834286213 | Testing Acc: 100.0\n",
            "Epoch: 430 | Training Loss: 0.01078798808157444 | Training Acc: 100.0 | Testing Loss: 0.010311948135495186 | Testing Acc: 100.0\n",
            "Epoch: 431 | Training Loss: 0.01076143141835928 | Training Acc: 100.0 | Testing Loss: 0.010294598527252674 | Testing Acc: 100.0\n",
            "Epoch: 432 | Training Loss: 0.010734951123595238 | Training Acc: 100.0 | Testing Loss: 0.01027634646743536 | Testing Acc: 100.0\n",
            "Epoch: 433 | Training Loss: 0.01070857048034668 | Training Acc: 100.0 | Testing Loss: 0.010258289985358715 | Testing Acc: 100.0\n",
            "Epoch: 434 | Training Loss: 0.010682301595807076 | Training Acc: 100.0 | Testing Loss: 0.010240359231829643 | Testing Acc: 100.0\n",
            "Epoch: 435 | Training Loss: 0.010656123980879784 | Training Acc: 100.0 | Testing Loss: 0.010222540237009525 | Testing Acc: 100.0\n",
            "Epoch: 436 | Training Loss: 0.010631606914103031 | Training Acc: 100.0 | Testing Loss: 0.010180703364312649 | Testing Acc: 100.0\n",
            "Epoch: 437 | Training Loss: 0.010605817660689354 | Training Acc: 100.0 | Testing Loss: 0.010172585025429726 | Testing Acc: 100.0\n",
            "Epoch: 438 | Training Loss: 0.01058162096887827 | Training Acc: 100.0 | Testing Loss: 0.010136999189853668 | Testing Acc: 100.0\n",
            "Epoch: 439 | Training Loss: 0.010556151159107685 | Training Acc: 100.0 | Testing Loss: 0.0101084029302001 | Testing Acc: 100.0\n",
            "Epoch: 440 | Training Loss: 0.010531921871006489 | Training Acc: 100.0 | Testing Loss: 0.010108474642038345 | Testing Acc: 100.0\n",
            "Epoch: 441 | Training Loss: 0.010506981052458286 | Training Acc: 100.0 | Testing Loss: 0.010077605955302715 | Testing Acc: 100.0\n",
            "Epoch: 442 | Training Loss: 0.010482558980584145 | Training Acc: 100.0 | Testing Loss: 0.010076804086565971 | Testing Acc: 100.0\n",
            "Epoch: 443 | Training Loss: 0.010458255186676979 | Training Acc: 100.0 | Testing Loss: 0.010045407339930534 | Testing Acc: 100.0\n",
            "Epoch: 444 | Training Loss: 0.010433603078126907 | Training Acc: 100.0 | Testing Loss: 0.010043542832136154 | Testing Acc: 100.0\n",
            "Epoch: 445 | Training Loss: 0.010409903712570667 | Training Acc: 100.0 | Testing Loss: 0.010011906735599041 | Testing Acc: 100.0\n",
            "Epoch: 446 | Training Loss: 0.010385235771536827 | Training Acc: 100.0 | Testing Loss: 0.009986860677599907 | Testing Acc: 100.0\n",
            "Epoch: 447 | Training Loss: 0.010361739434301853 | Training Acc: 100.0 | Testing Loss: 0.009988641366362572 | Testing Acc: 100.0\n",
            "Epoch: 448 | Training Loss: 0.01033751666545868 | Training Acc: 100.0 | Testing Loss: 0.009959472343325615 | Testing Acc: 100.0\n",
            "Epoch: 449 | Training Loss: 0.010313675738871098 | Training Acc: 100.0 | Testing Loss: 0.009959199465811253 | Testing Acc: 100.0\n",
            "Epoch: 450 | Training Loss: 0.01029018685221672 | Training Acc: 100.0 | Testing Loss: 0.00992918387055397 | Testing Acc: 100.0\n",
            "Epoch: 451 | Training Loss: 0.010266060009598732 | Training Acc: 100.0 | Testing Loss: 0.009928455576300621 | Testing Acc: 100.0\n",
            "Epoch: 452 | Training Loss: 0.010243210941553116 | Training Acc: 100.0 | Testing Loss: 0.009897775016725063 | Testing Acc: 100.0\n",
            "Epoch: 453 | Training Loss: 0.01021919772028923 | Training Acc: 100.0 | Testing Loss: 0.00987308006733656 | Testing Acc: 100.0\n",
            "Epoch: 454 | Training Loss: 0.010196078568696976 | Training Acc: 100.0 | Testing Loss: 0.009875306859612465 | Testing Acc: 100.0\n",
            "Epoch: 455 | Training Loss: 0.010172772221267223 | Training Acc: 100.0 | Testing Loss: 0.009847063571214676 | Testing Acc: 100.0\n",
            "Epoch: 456 | Training Loss: 0.010149272158741951 | Training Acc: 100.0 | Testing Loss: 0.009847769513726234 | Testing Acc: 100.0\n",
            "Epoch: 457 | Training Loss: 0.010126733221113682 | Training Acc: 100.0 | Testing Loss: 0.00981810037046671 | Testing Acc: 100.0\n",
            "Epoch: 458 | Training Loss: 0.010103196837008 | Training Acc: 100.0 | Testing Loss: 0.0097940843552351 | Testing Acc: 100.0\n",
            "Epoch: 459 | Training Loss: 0.010080662555992603 | Training Acc: 100.0 | Testing Loss: 0.009796621277928352 | Testing Acc: 100.0\n",
            "Epoch: 460 | Training Loss: 0.010057661682367325 | Training Acc: 100.0 | Testing Loss: 0.009768946096301079 | Testing Acc: 100.0\n",
            "Epoch: 461 | Training Loss: 0.010034693405032158 | Training Acc: 100.0 | Testing Loss: 0.00976970512419939 | Testing Acc: 100.0\n",
            "Epoch: 462 | Training Loss: 0.010012513026595116 | Training Acc: 100.0 | Testing Loss: 0.009740883484482765 | Testing Acc: 100.0\n",
            "Epoch: 463 | Training Loss: 0.009989424608647823 | Training Acc: 100.0 | Testing Loss: 0.009717222303152084 | Testing Acc: 100.0\n",
            "Epoch: 464 | Training Loss: 0.009967362508177757 | Training Acc: 100.0 | Testing Loss: 0.009719835594296455 | Testing Acc: 100.0\n",
            "Epoch: 465 | Training Loss: 0.009944736026227474 | Training Acc: 100.0 | Testing Loss: 0.00969256367534399 | Testing Acc: 100.0\n",
            "Epoch: 466 | Training Loss: 0.009922249242663383 | Training Acc: 100.0 | Testing Loss: 0.009693010710179806 | Testing Acc: 100.0\n",
            "Epoch: 467 | Training Loss: 0.009900374338030815 | Training Acc: 100.0 | Testing Loss: 0.009664582088589668 | Testing Acc: 100.0\n",
            "Epoch: 468 | Training Loss: 0.009877723641693592 | Training Acc: 100.0 | Testing Loss: 0.009642144665122032 | Testing Acc: 100.0\n",
            "Epoch: 469 | Training Loss: 0.00985613651573658 | Training Acc: 100.0 | Testing Loss: 0.009644652716815472 | Testing Acc: 100.0\n",
            "Epoch: 470 | Training Loss: 0.009833892807364464 | Training Acc: 100.0 | Testing Loss: 0.009617740288376808 | Testing Acc: 100.0\n",
            "Epoch: 471 | Training Loss: 0.009811853058636189 | Training Acc: 100.0 | Testing Loss: 0.00961819477379322 | Testing Acc: 100.0\n",
            "Epoch: 472 | Training Loss: 0.00979035347700119 | Training Acc: 100.0 | Testing Loss: 0.009590167552232742 | Testing Acc: 100.0\n",
            "Epoch: 473 | Training Loss: 0.009768103249371052 | Training Acc: 100.0 | Testing Loss: 0.009567396715283394 | Testing Acc: 100.0\n",
            "Epoch: 474 | Training Loss: 0.009746906347572803 | Training Acc: 100.0 | Testing Loss: 0.00957068707793951 | Testing Acc: 100.0\n",
            "Epoch: 475 | Training Loss: 0.009725124575197697 | Training Acc: 100.0 | Testing Loss: 0.009544400498270988 | Testing Acc: 100.0\n",
            "Epoch: 476 | Training Loss: 0.009703455492854118 | Training Acc: 100.0 | Testing Loss: 0.00954477395862341 | Testing Acc: 100.0\n",
            "Epoch: 477 | Training Loss: 0.009682326577603817 | Training Acc: 100.0 | Testing Loss: 0.009517044760286808 | Testing Acc: 100.0\n",
            "Epoch: 478 | Training Loss: 0.009660487994551659 | Training Acc: 100.0 | Testing Loss: 0.009494560770690441 | Testing Acc: 100.0\n",
            "Epoch: 479 | Training Loss: 0.009639691561460495 | Training Acc: 100.0 | Testing Loss: 0.009497366845607758 | Testing Acc: 100.0\n",
            "Epoch: 480 | Training Loss: 0.009618234820663929 | Training Acc: 100.0 | Testing Loss: 0.009471584111452103 | Testing Acc: 100.0\n",
            "Epoch: 481 | Training Loss: 0.00959705375134945 | Training Acc: 100.0 | Testing Loss: 0.009472481906414032 | Testing Acc: 100.0\n",
            "Epoch: 482 | Training Loss: 0.009576275013387203 | Training Acc: 100.0 | Testing Loss: 0.009445073083043098 | Testing Acc: 100.0\n",
            "Epoch: 483 | Training Loss: 0.009554829448461533 | Training Acc: 100.0 | Testing Loss: 0.009422853589057922 | Testing Acc: 100.0\n",
            "Epoch: 484 | Training Loss: 0.009534399025142193 | Training Acc: 100.0 | Testing Loss: 0.009425697848200798 | Testing Acc: 100.0\n",
            "Epoch: 485 | Training Loss: 0.00951329804956913 | Training Acc: 100.0 | Testing Loss: 0.009400051087141037 | Testing Acc: 100.0\n",
            "Epoch: 486 | Training Loss: 0.009492455050349236 | Training Acc: 100.0 | Testing Loss: 0.009400906041264534 | Testing Acc: 100.0\n",
            "Epoch: 487 | Training Loss: 0.009472093544900417 | Training Acc: 100.0 | Testing Loss: 0.009373901411890984 | Testing Acc: 100.0\n",
            "Epoch: 488 | Training Loss: 0.009451031684875488 | Training Acc: 100.0 | Testing Loss: 0.009351990185678005 | Testing Acc: 100.0\n",
            "Epoch: 489 | Training Loss: 0.009431020356714725 | Training Acc: 100.0 | Testing Loss: 0.009355215355753899 | Testing Acc: 100.0\n",
            "Epoch: 490 | Training Loss: 0.009410266764461994 | Training Acc: 100.0 | Testing Loss: 0.009329845197498798 | Testing Acc: 100.0\n",
            "Epoch: 491 | Training Loss: 0.009389778599143028 | Training Acc: 100.0 | Testing Loss: 0.00933051947504282 | Testing Acc: 100.0\n",
            "Epoch: 492 | Training Loss: 0.009369793348014355 | Training Acc: 100.0 | Testing Loss: 0.009304176084697247 | Testing Acc: 100.0\n",
            "Epoch: 493 | Training Loss: 0.009349093772470951 | Training Acc: 100.0 | Testing Loss: 0.009282478131353855 | Testing Acc: 100.0\n",
            "Epoch: 494 | Training Loss: 0.009329432621598244 | Training Acc: 100.0 | Testing Loss: 0.009285264648497105 | Testing Acc: 100.0\n",
            "Epoch: 495 | Training Loss: 0.009309020824730396 | Training Acc: 100.0 | Testing Loss: 0.009260259568691254 | Testing Acc: 100.0\n",
            "Epoch: 496 | Training Loss: 0.00928894616663456 | Training Acc: 100.0 | Testing Loss: 0.009261421859264374 | Testing Acc: 100.0\n",
            "Epoch: 497 | Training Loss: 0.009269250556826591 | Training Acc: 100.0 | Testing Loss: 0.009235192090272903 | Testing Acc: 100.0\n",
            "Epoch: 498 | Training Loss: 0.009248915128409863 | Training Acc: 100.0 | Testing Loss: 0.009214043617248535 | Testing Acc: 100.0\n",
            "Epoch: 499 | Training Loss: 0.009229637682437897 | Training Acc: 100.0 | Testing Loss: 0.009216728620231152 | Testing Acc: 100.0\n",
            "Epoch: 500 | Training Loss: 0.009209576062858105 | Training Acc: 100.0 | Testing Loss: 0.009192039258778095 | Testing Acc: 100.0\n",
            "Epoch: 501 | Training Loss: 0.009189853444695473 | Training Acc: 100.0 | Testing Loss: 0.009192755445837975 | Testing Acc: 100.0\n",
            "Epoch: 502 | Training Loss: 0.009170494973659515 | Training Acc: 100.0 | Testing Loss: 0.009166995994746685 | Testing Acc: 100.0\n",
            "Epoch: 503 | Training Loss: 0.009150492958724499 | Training Acc: 100.0 | Testing Loss: 0.009145960211753845 | Testing Acc: 100.0\n",
            "Epoch: 504 | Training Loss: 0.009131536819040775 | Training Acc: 100.0 | Testing Loss: 0.009149244986474514 | Testing Acc: 100.0\n",
            "Epoch: 505 | Training Loss: 0.009111897088587284 | Training Acc: 100.0 | Testing Loss: 0.009125081822276115 | Testing Acc: 100.0\n",
            "Epoch: 506 | Training Loss: 0.009092500433325768 | Training Acc: 100.0 | Testing Loss: 0.009125692769885063 | Testing Acc: 100.0\n",
            "Epoch: 507 | Training Loss: 0.009073439985513687 | Training Acc: 100.0 | Testing Loss: 0.009100127965211868 | Testing Acc: 100.0\n",
            "Epoch: 508 | Training Loss: 0.009053800255060196 | Training Acc: 100.0 | Testing Loss: 0.009079325012862682 | Testing Acc: 100.0\n",
            "Epoch: 509 | Training Loss: 0.009035171940922737 | Training Acc: 100.0 | Testing Loss: 0.009082162752747536 | Testing Acc: 100.0\n",
            "Epoch: 510 | Training Loss: 0.009015797637403011 | Training Acc: 100.0 | Testing Loss: 0.009058374911546707 | Testing Acc: 100.0\n",
            "Epoch: 511 | Training Loss: 0.008996822871267796 | Training Acc: 100.0 | Testing Loss: 0.00905901100486517 | Testing Acc: 100.0\n",
            "Epoch: 512 | Training Loss: 0.008978044614195824 | Training Acc: 100.0 | Testing Loss: 0.009033841080963612 | Testing Acc: 100.0\n",
            "Epoch: 513 | Training Loss: 0.008958748541772366 | Training Acc: 100.0 | Testing Loss: 0.009013745933771133 | Testing Acc: 100.0\n",
            "Epoch: 514 | Training Loss: 0.008940466679632664 | Training Acc: 100.0 | Testing Loss: 0.009016488678753376 | Testing Acc: 100.0\n",
            "Epoch: 515 | Training Loss: 0.008921402506530285 | Training Acc: 100.0 | Testing Loss: 0.008992770686745644 | Testing Acc: 100.0\n",
            "Epoch: 516 | Training Loss: 0.00890270620584488 | Training Acc: 100.0 | Testing Loss: 0.00899358931928873 | Testing Acc: 100.0\n",
            "Epoch: 517 | Training Loss: 0.008884322829544544 | Training Acc: 100.0 | Testing Loss: 0.008968963287770748 | Testing Acc: 100.0\n",
            "Epoch: 518 | Training Loss: 0.008865311741828918 | Training Acc: 100.0 | Testing Loss: 0.008948610164225101 | Testing Acc: 100.0\n",
            "Epoch: 519 | Training Loss: 0.008847381919622421 | Training Acc: 100.0 | Testing Loss: 0.008951378986239433 | Testing Acc: 100.0\n",
            "Epoch: 520 | Training Loss: 0.008828584104776382 | Training Acc: 100.0 | Testing Loss: 0.008927975781261921 | Testing Acc: 100.0\n",
            "Epoch: 521 | Training Loss: 0.008810258470475674 | Training Acc: 100.0 | Testing Loss: 0.008929227478802204 | Testing Acc: 100.0\n",
            "Epoch: 522 | Training Loss: 0.008792107924818993 | Training Acc: 100.0 | Testing Loss: 0.008904692716896534 | Testing Acc: 100.0\n",
            "Epoch: 523 | Training Loss: 0.008773460052907467 | Training Acc: 100.0 | Testing Loss: 0.008884845301508904 | Testing Acc: 100.0\n",
            "Epoch: 524 | Training Loss: 0.008755848743021488 | Training Acc: 100.0 | Testing Loss: 0.00888961274176836 | Testing Acc: 100.0\n",
            "Epoch: 525 | Training Loss: 0.008737282827496529 | Training Acc: 100.0 | Testing Loss: 0.008867757394909859 | Testing Acc: 100.0\n",
            "Epoch: 526 | Training Loss: 0.008719234727323055 | Training Acc: 100.0 | Testing Loss: 0.008869366720318794 | Testing Acc: 100.0\n",
            "Epoch: 527 | Training Loss: 0.008701305836439133 | Training Acc: 100.0 | Testing Loss: 0.008845662698149681 | Testing Acc: 100.0\n",
            "Epoch: 528 | Training Loss: 0.008682908490300179 | Training Acc: 100.0 | Testing Loss: 0.008846080861985683 | Testing Acc: 100.0\n",
            "Epoch: 529 | Training Loss: 0.008665589615702629 | Training Acc: 100.0 | Testing Loss: 0.008821861818432808 | Testing Acc: 100.0\n",
            "Epoch: 530 | Training Loss: 0.008647285401821136 | Training Acc: 100.0 | Testing Loss: 0.00880223698914051 | Testing Acc: 100.0\n",
            "Epoch: 531 | Training Loss: 0.00862964428961277 | Training Acc: 100.0 | Testing Loss: 0.00880471058189869 | Testing Acc: 100.0\n",
            "Epoch: 532 | Training Loss: 0.008611918427050114 | Training Acc: 100.0 | Testing Loss: 0.008781861513853073 | Testing Acc: 100.0\n",
            "Epoch: 533 | Training Loss: 0.008593911305069923 | Training Acc: 100.0 | Testing Loss: 0.008782634511590004 | Testing Acc: 100.0\n",
            "Epoch: 534 | Training Loss: 0.008576763793826103 | Training Acc: 100.0 | Testing Loss: 0.008758791722357273 | Testing Acc: 100.0\n",
            "Epoch: 535 | Training Loss: 0.00855887308716774 | Training Acc: 100.0 | Testing Loss: 0.008757575415074825 | Testing Acc: 100.0\n",
            "Epoch: 536 | Training Loss: 0.008541603572666645 | Training Acc: 100.0 | Testing Loss: 0.008753908798098564 | Testing Acc: 100.0\n",
            "Epoch: 537 | Training Loss: 0.008524170145392418 | Training Acc: 100.0 | Testing Loss: 0.008727484382689 | Testing Acc: 100.0\n",
            "Epoch: 538 | Training Loss: 0.008506487123668194 | Training Acc: 100.0 | Testing Loss: 0.008725866675376892 | Testing Acc: 100.0\n",
            "Epoch: 539 | Training Loss: 0.008489579893648624 | Training Acc: 100.0 | Testing Loss: 0.008700823411345482 | Testing Acc: 100.0\n",
            "Epoch: 540 | Training Loss: 0.008472000248730183 | Training Acc: 100.0 | Testing Loss: 0.008698844350874424 | Testing Acc: 100.0\n",
            "Epoch: 541 | Training Loss: 0.008455051109194756 | Training Acc: 100.0 | Testing Loss: 0.008694875054061413 | Testing Acc: 100.0\n",
            "Epoch: 542 | Training Loss: 0.008437858894467354 | Training Acc: 100.0 | Testing Loss: 0.008668292313814163 | Testing Acc: 100.0\n",
            "Epoch: 543 | Training Loss: 0.008420535363256931 | Training Acc: 100.0 | Testing Loss: 0.008666401728987694 | Testing Acc: 100.0\n",
            "Epoch: 544 | Training Loss: 0.008403828367590904 | Training Acc: 100.0 | Testing Loss: 0.00865921750664711 | Testing Acc: 100.0\n",
            "Epoch: 545 | Training Loss: 0.008386602625250816 | Training Acc: 100.0 | Testing Loss: 0.008633102290332317 | Testing Acc: 100.0\n",
            "Epoch: 546 | Training Loss: 0.008369991555809975 | Training Acc: 100.0 | Testing Loss: 0.00863112322986126 | Testing Acc: 100.0\n",
            "Epoch: 547 | Training Loss: 0.008352912031114101 | Training Acc: 100.0 | Testing Loss: 0.008606365881860256 | Testing Acc: 100.0\n",
            "Epoch: 548 | Training Loss: 0.008336072787642479 | Training Acc: 100.0 | Testing Loss: 0.008622894994914532 | Testing Acc: 100.0\n",
            "Epoch: 549 | Training Loss: 0.008319643326103687 | Training Acc: 100.0 | Testing Loss: 0.008592735044658184 | Testing Acc: 100.0\n",
            "Epoch: 550 | Training Loss: 0.008302463218569756 | Training Acc: 100.0 | Testing Loss: 0.008569789119064808 | Testing Acc: 100.0\n",
            "Epoch: 551 | Training Loss: 0.008286274038255215 | Training Acc: 100.0 | Testing Loss: 0.008569681085646152 | Testing Acc: 100.0\n",
            "Epoch: 552 | Training Loss: 0.008269360288977623 | Training Acc: 100.0 | Testing Loss: 0.008546056225895882 | Testing Acc: 100.0\n",
            "Epoch: 553 | Training Loss: 0.008253056555986404 | Training Acc: 100.0 | Testing Loss: 0.008562898263335228 | Testing Acc: 100.0\n",
            "Epoch: 554 | Training Loss: 0.008236641064286232 | Training Acc: 100.0 | Testing Loss: 0.008533510379493237 | Testing Acc: 100.0\n",
            "Epoch: 555 | Training Loss: 0.008219785057008266 | Training Acc: 100.0 | Testing Loss: 0.008529418148100376 | Testing Acc: 100.0\n",
            "Epoch: 556 | Training Loss: 0.008203914389014244 | Training Acc: 100.0 | Testing Loss: 0.008503937162458897 | Testing Acc: 100.0\n",
            "Epoch: 557 | Training Loss: 0.00818726234138012 | Training Acc: 100.0 | Testing Loss: 0.008500286377966404 | Testing Acc: 100.0\n",
            "Epoch: 558 | Training Loss: 0.00817122496664524 | Training Acc: 100.0 | Testing Loss: 0.008495749905705452 | Testing Acc: 100.0\n",
            "Epoch: 559 | Training Loss: 0.008154934272170067 | Training Acc: 100.0 | Testing Loss: 0.008469928056001663 | Testing Acc: 100.0\n",
            "Epoch: 560 | Training Loss: 0.00813857652246952 | Training Acc: 100.0 | Testing Loss: 0.00846831314265728 | Testing Acc: 100.0\n",
            "Epoch: 561 | Training Loss: 0.00812277477234602 | Training Acc: 100.0 | Testing Loss: 0.008461038582026958 | Testing Acc: 100.0\n",
            "Epoch: 562 | Training Loss: 0.008106476627290249 | Training Acc: 100.0 | Testing Loss: 0.008435850962996483 | Testing Acc: 100.0\n",
            "Epoch: 563 | Training Loss: 0.008090781047940254 | Training Acc: 100.0 | Testing Loss: 0.008433924056589603 | Testing Acc: 100.0\n",
            "Epoch: 564 | Training Loss: 0.008074556477367878 | Training Acc: 100.0 | Testing Loss: 0.00841006450355053 | Testing Acc: 100.0\n",
            "Epoch: 565 | Training Loss: 0.008058799430727959 | Training Acc: 100.0 | Testing Loss: 0.008425699546933174 | Testing Acc: 100.0\n",
            "Epoch: 566 | Training Loss: 0.00804308895021677 | Training Acc: 100.0 | Testing Loss: 0.008396998047828674 | Testing Acc: 100.0\n",
            "Epoch: 567 | Training Loss: 0.008026888594031334 | Training Acc: 100.0 | Testing Loss: 0.008392712101340294 | Testing Acc: 100.0\n",
            "Epoch: 568 | Training Loss: 0.00801160465925932 | Training Acc: 100.0 | Testing Loss: 0.008367517963051796 | Testing Acc: 100.0\n",
            "Epoch: 569 | Training Loss: 0.00799561757594347 | Training Acc: 100.0 | Testing Loss: 0.008363793604075909 | Testing Acc: 100.0\n",
            "Epoch: 570 | Training Loss: 0.007980132475495338 | Training Acc: 100.0 | Testing Loss: 0.008359846659004688 | Testing Acc: 100.0\n",
            "Epoch: 571 | Training Loss: 0.007964497432112694 | Training Acc: 100.0 | Testing Loss: 0.008334706537425518 | Testing Acc: 100.0\n",
            "Epoch: 572 | Training Loss: 0.007948780432343483 | Training Acc: 100.0 | Testing Loss: 0.008349098265171051 | Testing Acc: 100.0\n",
            "Epoch: 573 | Training Loss: 0.007933798246085644 | Training Acc: 100.0 | Testing Loss: 0.008320247754454613 | Testing Acc: 100.0\n",
            "Epoch: 574 | Training Loss: 0.007917815819382668 | Training Acc: 100.0 | Testing Loss: 0.008297598920762539 | Testing Acc: 100.0\n",
            "Epoch: 575 | Training Loss: 0.007902787998318672 | Training Acc: 100.0 | Testing Loss: 0.008313276804983616 | Testing Acc: 100.0\n",
            "Epoch: 576 | Training Loss: 0.007887342944741249 | Training Acc: 100.0 | Testing Loss: 0.008285412564873695 | Testing Acc: 100.0\n",
            "Epoch: 577 | Training Loss: 0.007871953770518303 | Training Acc: 100.0 | Testing Loss: 0.008281530812382698 | Testing Acc: 100.0\n",
            "Epoch: 578 | Training Loss: 0.007856860756874084 | Training Acc: 100.0 | Testing Loss: 0.00825725682079792 | Testing Acc: 100.0\n",
            "Epoch: 579 | Training Loss: 0.00784135702997446 | Training Acc: 100.0 | Testing Loss: 0.008271397091448307 | Testing Acc: 100.0\n",
            "Epoch: 580 | Training Loss: 0.00782672967761755 | Training Acc: 100.0 | Testing Loss: 0.008242886513471603 | Testing Acc: 100.0\n",
            "Epoch: 581 | Training Loss: 0.0078111449256539345 | Training Acc: 100.0 | Testing Loss: 0.008220749907195568 | Testing Acc: 100.0\n",
            "Epoch: 582 | Training Loss: 0.007796323858201504 | Training Acc: 100.0 | Testing Loss: 0.00822070799767971 | Testing Acc: 100.0\n",
            "Epoch: 583 | Training Loss: 0.007781228516250849 | Training Acc: 100.0 | Testing Loss: 0.008214527741074562 | Testing Acc: 100.0\n",
            "Epoch: 584 | Training Loss: 0.0077661811374127865 | Training Acc: 100.0 | Testing Loss: 0.00820893980562687 | Testing Acc: 100.0\n",
            "Epoch: 585 | Training Loss: 0.007751524448394775 | Training Acc: 100.0 | Testing Loss: 0.008183558471500874 | Testing Acc: 100.0\n",
            "Epoch: 586 | Training Loss: 0.007736267987638712 | Training Acc: 100.0 | Testing Loss: 0.00817906204611063 | Testing Acc: 100.0\n",
            "Epoch: 587 | Training Loss: 0.007721909787505865 | Training Acc: 100.0 | Testing Loss: 0.008174524642527103 | Testing Acc: 100.0\n",
            "Epoch: 588 | Training Loss: 0.0077068498358130455 | Training Acc: 100.0 | Testing Loss: 0.008150186389684677 | Testing Acc: 100.0\n",
            "Epoch: 589 | Training Loss: 0.007692126091569662 | Training Acc: 100.0 | Testing Loss: 0.00816376693546772 | Testing Acc: 100.0\n",
            "Epoch: 590 | Training Loss: 0.007677757181227207 | Training Acc: 100.0 | Testing Loss: 0.008136003278195858 | Testing Acc: 100.0\n",
            "Epoch: 591 | Training Loss: 0.007662697229534388 | Training Acc: 100.0 | Testing Loss: 0.008131883107125759 | Testing Acc: 100.0\n",
            "Epoch: 592 | Training Loss: 0.007648470811545849 | Training Acc: 100.0 | Testing Loss: 0.008107675239443779 | Testing Acc: 100.0\n",
            "Epoch: 593 | Training Loss: 0.007633580826222897 | Training Acc: 100.0 | Testing Loss: 0.008103681728243828 | Testing Acc: 100.0\n",
            "Epoch: 594 | Training Loss: 0.007619366981089115 | Training Acc: 100.0 | Testing Loss: 0.008099548518657684 | Testing Acc: 100.0\n",
            "Epoch: 595 | Training Loss: 0.007604777812957764 | Training Acc: 100.0 | Testing Loss: 0.008075936697423458 | Testing Acc: 100.0\n",
            "Epoch: 596 | Training Loss: 0.007590377237647772 | Training Acc: 100.0 | Testing Loss: 0.008089221082627773 | Testing Acc: 100.0\n",
            "Epoch: 597 | Training Loss: 0.007576212286949158 | Training Acc: 100.0 | Testing Loss: 0.008061825297772884 | Testing Acc: 100.0\n",
            "Epoch: 598 | Training Loss: 0.007561425678431988 | Training Acc: 100.0 | Testing Loss: 0.00805760733783245 | Testing Acc: 100.0\n",
            "Epoch: 599 | Training Loss: 0.007547605782747269 | Training Acc: 100.0 | Testing Loss: 0.008033938705921173 | Testing Acc: 100.0\n",
            "Epoch: 600 | Training Loss: 0.007533113472163677 | Training Acc: 100.0 | Testing Loss: 0.008029943332076073 | Testing Acc: 100.0\n",
            "Epoch: 601 | Training Loss: 0.007519053760915995 | Training Acc: 100.0 | Testing Loss: 0.008026174269616604 | Testing Acc: 100.0\n",
            "Epoch: 602 | Training Loss: 0.00750482315197587 | Training Acc: 100.0 | Testing Loss: 0.00800273846834898 | Testing Acc: 100.0\n",
            "Epoch: 603 | Training Loss: 0.0074907587841153145 | Training Acc: 100.0 | Testing Loss: 0.008016147650778294 | Testing Acc: 100.0\n",
            "Epoch: 604 | Training Loss: 0.0074768983758986 | Training Acc: 100.0 | Testing Loss: 0.007989229634404182 | Testing Acc: 100.0\n",
            "Epoch: 605 | Training Loss: 0.007462416775524616 | Training Acc: 100.0 | Testing Loss: 0.00796822551637888 | Testing Acc: 100.0\n",
            "Epoch: 606 | Training Loss: 0.007448968477547169 | Training Acc: 100.0 | Testing Loss: 0.007982579991221428 | Testing Acc: 100.0\n",
            "Epoch: 607 | Training Loss: 0.0074348002672195435 | Training Acc: 100.0 | Testing Loss: 0.007957013323903084 | Testing Acc: 100.0\n",
            "Epoch: 608 | Training Loss: 0.007420848123729229 | Training Acc: 100.0 | Testing Loss: 0.007968263700604439 | Testing Acc: 100.0\n",
            "Epoch: 609 | Training Loss: 0.007407209370285273 | Training Acc: 100.0 | Testing Loss: 0.007940568029880524 | Testing Acc: 100.0\n",
            "Epoch: 610 | Training Loss: 0.00739304581657052 | Training Acc: 100.0 | Testing Loss: 0.007935801520943642 | Testing Acc: 100.0\n",
            "Epoch: 611 | Training Loss: 0.0073796166107058525 | Training Acc: 100.0 | Testing Loss: 0.007912454195320606 | Testing Acc: 100.0\n",
            "Epoch: 612 | Training Loss: 0.007365635596215725 | Training Acc: 100.0 | Testing Loss: 0.007908056490123272 | Testing Acc: 100.0\n",
            "Epoch: 613 | Training Loss: 0.007352213375270367 | Training Acc: 100.0 | Testing Loss: 0.007904374040663242 | Testing Acc: 100.0\n",
            "Epoch: 614 | Training Loss: 0.007338371127843857 | Training Acc: 100.0 | Testing Loss: 0.007881496101617813 | Testing Acc: 100.0\n",
            "Epoch: 615 | Training Loss: 0.0073248534463346004 | Training Acc: 100.0 | Testing Loss: 0.007894051261246204 | Testing Acc: 100.0\n",
            "Epoch: 616 | Training Loss: 0.007311420980840921 | Training Acc: 100.0 | Testing Loss: 0.00786787923425436 | Testing Acc: 100.0\n",
            "Epoch: 617 | Training Loss: 0.0072974832728505135 | Training Acc: 100.0 | Testing Loss: 0.007864000275731087 | Testing Acc: 100.0\n",
            "Epoch: 618 | Training Loss: 0.0072845397517085075 | Training Acc: 100.0 | Testing Loss: 0.007855559699237347 | Testing Acc: 100.0\n",
            "Epoch: 619 | Training Loss: 0.007270758505910635 | Training Acc: 100.0 | Testing Loss: 0.007832514122128487 | Testing Acc: 100.0\n",
            "Epoch: 620 | Training Loss: 0.007257544435560703 | Training Acc: 100.0 | Testing Loss: 0.007844720967113972 | Testing Acc: 100.0\n",
            "Epoch: 621 | Training Loss: 0.0072442032396793365 | Training Acc: 100.0 | Testing Loss: 0.007818791083991528 | Testing Acc: 100.0\n",
            "Epoch: 622 | Training Loss: 0.007230691611766815 | Training Acc: 100.0 | Testing Loss: 0.007814724929630756 | Testing Acc: 100.0\n",
            "Epoch: 623 | Training Loss: 0.007217618636786938 | Training Acc: 100.0 | Testing Loss: 0.007792408112436533 | Testing Acc: 100.0\n",
            "Epoch: 624 | Training Loss: 0.007204187568277121 | Training Acc: 100.0 | Testing Loss: 0.007804480381309986 | Testing Acc: 100.0\n",
            "Epoch: 625 | Training Loss: 0.007191366050392389 | Training Acc: 100.0 | Testing Loss: 0.007778873201459646 | Testing Acc: 100.0\n",
            "Epoch: 626 | Training Loss: 0.007177817635238171 | Training Acc: 100.0 | Testing Loss: 0.007772800512611866 | Testing Acc: 100.0\n",
            "Epoch: 627 | Training Loss: 0.007164880633354187 | Training Acc: 100.0 | Testing Loss: 0.007767816539853811 | Testing Acc: 100.0\n",
            "Epoch: 628 | Training Loss: 0.0071517652831971645 | Training Acc: 100.0 | Testing Loss: 0.007745098322629929 | Testing Acc: 100.0\n",
            "Epoch: 629 | Training Loss: 0.007138678338378668 | Training Acc: 100.0 | Testing Loss: 0.007756994105875492 | Testing Acc: 100.0\n",
            "Epoch: 630 | Training Loss: 0.007125929929316044 | Training Acc: 100.0 | Testing Loss: 0.0077313268557190895 | Testing Acc: 100.0\n",
            "Epoch: 631 | Training Loss: 0.007112526334822178 | Training Acc: 100.0 | Testing Loss: 0.007711193524301052 | Testing Acc: 100.0\n",
            "Epoch: 632 | Training Loss: 0.007100098766386509 | Training Acc: 100.0 | Testing Loss: 0.007724412716925144 | Testing Acc: 100.0\n",
            "Epoch: 633 | Training Loss: 0.007086908910423517 | Training Acc: 100.0 | Testing Loss: 0.007700197398662567 | Testing Acc: 100.0\n",
            "Epoch: 634 | Training Loss: 0.0070740580558776855 | Training Acc: 100.0 | Testing Loss: 0.007710852660238743 | Testing Acc: 100.0\n",
            "Epoch: 635 | Training Loss: 0.007061441894620657 | Training Acc: 100.0 | Testing Loss: 0.007684823125600815 | Testing Acc: 100.0\n",
            "Epoch: 636 | Training Loss: 0.007048369385302067 | Training Acc: 100.0 | Testing Loss: 0.00768034253269434 | Testing Acc: 100.0\n",
            "Epoch: 637 | Training Loss: 0.007035909686237574 | Training Acc: 100.0 | Testing Loss: 0.007671824656426907 | Testing Acc: 100.0\n",
            "Epoch: 638 | Training Loss: 0.007022968027740717 | Training Acc: 100.0 | Testing Loss: 0.007649569772183895 | Testing Acc: 100.0\n",
            "Epoch: 639 | Training Loss: 0.0070105381309986115 | Training Acc: 100.0 | Testing Loss: 0.007647452410310507 | Testing Acc: 100.0\n",
            "Epoch: 640 | Training Loss: 0.006997756659984589 | Training Acc: 100.0 | Testing Loss: 0.007640617899596691 | Testing Acc: 100.0\n",
            "Epoch: 641 | Training Loss: 0.006985223386436701 | Training Acc: 100.0 | Testing Loss: 0.007635137531906366 | Testing Acc: 100.0\n",
            "Epoch: 642 | Training Loss: 0.0069727106019854546 | Training Acc: 100.0 | Testing Loss: 0.007612521760165691 | Testing Acc: 100.0\n",
            "Epoch: 643 | Training Loss: 0.006960074417293072 | Training Acc: 100.0 | Testing Loss: 0.007623409386724234 | Testing Acc: 100.0\n",
            "Epoch: 644 | Training Loss: 0.0069478778168559074 | Training Acc: 100.0 | Testing Loss: 0.007598519325256348 | Testing Acc: 100.0\n",
            "Epoch: 645 | Training Loss: 0.006935052573680878 | Training Acc: 100.0 | Testing Loss: 0.007592291571199894 | Testing Acc: 100.0\n",
            "Epoch: 646 | Training Loss: 0.0069229924120008945 | Training Acc: 100.0 | Testing Loss: 0.007587933447211981 | Testing Acc: 100.0\n",
            "Epoch: 647 | Training Loss: 0.006910429801791906 | Training Acc: 100.0 | Testing Loss: 0.0075657665729522705 | Testing Acc: 100.0\n",
            "Epoch: 648 | Training Loss: 0.006898172199726105 | Training Acc: 100.0 | Testing Loss: 0.007576638367027044 | Testing Acc: 100.0\n",
            "Epoch: 649 | Training Loss: 0.006885972805321217 | Training Acc: 100.0 | Testing Loss: 0.007552136667072773 | Testing Acc: 100.0\n",
            "Epoch: 650 | Training Loss: 0.006873361766338348 | Training Acc: 100.0 | Testing Loss: 0.007561452686786652 | Testing Acc: 100.0\n",
            "Epoch: 651 | Training Loss: 0.006861685775220394 | Training Acc: 100.0 | Testing Loss: 0.007536151446402073 | Testing Acc: 100.0\n",
            "Epoch: 652 | Training Loss: 0.006849013268947601 | Training Acc: 100.0 | Testing Loss: 0.007516111247241497 | Testing Acc: 100.0\n",
            "Epoch: 653 | Training Loss: 0.006837217602878809 | Training Acc: 100.0 | Testing Loss: 0.007528048940002918 | Testing Acc: 100.0\n",
            "Epoch: 654 | Training Loss: 0.006824924610555172 | Training Acc: 100.0 | Testing Loss: 0.007504676468670368 | Testing Acc: 100.0\n",
            "Epoch: 655 | Training Loss: 0.006812737789005041 | Training Acc: 100.0 | Testing Loss: 0.007514406926929951 | Testing Acc: 100.0\n",
            "Epoch: 656 | Training Loss: 0.0068009644746780396 | Training Acc: 100.0 | Testing Loss: 0.007489756681025028 | Testing Acc: 100.0\n",
            "Epoch: 657 | Training Loss: 0.006788562051951885 | Training Acc: 100.0 | Testing Loss: 0.007485417183488607 | Testing Acc: 100.0\n",
            "Epoch: 658 | Training Loss: 0.006776976399123669 | Training Acc: 100.0 | Testing Loss: 0.007477021310478449 | Testing Acc: 100.0\n",
            "Epoch: 659 | Training Loss: 0.006764732301235199 | Training Acc: 100.0 | Testing Loss: 0.007455774582922459 | Testing Acc: 100.0\n",
            "Epoch: 660 | Training Loss: 0.006753021385520697 | Training Acc: 100.0 | Testing Loss: 0.007466560695320368 | Testing Acc: 100.0\n",
            "Epoch: 661 | Training Loss: 0.006741099059581757 | Training Acc: 100.0 | Testing Loss: 0.007442891597747803 | Testing Acc: 100.0\n",
            "Epoch: 662 | Training Loss: 0.006729112006723881 | Training Acc: 100.0 | Testing Loss: 0.007439352571964264 | Testing Acc: 100.0\n",
            "Epoch: 663 | Training Loss: 0.006717486772686243 | Training Acc: 100.0 | Testing Loss: 0.00743133295327425 | Testing Acc: 100.0\n",
            "Epoch: 664 | Training Loss: 0.006705418229103088 | Training Acc: 100.0 | Testing Loss: 0.007425891701132059 | Testing Acc: 100.0\n",
            "Epoch: 665 | Training Loss: 0.006694032810628414 | Training Acc: 100.0 | Testing Loss: 0.00740386638790369 | Testing Acc: 100.0\n",
            "Epoch: 666 | Training Loss: 0.006682096514850855 | Training Acc: 100.0 | Testing Loss: 0.007398765534162521 | Testing Acc: 100.0\n",
            "Epoch: 667 | Training Loss: 0.006670522503554821 | Training Acc: 100.0 | Testing Loss: 0.007394993212074041 | Testing Acc: 100.0\n",
            "Epoch: 668 | Training Loss: 0.006658836267888546 | Training Acc: 100.0 | Testing Loss: 0.007387174759060144 | Testing Acc: 100.0\n",
            "Epoch: 669 | Training Loss: 0.0066471705213189125 | Training Acc: 100.0 | Testing Loss: 0.007381369359791279 | Testing Acc: 100.0\n",
            "Epoch: 670 | Training Loss: 0.006635728292167187 | Training Acc: 100.0 | Testing Loss: 0.007372111082077026 | Testing Acc: 100.0\n",
            "Epoch: 671 | Training Loss: 0.006623937748372555 | Training Acc: 100.0 | Testing Loss: 0.007350697182118893 | Testing Acc: 100.0\n",
            "Epoch: 672 | Training Loss: 0.006612793542444706 | Training Acc: 100.0 | Testing Loss: 0.007360723800957203 | Testing Acc: 100.0\n",
            "Epoch: 673 | Training Loss: 0.006601096596568823 | Training Acc: 100.0 | Testing Loss: 0.007337529212236404 | Testing Acc: 100.0\n",
            "Epoch: 674 | Training Loss: 0.006589741911739111 | Training Acc: 100.0 | Testing Loss: 0.007333710789680481 | Testing Acc: 100.0\n",
            "Epoch: 675 | Training Loss: 0.0065782563760876656 | Training Acc: 100.0 | Testing Loss: 0.007325727492570877 | Testing Acc: 100.0\n",
            "Epoch: 676 | Training Loss: 0.006566798780113459 | Training Acc: 100.0 | Testing Loss: 0.007320077624171972 | Testing Acc: 100.0\n",
            "Epoch: 677 | Training Loss: 0.006555575877428055 | Training Acc: 100.0 | Testing Loss: 0.007310981396585703 | Testing Acc: 100.0\n",
            "Epoch: 678 | Training Loss: 0.006544037256389856 | Training Acc: 100.0 | Testing Loss: 0.007289949804544449 | Testing Acc: 100.0\n",
            "Epoch: 679 | Training Loss: 0.006533077452331781 | Training Acc: 100.0 | Testing Loss: 0.007300087716430426 | Testing Acc: 100.0\n",
            "Epoch: 680 | Training Loss: 0.006521708332002163 | Training Acc: 100.0 | Testing Loss: 0.00727719534188509 | Testing Acc: 100.0\n",
            "Epoch: 681 | Training Loss: 0.006510487757623196 | Training Acc: 100.0 | Testing Loss: 0.007273369934409857 | Testing Acc: 100.0\n",
            "Epoch: 682 | Training Loss: 0.006499242968857288 | Training Acc: 100.0 | Testing Loss: 0.007265460677444935 | Testing Acc: 100.0\n",
            "Epoch: 683 | Training Loss: 0.006487998180091381 | Training Acc: 100.0 | Testing Loss: 0.007259921170771122 | Testing Acc: 100.0\n",
            "Epoch: 684 | Training Loss: 0.006477002054452896 | Training Acc: 100.0 | Testing Loss: 0.007250878028571606 | Testing Acc: 100.0\n",
            "Epoch: 685 | Training Loss: 0.006465706042945385 | Training Acc: 100.0 | Testing Loss: 0.007230281829833984 | Testing Acc: 100.0\n",
            "Epoch: 686 | Training Loss: 0.0064549995586276054 | Training Acc: 100.0 | Testing Loss: 0.0072397938929498196 | Testing Acc: 100.0\n",
            "Epoch: 687 | Training Loss: 0.006443723104894161 | Training Acc: 100.0 | Testing Loss: 0.007217167876660824 | Testing Acc: 100.0\n",
            "Epoch: 688 | Training Loss: 0.006432780064642429 | Training Acc: 100.0 | Testing Loss: 0.007213524542748928 | Testing Acc: 100.0\n",
            "Epoch: 689 | Training Loss: 0.0064217569306492805 | Training Acc: 100.0 | Testing Loss: 0.007205726113170385 | Testing Acc: 100.0\n",
            "Epoch: 690 | Training Loss: 0.006410722620785236 | Training Acc: 100.0 | Testing Loss: 0.007200321648269892 | Testing Acc: 100.0\n",
            "Epoch: 691 | Training Loss: 0.006399982608854771 | Training Acc: 100.0 | Testing Loss: 0.00719152856618166 | Testing Acc: 100.0\n",
            "Epoch: 692 | Training Loss: 0.006388829555362463 | Training Acc: 100.0 | Testing Loss: 0.007185251917690039 | Testing Acc: 100.0\n",
            "Epoch: 693 | Training Loss: 0.0063783093355596066 | Training Acc: 100.0 | Testing Loss: 0.007175799459218979 | Testing Acc: 100.0\n",
            "Epoch: 694 | Training Loss: 0.006367242895066738 | Training Acc: 100.0 | Testing Loss: 0.0071551017463207245 | Testing Acc: 100.0\n",
            "Epoch: 695 | Training Loss: 0.006356591824442148 | Training Acc: 100.0 | Testing Loss: 0.007164223585277796 | Testing Acc: 100.0\n",
            "Epoch: 696 | Training Loss: 0.006345830857753754 | Training Acc: 100.0 | Testing Loss: 0.007142084650695324 | Testing Acc: 100.0\n",
            "Epoch: 697 | Training Loss: 0.006334972567856312 | Training Acc: 100.0 | Testing Loss: 0.007149999029934406 | Testing Acc: 100.0\n",
            "Epoch: 698 | Training Loss: 0.006324491463601589 | Training Acc: 100.0 | Testing Loss: 0.007126984652131796 | Testing Acc: 100.0\n",
            "Epoch: 699 | Training Loss: 0.006313462741672993 | Training Acc: 100.0 | Testing Loss: 0.007122841663658619 | Testing Acc: 100.0\n",
            "Epoch: 700 | Training Loss: 0.006303147878497839 | Training Acc: 100.0 | Testing Loss: 0.007114744745194912 | Testing Acc: 100.0\n",
            "Epoch: 701 | Training Loss: 0.006292239762842655 | Training Acc: 100.0 | Testing Loss: 0.007095272187143564 | Testing Acc: 100.0\n",
            "Epoch: 702 | Training Loss: 0.006281906273216009 | Training Acc: 100.0 | Testing Loss: 0.007105210330337286 | Testing Acc: 100.0\n",
            "Epoch: 703 | Training Loss: 0.006271238438785076 | Training Acc: 100.0 | Testing Loss: 0.0070835864171385765 | Testing Acc: 100.0\n",
            "Epoch: 704 | Training Loss: 0.006260665599256754 | Training Acc: 100.0 | Testing Loss: 0.007091569248586893 | Testing Acc: 100.0\n",
            "Epoch: 705 | Training Loss: 0.006250309757888317 | Training Acc: 100.0 | Testing Loss: 0.007069134619086981 | Testing Acc: 100.0\n",
            "Epoch: 706 | Training Loss: 0.006239521317183971 | Training Acc: 100.0 | Testing Loss: 0.007076500914990902 | Testing Acc: 100.0\n",
            "Epoch: 707 | Training Loss: 0.006229507736861706 | Training Acc: 100.0 | Testing Loss: 0.007053936831653118 | Testing Acc: 100.0\n",
            "Epoch: 708 | Training Loss: 0.006218677386641502 | Training Acc: 100.0 | Testing Loss: 0.007047113962471485 | Testing Acc: 100.0\n",
            "Epoch: 709 | Training Loss: 0.006208594888448715 | Training Acc: 100.0 | Testing Loss: 0.0070425658486783504 | Testing Acc: 100.0\n",
            "Epoch: 710 | Training Loss: 0.006198020186275244 | Training Acc: 100.0 | Testing Loss: 0.007022997830063105 | Testing Acc: 100.0\n",
            "Epoch: 711 | Training Loss: 0.0061878180131316185 | Training Acc: 100.0 | Testing Loss: 0.0070319995284080505 | Testing Acc: 100.0\n",
            "Epoch: 712 | Training Loss: 0.006177505943924189 | Training Acc: 100.0 | Testing Loss: 0.007010714616626501 | Testing Acc: 100.0\n",
            "Epoch: 713 | Training Loss: 0.006167053245007992 | Training Acc: 100.0 | Testing Loss: 0.007018460426479578 | Testing Acc: 100.0\n",
            "Epoch: 714 | Training Loss: 0.0061571053229272366 | Training Acc: 100.0 | Testing Loss: 0.006996614392846823 | Testing Acc: 100.0\n",
            "Epoch: 715 | Training Loss: 0.006146556697785854 | Training Acc: 100.0 | Testing Loss: 0.0069900862872600555 | Testing Acc: 100.0\n",
            "Epoch: 716 | Training Loss: 0.006136602256447077 | Training Acc: 100.0 | Testing Loss: 0.006985818035900593 | Testing Acc: 100.0\n",
            "Epoch: 717 | Training Loss: 0.006126275286078453 | Training Acc: 100.0 | Testing Loss: 0.006976839154958725 | Testing Acc: 100.0\n",
            "Epoch: 718 | Training Loss: 0.006116182077676058 | Training Acc: 100.0 | Testing Loss: 0.006971004419028759 | Testing Acc: 100.0\n",
            "Epoch: 719 | Training Loss: 0.006106161046773195 | Training Acc: 100.0 | Testing Loss: 0.006961761508136988 | Testing Acc: 100.0\n",
            "Epoch: 720 | Training Loss: 0.00609593465924263 | Training Acc: 100.0 | Testing Loss: 0.006955910474061966 | Testing Acc: 100.0\n",
            "Epoch: 721 | Training Loss: 0.006086106877774 | Training Acc: 100.0 | Testing Loss: 0.006935770623385906 | Testing Acc: 100.0\n",
            "Epoch: 722 | Training Loss: 0.006075920537114143 | Training Acc: 100.0 | Testing Loss: 0.006930175237357616 | Testing Acc: 100.0\n",
            "Epoch: 723 | Training Loss: 0.0060661183670163155 | Training Acc: 100.0 | Testing Loss: 0.006937473081052303 | Testing Acc: 100.0\n",
            "Epoch: 724 | Training Loss: 0.0060560815036296844 | Training Acc: 100.0 | Testing Loss: 0.006915908306837082 | Testing Acc: 100.0\n",
            "Epoch: 725 | Training Loss: 0.006046133581548929 | Training Acc: 100.0 | Testing Loss: 0.006922760047018528 | Testing Acc: 100.0\n",
            "Epoch: 726 | Training Loss: 0.006036325357854366 | Training Acc: 100.0 | Testing Loss: 0.00690087815746665 | Testing Acc: 100.0\n",
            "Epoch: 727 | Training Loss: 0.006026198621839285 | Training Acc: 100.0 | Testing Loss: 0.006896906532347202 | Testing Acc: 100.0\n",
            "Epoch: 728 | Training Loss: 0.006016625557094812 | Training Acc: 100.0 | Testing Loss: 0.006888532545417547 | Testing Acc: 100.0\n",
            "Epoch: 729 | Training Loss: 0.0060064950957894325 | Training Acc: 100.0 | Testing Loss: 0.006870263256132603 | Testing Acc: 100.0\n",
            "Epoch: 730 | Training Loss: 0.005997002590447664 | Training Acc: 100.0 | Testing Loss: 0.006878850515931845 | Testing Acc: 100.0\n",
            "Epoch: 731 | Training Loss: 0.0059870523400604725 | Training Acc: 100.0 | Testing Loss: 0.006858411245048046 | Testing Acc: 100.0\n",
            "Epoch: 732 | Training Loss: 0.005977347958832979 | Training Acc: 100.0 | Testing Loss: 0.0068657309748232365 | Testing Acc: 100.0\n",
            "Epoch: 733 | Training Loss: 0.005967733450233936 | Training Acc: 100.0 | Testing Loss: 0.006845173425972462 | Testing Acc: 100.0\n",
            "Epoch: 734 | Training Loss: 0.005957824178040028 | Training Acc: 100.0 | Testing Loss: 0.006851828191429377 | Testing Acc: 100.0\n",
            "Epoch: 735 | Training Loss: 0.0059484392404556274 | Training Acc: 100.0 | Testing Loss: 0.006830508820712566 | Testing Acc: 100.0\n",
            "Epoch: 736 | Training Loss: 0.005938433110713959 | Training Acc: 100.0 | Testing Loss: 0.006823939271271229 | Testing Acc: 100.0\n",
            "Epoch: 737 | Training Loss: 0.005929157603532076 | Training Acc: 100.0 | Testing Loss: 0.006819966249167919 | Testing Acc: 100.0\n",
            "Epoch: 738 | Training Loss: 0.005919367540627718 | Training Acc: 100.0 | Testing Loss: 0.006801660172641277 | Testing Acc: 100.0\n",
            "Epoch: 739 | Training Loss: 0.0059099276550114155 | Training Acc: 100.0 | Testing Loss: 0.0068099224008619785 | Testing Acc: 100.0\n",
            "Epoch: 740 | Training Loss: 0.005900351330637932 | Training Acc: 100.0 | Testing Loss: 0.006790042854845524 | Testing Acc: 100.0\n",
            "Epoch: 741 | Training Loss: 0.005890741012990475 | Training Acc: 100.0 | Testing Loss: 0.006797193083912134 | Testing Acc: 100.0\n",
            "Epoch: 742 | Training Loss: 0.00588142778724432 | Training Acc: 100.0 | Testing Loss: 0.006776656024158001 | Testing Acc: 100.0\n",
            "Epoch: 743 | Training Loss: 0.0058717126958072186 | Training Acc: 100.0 | Testing Loss: 0.006770374719053507 | Testing Acc: 100.0\n",
            "Epoch: 744 | Training Loss: 0.005862529389560223 | Training Acc: 100.0 | Testing Loss: 0.006766420789062977 | Testing Acc: 100.0\n",
            "Epoch: 745 | Training Loss: 0.005852875299751759 | Training Acc: 100.0 | Testing Loss: 0.006758853793144226 | Testing Acc: 100.0\n",
            "Epoch: 746 | Training Loss: 0.005843620281666517 | Training Acc: 100.0 | Testing Loss: 0.00675385445356369 | Testing Acc: 100.0\n",
            "Epoch: 747 | Training Loss: 0.005834180396050215 | Training Acc: 100.0 | Testing Loss: 0.00673521775752306 | Testing Acc: 100.0\n",
            "Epoch: 748 | Training Loss: 0.005824842490255833 | Training Acc: 100.0 | Testing Loss: 0.006743096746504307 | Testing Acc: 100.0\n",
            "Epoch: 749 | Training Loss: 0.005815632175654173 | Training Acc: 100.0 | Testing Loss: 0.006723237223923206 | Testing Acc: 100.0\n",
            "Epoch: 750 | Training Loss: 0.005806095898151398 | Training Acc: 100.0 | Testing Loss: 0.006717500276863575 | Testing Acc: 100.0\n",
            "Epoch: 751 | Training Loss: 0.005797125864773989 | Training Acc: 100.0 | Testing Loss: 0.006723938975483179 | Testing Acc: 100.0\n",
            "Epoch: 752 | Training Loss: 0.00578771997243166 | Training Acc: 100.0 | Testing Loss: 0.0067034573294222355 | Testing Acc: 100.0\n",
            "Epoch: 753 | Training Loss: 0.005778543651103973 | Training Acc: 100.0 | Testing Loss: 0.00669963750988245 | Testing Acc: 100.0\n",
            "Epoch: 754 | Training Loss: 0.005769328214228153 | Training Acc: 100.0 | Testing Loss: 0.006691929884254932 | Testing Acc: 100.0\n",
            "Epoch: 755 | Training Loss: 0.00576003547757864 | Training Acc: 100.0 | Testing Loss: 0.00668720668181777 | Testing Acc: 100.0\n",
            "Epoch: 756 | Training Loss: 0.005751097574830055 | Training Acc: 100.0 | Testing Loss: 0.006678864359855652 | Testing Acc: 100.0\n",
            "Epoch: 757 | Training Loss: 0.005741679109632969 | Training Acc: 100.0 | Testing Loss: 0.0066613564267754555 | Testing Acc: 100.0\n",
            "Epoch: 758 | Training Loss: 0.005732945166528225 | Training Acc: 100.0 | Testing Loss: 0.0066692521795630455 | Testing Acc: 100.0\n",
            "Epoch: 759 | Training Loss: 0.005723617039620876 | Training Acc: 100.0 | Testing Loss: 0.006650013383477926 | Testing Acc: 100.0\n",
            "Epoch: 760 | Training Loss: 0.005714657716453075 | Training Acc: 100.0 | Testing Loss: 0.006656908430159092 | Testing Acc: 100.0\n",
            "Epoch: 761 | Training Loss: 0.005705629475414753 | Training Acc: 100.0 | Testing Loss: 0.00663704564794898 | Testing Acc: 100.0\n",
            "Epoch: 762 | Training Loss: 0.005696496460586786 | Training Acc: 100.0 | Testing Loss: 0.006643315311521292 | Testing Acc: 100.0\n",
            "Epoch: 763 | Training Loss: 0.005687745753675699 | Training Acc: 100.0 | Testing Loss: 0.006623395718634129 | Testing Acc: 100.0\n",
            "Epoch: 764 | Training Loss: 0.005678459536284208 | Training Acc: 100.0 | Testing Loss: 0.00662942323833704 | Testing Acc: 100.0\n",
            "Epoch: 765 | Training Loss: 0.005669903941452503 | Training Acc: 100.0 | Testing Loss: 0.006609396077692509 | Testing Acc: 100.0\n",
            "Epoch: 766 | Training Loss: 0.005660681985318661 | Training Acc: 100.0 | Testing Loss: 0.006602935492992401 | Testing Acc: 100.0\n",
            "Epoch: 767 | Training Loss: 0.005651913583278656 | Training Acc: 100.0 | Testing Loss: 0.006599025335162878 | Testing Acc: 100.0\n",
            "Epoch: 768 | Training Loss: 0.00564297940582037 | Training Acc: 100.0 | Testing Loss: 0.006591507233679295 | Testing Acc: 100.0\n",
            "Epoch: 769 | Training Loss: 0.005634083412587643 | Training Acc: 100.0 | Testing Loss: 0.006586718373000622 | Testing Acc: 100.0\n",
            "Epoch: 770 | Training Loss: 0.005625365301966667 | Training Acc: 100.0 | Testing Loss: 0.006578610744327307 | Testing Acc: 100.0\n",
            "Epoch: 771 | Training Loss: 0.005616357084363699 | Training Acc: 100.0 | Testing Loss: 0.006573422811925411 | Testing Acc: 100.0\n",
            "Epoch: 772 | Training Loss: 0.005607844330370426 | Training Acc: 100.0 | Testing Loss: 0.006564932409673929 | Testing Acc: 100.0\n",
            "Epoch: 773 | Training Loss: 0.005598899908363819 | Training Acc: 100.0 | Testing Loss: 0.006547622382640839 | Testing Acc: 100.0\n",
            "Epoch: 774 | Training Loss: 0.0055903419852256775 | Training Acc: 100.0 | Testing Loss: 0.006555165164172649 | Testing Acc: 100.0\n",
            "Epoch: 775 | Training Loss: 0.005581524223089218 | Training Acc: 100.0 | Testing Loss: 0.006536561064422131 | Testing Acc: 100.0\n",
            "Epoch: 776 | Training Loss: 0.005572809837758541 | Training Acc: 100.0 | Testing Loss: 0.0065429480746388435 | Testing Acc: 100.0\n",
            "Epoch: 777 | Training Loss: 0.005564230028539896 | Training Acc: 100.0 | Testing Loss: 0.006523699965327978 | Testing Acc: 100.0\n",
            "Epoch: 778 | Training Loss: 0.005555346142500639 | Training Acc: 100.0 | Testing Loss: 0.006529801990836859 | Testing Acc: 100.0\n",
            "Epoch: 779 | Training Loss: 0.0055470336228609085 | Training Acc: 100.0 | Testing Loss: 0.006510350853204727 | Testing Acc: 100.0\n",
            "Epoch: 780 | Training Loss: 0.005538120400160551 | Training Acc: 100.0 | Testing Loss: 0.006504120770841837 | Testing Acc: 100.0\n",
            "Epoch: 781 | Training Loss: 0.0055297487415373325 | Training Acc: 100.0 | Testing Loss: 0.00650053471326828 | Testing Acc: 100.0\n",
            "Epoch: 782 | Training Loss: 0.005521068349480629 | Training Acc: 100.0 | Testing Loss: 0.006493059452623129 | Testing Acc: 100.0\n",
            "Epoch: 783 | Training Loss: 0.005512521602213383 | Training Acc: 100.0 | Testing Loss: 0.0064886040054261684 | Testing Acc: 100.0\n",
            "Epoch: 784 | Training Loss: 0.005504096858203411 | Training Acc: 100.0 | Testing Loss: 0.006480548530817032 | Testing Acc: 100.0\n",
            "Epoch: 785 | Training Loss: 0.005495395045727491 | Training Acc: 100.0 | Testing Loss: 0.006475620903074741 | Testing Acc: 100.0\n",
            "Epoch: 786 | Training Loss: 0.005487191025167704 | Training Acc: 100.0 | Testing Loss: 0.0064674364402890205 | Testing Acc: 100.0\n",
            "Epoch: 787 | Training Loss: 0.00547847431153059 | Training Acc: 100.0 | Testing Loss: 0.006450474262237549 | Testing Acc: 100.0\n",
            "Epoch: 788 | Training Loss: 0.0054702963680028915 | Training Acc: 100.0 | Testing Loss: 0.006457542069256306 | Testing Acc: 100.0\n",
            "Epoch: 789 | Training Loss: 0.005461747758090496 | Training Acc: 100.0 | Testing Loss: 0.0064395456574857235 | Testing Acc: 100.0\n",
            "Epoch: 790 | Training Loss: 0.0054534003138542175 | Training Acc: 100.0 | Testing Loss: 0.006445720791816711 | Testing Acc: 100.0\n",
            "Epoch: 791 | Training Loss: 0.0054450323805212975 | Training Acc: 100.0 | Testing Loss: 0.00642725545912981 | Testing Acc: 100.0\n",
            "Epoch: 792 | Training Loss: 0.0054365829564630985 | Training Acc: 100.0 | Testing Loss: 0.006432960741221905 | Testing Acc: 100.0\n",
            "Epoch: 793 | Training Loss: 0.005428393837064505 | Training Acc: 100.0 | Testing Loss: 0.006414281670004129 | Testing Acc: 100.0\n",
            "Epoch: 794 | Training Loss: 0.005419902969151735 | Training Acc: 100.0 | Testing Loss: 0.006408338900655508 | Testing Acc: 100.0\n",
            "Epoch: 795 | Training Loss: 0.0054118018597364426 | Training Acc: 100.0 | Testing Loss: 0.00641377130523324 | Testing Acc: 100.0\n",
            "Epoch: 796 | Training Loss: 0.005403437651693821 | Training Acc: 100.0 | Testing Loss: 0.006394924130290747 | Testing Acc: 100.0\n",
            "Epoch: 797 | Training Loss: 0.005395201500505209 | Training Acc: 100.0 | Testing Loss: 0.006400284823030233 | Testing Acc: 100.0\n",
            "Epoch: 798 | Training Loss: 0.005387000273913145 | Training Acc: 100.0 | Testing Loss: 0.00638150330632925 | Testing Acc: 100.0\n",
            "Epoch: 799 | Training Loss: 0.005378698464483023 | Training Acc: 100.0 | Testing Loss: 0.006386778317391872 | Testing Acc: 100.0\n",
            "Epoch: 800 | Training Loss: 0.005370669532567263 | Training Acc: 100.0 | Testing Loss: 0.006367943249642849 | Testing Acc: 100.0\n",
            "Epoch: 801 | Training Loss: 0.005362219177186489 | Training Acc: 100.0 | Testing Loss: 0.006361828185617924 | Testing Acc: 100.0\n",
            "Epoch: 802 | Training Loss: 0.005354371387511492 | Training Acc: 100.0 | Testing Loss: 0.0063581885769963264 | Testing Acc: 100.0\n",
            "Epoch: 803 | Training Loss: 0.005346033256500959 | Training Acc: 100.0 | Testing Loss: 0.006350879557430744 | Testing Acc: 100.0\n",
            "Epoch: 804 | Training Loss: 0.005338014103472233 | Training Acc: 100.0 | Testing Loss: 0.0063466280698776245 | Testing Acc: 100.0\n",
            "Epoch: 805 | Training Loss: 0.005329924635589123 | Training Acc: 100.0 | Testing Loss: 0.006338790990412235 | Testing Acc: 100.0\n",
            "Epoch: 806 | Training Loss: 0.005321744829416275 | Training Acc: 100.0 | Testing Loss: 0.006334060337394476 | Testing Acc: 100.0\n",
            "Epoch: 807 | Training Loss: 0.005313904024660587 | Training Acc: 100.0 | Testing Loss: 0.00632615527138114 | Testing Acc: 100.0\n",
            "Epoch: 808 | Training Loss: 0.005305586848407984 | Training Acc: 100.0 | Testing Loss: 0.006330035626888275 | Testing Acc: 100.0\n",
            "Epoch: 809 | Training Loss: 0.005297972355037928 | Training Acc: 100.0 | Testing Loss: 0.00631073210388422 | Testing Acc: 100.0\n",
            "Epoch: 810 | Training Loss: 0.005289661698043346 | Training Acc: 100.0 | Testing Loss: 0.006303870584815741 | Testing Acc: 100.0\n",
            "Epoch: 811 | Training Loss: 0.005281842313706875 | Training Acc: 100.0 | Testing Loss: 0.006300128996372223 | Testing Acc: 100.0\n",
            "Epoch: 812 | Training Loss: 0.005273829679936171 | Training Acc: 100.0 | Testing Loss: 0.006292678415775299 | Testing Acc: 100.0\n",
            "Epoch: 813 | Training Loss: 0.005265865009278059 | Training Acc: 100.0 | Testing Loss: 0.00628810515627265 | Testing Acc: 100.0\n",
            "Epoch: 814 | Training Loss: 0.005258047021925449 | Training Acc: 100.0 | Testing Loss: 0.006280338857322931 | Testing Acc: 100.0\n",
            "Epoch: 815 | Training Loss: 0.005249974317848682 | Training Acc: 100.0 | Testing Loss: 0.006275581661611795 | Testing Acc: 100.0\n",
            "Epoch: 816 | Training Loss: 0.005242365412414074 | Training Acc: 100.0 | Testing Loss: 0.0062675075605511665 | Testing Acc: 100.0\n",
            "Epoch: 817 | Training Loss: 0.005234253592789173 | Training Acc: 100.0 | Testing Loss: 0.006260192487388849 | Testing Acc: 100.0\n",
            "Epoch: 818 | Training Loss: 0.005226650275290012 | Training Acc: 100.0 | Testing Loss: 0.006255725864320993 | Testing Acc: 100.0\n",
            "Epoch: 819 | Training Loss: 0.005218668840825558 | Training Acc: 100.0 | Testing Loss: 0.006247914396226406 | Testing Acc: 100.0\n",
            "Epoch: 820 | Training Loss: 0.0052109393291175365 | Training Acc: 100.0 | Testing Loss: 0.006243361625820398 | Testing Acc: 100.0\n",
            "Epoch: 821 | Training Loss: 0.005203185137361288 | Training Acc: 100.0 | Testing Loss: 0.0062354025430977345 | Testing Acc: 100.0\n",
            "Epoch: 822 | Training Loss: 0.005195281468331814 | Training Acc: 100.0 | Testing Loss: 0.006230744533240795 | Testing Acc: 100.0\n",
            "Epoch: 823 | Training Loss: 0.005187776871025562 | Training Acc: 100.0 | Testing Loss: 0.006222694180905819 | Testing Acc: 100.0\n",
            "Epoch: 824 | Training Loss: 0.005179789382964373 | Training Acc: 100.0 | Testing Loss: 0.006206846330314875 | Testing Acc: 100.0\n",
            "Epoch: 825 | Training Loss: 0.0051723956130445 | Training Acc: 100.0 | Testing Loss: 0.006213295739144087 | Testing Acc: 100.0\n",
            "Epoch: 826 | Training Loss: 0.005164484027773142 | Training Acc: 100.0 | Testing Loss: 0.006204861216247082 | Testing Acc: 100.0\n",
            "Epoch: 827 | Training Loss: 0.0051568946801126 | Training Acc: 100.0 | Testing Loss: 0.0061997766606509686 | Testing Acc: 100.0\n",
            "Epoch: 828 | Training Loss: 0.005149253644049168 | Training Acc: 100.0 | Testing Loss: 0.0061917053535580635 | Testing Acc: 100.0\n",
            "Epoch: 829 | Training Loss: 0.00514153391122818 | Training Acc: 100.0 | Testing Loss: 0.00618677819147706 | Testing Acc: 100.0\n",
            "Epoch: 830 | Training Loss: 0.005134068429470062 | Training Acc: 100.0 | Testing Loss: 0.006178758107125759 | Testing Acc: 100.0\n",
            "Epoch: 831 | Training Loss: 0.005126246716827154 | Training Acc: 100.0 | Testing Loss: 0.006182675715535879 | Testing Acc: 100.0\n",
            "Epoch: 832 | Training Loss: 0.005119040608406067 | Training Acc: 100.0 | Testing Loss: 0.0061641414649784565 | Testing Acc: 100.0\n",
            "Epoch: 833 | Training Loss: 0.005111190490424633 | Training Acc: 100.0 | Testing Loss: 0.0061576166190207005 | Testing Acc: 100.0\n",
            "Epoch: 834 | Training Loss: 0.005103800445795059 | Training Acc: 100.0 | Testing Loss: 0.006153772585093975 | Testing Acc: 100.0\n",
            "Epoch: 835 | Training Loss: 0.005096200853586197 | Training Acc: 100.0 | Testing Loss: 0.006146457511931658 | Testing Acc: 100.0\n",
            "Epoch: 836 | Training Loss: 0.0050886827521026134 | Training Acc: 100.0 | Testing Loss: 0.0061423033475875854 | Testing Acc: 100.0\n",
            "Epoch: 837 | Training Loss: 0.005081311799585819 | Training Acc: 100.0 | Testing Loss: 0.0061346604488790035 | Testing Acc: 100.0\n",
            "Epoch: 838 | Training Loss: 0.005073638167232275 | Training Acc: 100.0 | Testing Loss: 0.006138343829661608 | Testing Acc: 100.0\n",
            "Epoch: 839 | Training Loss: 0.005066537763923407 | Training Acc: 100.0 | Testing Loss: 0.006120305508375168 | Testing Acc: 100.0\n",
            "Epoch: 840 | Training Loss: 0.005058794282376766 | Training Acc: 100.0 | Testing Loss: 0.006113752722740173 | Testing Acc: 100.0\n",
            "Epoch: 841 | Training Loss: 0.005051569081842899 | Training Acc: 100.0 | Testing Loss: 0.006110160145908594 | Testing Acc: 100.0\n",
            "Epoch: 842 | Training Loss: 0.005044072866439819 | Training Acc: 100.0 | Testing Loss: 0.0061028944328427315 | Testing Acc: 100.0\n",
            "Epoch: 843 | Training Loss: 0.005036708898842335 | Training Acc: 100.0 | Testing Loss: 0.006106797605752945 | Testing Acc: 100.0\n",
            "Epoch: 844 | Training Loss: 0.005029461346566677 | Training Acc: 100.0 | Testing Loss: 0.006089106202125549 | Testing Acc: 100.0\n",
            "Epoch: 845 | Training Loss: 0.005021978169679642 | Training Acc: 100.0 | Testing Loss: 0.006093363277614117 | Testing Acc: 100.0\n",
            "Epoch: 846 | Training Loss: 0.005014812108129263 | Training Acc: 100.0 | Testing Loss: 0.006076043471693993 | Testing Acc: 100.0\n",
            "Epoch: 847 | Training Loss: 0.0050073107704520226 | Training Acc: 100.0 | Testing Loss: 0.0060804905369877815 | Testing Acc: 100.0\n",
            "Epoch: 848 | Training Loss: 0.005000262521207333 | Training Acc: 100.0 | Testing Loss: 0.0060632918030023575 | Testing Acc: 100.0\n",
            "Epoch: 849 | Training Loss: 0.004992778412997723 | Training Acc: 100.0 | Testing Loss: 0.006057324819266796 | Testing Acc: 100.0\n",
            "Epoch: 850 | Training Loss: 0.004985678009688854 | Training Acc: 100.0 | Testing Loss: 0.006062147673219442 | Testing Acc: 100.0\n",
            "Epoch: 851 | Training Loss: 0.0049783592112362385 | Training Acc: 100.0 | Testing Loss: 0.006045134738087654 | Testing Acc: 100.0\n",
            "Epoch: 852 | Training Loss: 0.0049710990861058235 | Training Acc: 100.0 | Testing Loss: 0.006049805320799351 | Testing Acc: 100.0\n",
            "Epoch: 853 | Training Loss: 0.004963989835232496 | Training Acc: 100.0 | Testing Loss: 0.0060328831896185875 | Testing Acc: 100.0\n",
            "Epoch: 854 | Training Loss: 0.004956653341650963 | Training Acc: 100.0 | Testing Loss: 0.006037378683686256 | Testing Acc: 100.0\n",
            "Epoch: 855 | Training Loss: 0.004949703346937895 | Training Acc: 100.0 | Testing Loss: 0.006028532050549984 | Testing Acc: 100.0\n",
            "Epoch: 856 | Training Loss: 0.00494233425706625 | Training Acc: 100.0 | Testing Loss: 0.006012666970491409 | Testing Acc: 100.0\n",
            "Epoch: 857 | Training Loss: 0.0049353851936757565 | Training Acc: 100.0 | Testing Loss: 0.00601781764999032 | Testing Acc: 100.0\n",
            "Epoch: 858 | Training Loss: 0.004928148351609707 | Training Acc: 100.0 | Testing Loss: 0.006001732312142849 | Testing Acc: 100.0\n",
            "Epoch: 859 | Training Loss: 0.004921121057122946 | Training Acc: 100.0 | Testing Loss: 0.00600651279091835 | Testing Acc: 100.0\n",
            "Epoch: 860 | Training Loss: 0.004913997370749712 | Training Acc: 100.0 | Testing Loss: 0.0059979758225381374 | Testing Acc: 100.0\n",
            "Epoch: 861 | Training Loss: 0.004906824789941311 | Training Acc: 100.0 | Testing Loss: 0.00599278137087822 | Testing Acc: 100.0\n",
            "Epoch: 862 | Training Loss: 0.004899951163679361 | Training Acc: 100.0 | Testing Loss: 0.005984608083963394 | Testing Acc: 100.0\n",
            "Epoch: 863 | Training Loss: 0.004892685450613499 | Training Acc: 100.0 | Testing Loss: 0.005977326538413763 | Testing Acc: 100.0\n",
            "Epoch: 864 | Training Loss: 0.0048859138041734695 | Training Acc: 100.0 | Testing Loss: 0.005973016377538443 | Testing Acc: 100.0\n",
            "Epoch: 865 | Training Loss: 0.0048787351697683334 | Training Acc: 100.0 | Testing Loss: 0.0059655411168932915 | Testing Acc: 100.0\n",
            "Epoch: 866 | Training Loss: 0.004871824756264687 | Training Acc: 100.0 | Testing Loss: 0.005968932062387466 | Testing Acc: 100.0\n",
            "Epoch: 867 | Training Loss: 0.004864883609116077 | Training Acc: 100.0 | Testing Loss: 0.00595168536528945 | Testing Acc: 100.0\n",
            "Epoch: 868 | Training Loss: 0.0048578521236777306 | Training Acc: 100.0 | Testing Loss: 0.005955737549811602 | Testing Acc: 100.0\n",
            "Epoch: 869 | Training Loss: 0.004851070232689381 | Training Acc: 100.0 | Testing Loss: 0.005939089693129063 | Testing Acc: 100.0\n",
            "Epoch: 870 | Training Loss: 0.004843942355364561 | Training Acc: 100.0 | Testing Loss: 0.0059431167319417 | Testing Acc: 100.0\n",
            "Epoch: 871 | Training Loss: 0.004837234504520893 | Training Acc: 100.0 | Testing Loss: 0.0059267254546284676 | Testing Acc: 100.0\n",
            "Epoch: 872 | Training Loss: 0.004830182529985905 | Training Acc: 100.0 | Testing Loss: 0.005920697003602982 | Testing Acc: 100.0\n",
            "Epoch: 873 | Training Loss: 0.00482346024364233 | Training Acc: 100.0 | Testing Loss: 0.0059249443002045155 | Testing Acc: 100.0\n",
            "Epoch: 874 | Training Loss: 0.00481647253036499 | Training Acc: 100.0 | Testing Loss: 0.005908888764679432 | Testing Acc: 100.0\n",
            "Epoch: 875 | Training Loss: 0.004809713456779718 | Training Acc: 100.0 | Testing Loss: 0.005913160741329193 | Testing Acc: 100.0\n",
            "Epoch: 876 | Training Loss: 0.004802843555808067 | Training Acc: 100.0 | Testing Loss: 0.005904468707740307 | Testing Acc: 100.0\n",
            "Epoch: 877 | Training Loss: 0.004795932210981846 | Training Acc: 100.0 | Testing Loss: 0.00589944701641798 | Testing Acc: 100.0\n",
            "Epoch: 878 | Training Loss: 0.004789319820702076 | Training Acc: 100.0 | Testing Loss: 0.005891350097954273 | Testing Acc: 100.0\n",
            "Epoch: 879 | Training Loss: 0.00478230370208621 | Training Acc: 100.0 | Testing Loss: 0.0058840056881308556 | Testing Acc: 100.0\n",
            "Epoch: 880 | Training Loss: 0.004775782115757465 | Training Acc: 100.0 | Testing Loss: 0.0058799623511731625 | Testing Acc: 100.0\n",
            "Epoch: 881 | Training Loss: 0.0047688730992376804 | Training Acc: 100.0 | Testing Loss: 0.005872552283108234 | Testing Acc: 100.0\n",
            "Epoch: 882 | Training Loss: 0.004762211348861456 | Training Acc: 100.0 | Testing Loss: 0.00587571132928133 | Testing Acc: 100.0\n",
            "Epoch: 883 | Training Loss: 0.004755544476211071 | Training Acc: 100.0 | Testing Loss: 0.005859129596501589 | Testing Acc: 100.0\n",
            "Epoch: 884 | Training Loss: 0.004748758859932423 | Training Acc: 100.0 | Testing Loss: 0.0058627622202038765 | Testing Acc: 100.0\n",
            "Epoch: 885 | Training Loss: 0.004742134362459183 | Training Acc: 100.0 | Testing Loss: 0.005846629850566387 | Testing Acc: 100.0\n",
            "Epoch: 886 | Training Loss: 0.004735362250357866 | Training Acc: 100.0 | Testing Loss: 0.005850729998201132 | Testing Acc: 100.0\n",
            "Epoch: 887 | Training Loss: 0.004728853236883879 | Training Acc: 100.0 | Testing Loss: 0.005841965787112713 | Testing Acc: 100.0\n",
            "Epoch: 888 | Training Loss: 0.004722021520137787 | Training Acc: 100.0 | Testing Loss: 0.0058270706795156 | Testing Acc: 100.0\n",
            "Epoch: 889 | Training Loss: 0.004715623799711466 | Training Acc: 100.0 | Testing Loss: 0.0058316513895988464 | Testing Acc: 100.0\n",
            "Epoch: 890 | Training Loss: 0.004708828870207071 | Training Acc: 100.0 | Testing Loss: 0.005823569372296333 | Testing Acc: 100.0\n",
            "Epoch: 891 | Training Loss: 0.004702269099652767 | Training Acc: 100.0 | Testing Loss: 0.005818978883326054 | Testing Acc: 100.0\n",
            "Epoch: 892 | Training Loss: 0.004695739597082138 | Training Acc: 100.0 | Testing Loss: 0.005811239592730999 | Testing Acc: 100.0\n",
            "Epoch: 893 | Training Loss: 0.004689044319093227 | Training Acc: 100.0 | Testing Loss: 0.005813946947455406 | Testing Acc: 100.0\n",
            "Epoch: 894 | Training Loss: 0.004682702012360096 | Training Acc: 100.0 | Testing Loss: 0.005797527264803648 | Testing Acc: 100.0\n",
            "Epoch: 895 | Training Loss: 0.0046759373508393764 | Training Acc: 100.0 | Testing Loss: 0.00580087024718523 | Testing Acc: 100.0\n",
            "Epoch: 896 | Training Loss: 0.004669633693993092 | Training Acc: 100.0 | Testing Loss: 0.005792082753032446 | Testing Acc: 100.0\n",
            "Epoch: 897 | Training Loss: 0.004662956111133099 | Training Acc: 100.0 | Testing Loss: 0.005777102895081043 | Testing Acc: 100.0\n",
            "Epoch: 898 | Training Loss: 0.004656617995351553 | Training Acc: 100.0 | Testing Loss: 0.0057814884930849075 | Testing Acc: 100.0\n",
            "Epoch: 899 | Training Loss: 0.004650021903216839 | Training Acc: 100.0 | Testing Loss: 0.005773480515927076 | Testing Acc: 100.0\n",
            "Epoch: 900 | Training Loss: 0.004643559455871582 | Training Acc: 100.0 | Testing Loss: 0.005768804810941219 | Testing Acc: 100.0\n",
            "Epoch: 901 | Training Loss: 0.0046371715143322945 | Training Acc: 100.0 | Testing Loss: 0.005761139560490847 | Testing Acc: 100.0\n",
            "Epoch: 902 | Training Loss: 0.004630601033568382 | Training Acc: 100.0 | Testing Loss: 0.00576389953494072 | Testing Acc: 100.0\n",
            "Epoch: 903 | Training Loss: 0.004624401219189167 | Training Acc: 100.0 | Testing Loss: 0.005747571587562561 | Testing Acc: 100.0\n",
            "Epoch: 904 | Training Loss: 0.0046177818439900875 | Training Acc: 100.0 | Testing Loss: 0.005741487257182598 | Testing Acc: 100.0\n",
            "Epoch: 905 | Training Loss: 0.004611565265804529 | Training Acc: 100.0 | Testing Loss: 0.005745110101997852 | Testing Acc: 100.0\n",
            "Epoch: 906 | Training Loss: 0.004605080466717482 | Training Acc: 100.0 | Testing Loss: 0.005729579832404852 | Testing Acc: 100.0\n",
            "Epoch: 907 | Training Loss: 0.004598798230290413 | Training Acc: 100.0 | Testing Loss: 0.005733303260058165 | Testing Acc: 100.0\n",
            "Epoch: 908 | Training Loss: 0.004592422861605883 | Training Acc: 100.0 | Testing Loss: 0.005725041963160038 | Testing Acc: 100.0\n",
            "Epoch: 909 | Training Loss: 0.004586013033986092 | Training Acc: 100.0 | Testing Loss: 0.005720141343772411 | Testing Acc: 100.0\n",
            "Epoch: 910 | Training Loss: 0.004579807631671429 | Training Acc: 100.0 | Testing Loss: 0.005712359212338924 | Testing Acc: 100.0\n",
            "Epoch: 911 | Training Loss: 0.00457333866506815 | Training Acc: 100.0 | Testing Loss: 0.005714909173548222 | Testing Acc: 100.0\n",
            "Epoch: 912 | Training Loss: 0.004567291121929884 | Training Acc: 100.0 | Testing Loss: 0.005698794033378363 | Testing Acc: 100.0\n",
            "Epoch: 913 | Training Loss: 0.004560799337923527 | Training Acc: 100.0 | Testing Loss: 0.005692444741725922 | Testing Acc: 100.0\n",
            "Epoch: 914 | Training Loss: 0.004554706625640392 | Training Acc: 100.0 | Testing Loss: 0.005695966072380543 | Testing Acc: 100.0\n",
            "Epoch: 915 | Training Loss: 0.004548342432826757 | Training Acc: 100.0 | Testing Loss: 0.0056807175278663635 | Testing Acc: 100.0\n",
            "Epoch: 916 | Training Loss: 0.004542158916592598 | Training Acc: 100.0 | Testing Loss: 0.005684321280568838 | Testing Acc: 100.0\n",
            "Epoch: 917 | Training Loss: 0.004535974469035864 | Training Acc: 100.0 | Testing Loss: 0.0056761810556054115 | Testing Acc: 100.0\n",
            "Epoch: 918 | Training Loss: 0.004529634956270456 | Training Acc: 100.0 | Testing Loss: 0.005678142886608839 | Testing Acc: 100.0\n",
            "Epoch: 919 | Training Loss: 0.004523644223809242 | Training Acc: 100.0 | Testing Loss: 0.0056620328687131405 | Testing Acc: 100.0\n",
            "Epoch: 920 | Training Loss: 0.0045172483660280704 | Training Acc: 100.0 | Testing Loss: 0.00565551919862628 | Testing Acc: 100.0\n",
            "Epoch: 921 | Training Loss: 0.004511288367211819 | Training Acc: 100.0 | Testing Loss: 0.005658948328346014 | Testing Acc: 100.0\n",
            "Epoch: 922 | Training Loss: 0.004505009390413761 | Training Acc: 100.0 | Testing Loss: 0.005643858574330807 | Testing Acc: 100.0\n",
            "Epoch: 923 | Training Loss: 0.0044989632442593575 | Training Acc: 100.0 | Testing Loss: 0.005647267680615187 | Testing Acc: 100.0\n",
            "Epoch: 924 | Training Loss: 0.004492760170251131 | Training Acc: 100.0 | Testing Loss: 0.005639035254716873 | Testing Acc: 100.0\n",
            "Epoch: 925 | Training Loss: 0.004486622754484415 | Training Acc: 100.0 | Testing Loss: 0.005634368397295475 | Testing Acc: 100.0\n",
            "Epoch: 926 | Training Loss: 0.004480612464249134 | Training Acc: 100.0 | Testing Loss: 0.005626710131764412 | Testing Acc: 100.0\n",
            "Epoch: 927 | Training Loss: 0.004474381450563669 | Training Acc: 100.0 | Testing Loss: 0.0056290337815880775 | Testing Acc: 100.0\n",
            "Epoch: 928 | Training Loss: 0.004468531347811222 | Training Acc: 100.0 | Testing Loss: 0.005613491404801607 | Testing Acc: 100.0\n",
            "Epoch: 929 | Training Loss: 0.004462283570319414 | Training Acc: 100.0 | Testing Loss: 0.005607251543551683 | Testing Acc: 100.0\n",
            "Epoch: 930 | Training Loss: 0.004456395749002695 | Training Acc: 100.0 | Testing Loss: 0.005610543768852949 | Testing Acc: 100.0\n",
            "Epoch: 931 | Training Loss: 0.004450247157365084 | Training Acc: 100.0 | Testing Loss: 0.005595857743173838 | Testing Acc: 100.0\n",
            "Epoch: 932 | Training Loss: 0.004444332327693701 | Training Acc: 100.0 | Testing Loss: 0.005599305499345064 | Testing Acc: 100.0\n",
            "Epoch: 933 | Training Loss: 0.0044382670894265175 | Training Acc: 100.0 | Testing Loss: 0.005591234657913446 | Testing Acc: 100.0\n",
            "Epoch: 934 | Training Loss: 0.004432217683643103 | Training Acc: 100.0 | Testing Loss: 0.005593248642981052 | Testing Acc: 100.0\n",
            "Epoch: 935 | Training Loss: 0.004426368046551943 | Training Acc: 100.0 | Testing Loss: 0.005577550269663334 | Testing Acc: 100.0\n",
            "Epoch: 936 | Training Loss: 0.004420226439833641 | Training Acc: 100.0 | Testing Loss: 0.005580292548984289 | Testing Acc: 100.0\n",
            "Epoch: 937 | Training Loss: 0.004414496012032032 | Training Acc: 100.0 | Testing Loss: 0.005571860354393721 | Testing Acc: 100.0\n",
            "Epoch: 938 | Training Loss: 0.004408345557749271 | Training Acc: 100.0 | Testing Loss: 0.005564313847571611 | Testing Acc: 100.0\n",
            "Epoch: 939 | Training Loss: 0.00440255319699645 | Training Acc: 100.0 | Testing Loss: 0.005560362711548805 | Testing Acc: 100.0\n",
            "Epoch: 940 | Training Loss: 0.004396588541567326 | Training Acc: 100.0 | Testing Loss: 0.005553091410547495 | Testing Acc: 100.0\n",
            "Epoch: 941 | Training Loss: 0.004390698857605457 | Training Acc: 100.0 | Testing Loss: 0.005555459763854742 | Testing Acc: 100.0\n",
            "Epoch: 942 | Training Loss: 0.004384856205433607 | Training Acc: 100.0 | Testing Loss: 0.0055404724553227425 | Testing Acc: 100.0\n",
            "Epoch: 943 | Training Loss: 0.004378925077617168 | Training Acc: 100.0 | Testing Loss: 0.0055434321984648705 | Testing Acc: 100.0\n",
            "Epoch: 944 | Training Loss: 0.004373152740299702 | Training Acc: 100.0 | Testing Loss: 0.0055352007038891315 | Testing Acc: 100.0\n",
            "Epoch: 945 | Training Loss: 0.004367116838693619 | Training Acc: 100.0 | Testing Loss: 0.0055279904045164585 | Testing Acc: 100.0\n",
            "Epoch: 946 | Training Loss: 0.004361513536423445 | Training Acc: 100.0 | Testing Loss: 0.005523896776139736 | Testing Acc: 100.0\n",
            "Epoch: 947 | Training Loss: 0.00435551768168807 | Training Acc: 100.0 | Testing Loss: 0.005516842473298311 | Testing Acc: 100.0\n",
            "Epoch: 948 | Training Loss: 0.004349802620708942 | Training Acc: 100.0 | Testing Loss: 0.005519378464668989 | Testing Acc: 100.0\n",
            "Epoch: 949 | Training Loss: 0.004343981388956308 | Training Acc: 100.0 | Testing Loss: 0.005504549015313387 | Testing Acc: 100.0\n",
            "Epoch: 950 | Training Loss: 0.004338202066719532 | Training Acc: 100.0 | Testing Loss: 0.005507501307874918 | Testing Acc: 100.0\n",
            "Epoch: 951 | Training Loss: 0.004332450684159994 | Training Acc: 100.0 | Testing Loss: 0.0054994928650557995 | Testing Acc: 100.0\n",
            "Epoch: 952 | Training Loss: 0.004326594062149525 | Training Acc: 100.0 | Testing Loss: 0.005501123145222664 | Testing Acc: 100.0\n",
            "Epoch: 953 | Training Loss: 0.004321013577282429 | Training Acc: 100.0 | Testing Loss: 0.005485817790031433 | Testing Acc: 100.0\n",
            "Epoch: 954 | Training Loss: 0.0043150801211595535 | Training Acc: 100.0 | Testing Loss: 0.005479608196765184 | Testing Acc: 100.0\n",
            "Epoch: 955 | Training Loss: 0.004309572279453278 | Training Acc: 100.0 | Testing Loss: 0.005482470151036978 | Testing Acc: 100.0\n",
            "Epoch: 956 | Training Loss: 0.004303694702684879 | Training Acc: 100.0 | Testing Loss: 0.005468237213790417 | Testing Acc: 100.0\n",
            "Epoch: 957 | Training Loss: 0.004298098385334015 | Training Acc: 100.0 | Testing Loss: 0.005471701733767986 | Testing Acc: 100.0\n",
            "Epoch: 958 | Training Loss: 0.004292375408113003 | Training Acc: 100.0 | Testing Loss: 0.005463838577270508 | Testing Acc: 100.0\n",
            "Epoch: 959 | Training Loss: 0.004286644048988819 | Training Acc: 100.0 | Testing Loss: 0.005465530790388584 | Testing Acc: 100.0\n",
            "Epoch: 960 | Training Loss: 0.004281110595911741 | Training Acc: 100.0 | Testing Loss: 0.0054506342858076096 | Testing Acc: 100.0\n",
            "Epoch: 961 | Training Loss: 0.004275316372513771 | Training Acc: 100.0 | Testing Loss: 0.005453133024275303 | Testing Acc: 100.0\n",
            "Epoch: 962 | Training Loss: 0.004269823431968689 | Training Acc: 100.0 | Testing Loss: 0.005444906186312437 | Testing Acc: 100.0\n",
            "Epoch: 963 | Training Loss: 0.004264029674232006 | Training Acc: 100.0 | Testing Loss: 0.0054377177730202675 | Testing Acc: 100.0\n",
            "Epoch: 964 | Training Loss: 0.004258578643202782 | Training Acc: 100.0 | Testing Loss: 0.005439762957394123 | Testing Acc: 100.0\n",
            "Epoch: 965 | Training Loss: 0.004252905957400799 | Training Acc: 100.0 | Testing Loss: 0.005425170995295048 | Testing Acc: 100.0\n",
            "Epoch: 966 | Training Loss: 0.004247339908033609 | Training Acc: 100.0 | Testing Loss: 0.005427966825664043 | Testing Acc: 100.0\n",
            "Epoch: 967 | Training Loss: 0.004241749178618193 | Training Acc: 100.0 | Testing Loss: 0.005419929511845112 | Testing Acc: 100.0\n",
            "Epoch: 968 | Training Loss: 0.004236120730638504 | Training Acc: 100.0 | Testing Loss: 0.005415401421487331 | Testing Acc: 100.0\n",
            "Epoch: 969 | Training Loss: 0.004230694845318794 | Training Acc: 100.0 | Testing Loss: 0.005408161785453558 | Testing Acc: 100.0\n",
            "Epoch: 970 | Training Loss: 0.004225000273436308 | Training Acc: 100.0 | Testing Loss: 0.00541009521111846 | Testing Acc: 100.0\n",
            "Epoch: 971 | Training Loss: 0.004219642840325832 | Training Acc: 100.0 | Testing Loss: 0.005395637359470129 | Testing Acc: 100.0\n",
            "Epoch: 972 | Training Loss: 0.0042139762081205845 | Training Acc: 100.0 | Testing Loss: 0.005389746278524399 | Testing Acc: 100.0\n",
            "Epoch: 973 | Training Loss: 0.00420862901955843 | Training Acc: 100.0 | Testing Loss: 0.005392652470618486 | Testing Acc: 100.0\n",
            "Epoch: 974 | Training Loss: 0.004203003831207752 | Training Acc: 100.0 | Testing Loss: 0.005384945310652256 | Testing Acc: 100.0\n",
            "Epoch: 975 | Training Loss: 0.004197560716420412 | Training Acc: 100.0 | Testing Loss: 0.0053868480026721954 | Testing Acc: 100.0\n",
            "Epoch: 976 | Training Loss: 0.004192130174487829 | Training Acc: 100.0 | Testing Loss: 0.005372245330363512 | Testing Acc: 100.0\n",
            "Epoch: 977 | Training Loss: 0.004186590667814016 | Training Acc: 100.0 | Testing Loss: 0.005374608561396599 | Testing Acc: 100.0\n",
            "Epoch: 978 | Training Loss: 0.004181266762316227 | Training Acc: 100.0 | Testing Loss: 0.005366677418351173 | Testing Acc: 100.0\n",
            "Epoch: 979 | Training Loss: 0.004175649490207434 | Training Acc: 100.0 | Testing Loss: 0.0053595202043652534 | Testing Acc: 100.0\n",
            "Epoch: 980 | Training Loss: 0.004170411732047796 | Training Acc: 100.0 | Testing Loss: 0.005361481569707394 | Testing Acc: 100.0\n",
            "Epoch: 981 | Training Loss: 0.004164912737905979 | Training Acc: 100.0 | Testing Loss: 0.005347401835024357 | Testing Acc: 100.0\n",
            "Epoch: 982 | Training Loss: 0.004159592092037201 | Training Acc: 100.0 | Testing Loss: 0.005349968560039997 | Testing Acc: 100.0\n",
            "Epoch: 983 | Training Loss: 0.004154098220169544 | Training Acc: 100.0 | Testing Loss: 0.005342158488929272 | Testing Acc: 100.0\n",
            "Epoch: 984 | Training Loss: 0.004148730542510748 | Training Acc: 100.0 | Testing Loss: 0.005337870679795742 | Testing Acc: 100.0\n",
            "Epoch: 985 | Training Loss: 0.0041434126906096935 | Training Acc: 100.0 | Testing Loss: 0.005330677144229412 | Testing Acc: 100.0\n",
            "Epoch: 986 | Training Loss: 0.004137963522225618 | Training Acc: 100.0 | Testing Loss: 0.005332532338798046 | Testing Acc: 100.0\n",
            "Epoch: 987 | Training Loss: 0.004132765810936689 | Training Acc: 100.0 | Testing Loss: 0.005324388854205608 | Testing Acc: 100.0\n",
            "Epoch: 988 | Training Loss: 0.00412724819034338 | Training Acc: 100.0 | Testing Loss: 0.005317084025591612 | Testing Acc: 100.0\n",
            "Epoch: 989 | Training Loss: 0.00412210775539279 | Training Acc: 100.0 | Testing Loss: 0.005313092842698097 | Testing Acc: 100.0\n",
            "Epoch: 990 | Training Loss: 0.004116674419492483 | Training Acc: 100.0 | Testing Loss: 0.005306310020387173 | Testing Acc: 100.0\n",
            "Epoch: 991 | Training Loss: 0.004111465532332659 | Training Acc: 100.0 | Testing Loss: 0.0053083403035998344 | Testing Acc: 100.0\n",
            "Epoch: 992 | Training Loss: 0.004106106702238321 | Training Acc: 100.0 | Testing Loss: 0.005300299730151892 | Testing Acc: 100.0\n",
            "Epoch: 993 | Training Loss: 0.004100840538740158 | Training Acc: 100.0 | Testing Loss: 0.0052960105240345 | Testing Acc: 100.0\n",
            "Epoch: 994 | Training Loss: 0.0040956223383545876 | Training Acc: 100.0 | Testing Loss: 0.005288700107485056 | Testing Acc: 100.0\n",
            "Epoch: 995 | Training Loss: 0.004090254660695791 | Training Acc: 100.0 | Testing Loss: 0.005290365312248468 | Testing Acc: 100.0\n",
            "Epoch: 996 | Training Loss: 0.004085177090018988 | Training Acc: 100.0 | Testing Loss: 0.005282220896333456 | Testing Acc: 100.0\n",
            "Epoch: 997 | Training Loss: 0.004079768899828196 | Training Acc: 100.0 | Testing Loss: 0.0052749463357031345 | Testing Acc: 100.0\n",
            "Epoch: 998 | Training Loss: 0.004074708558619022 | Training Acc: 100.0 | Testing Loss: 0.005270945839583874 | Testing Acc: 100.0\n",
            "Epoch: 999 | Training Loss: 0.004069391172379255 | Training Acc: 100.0 | Testing Loss: 0.0052642012014985085 | Testing Acc: 100.0\n",
            "Epoch: 1000 | Training Loss: 0.004064264707267284 | Training Acc: 100.0 | Testing Loss: 0.005266140215098858 | Testing Acc: 100.0\n",
            "Epoch: 1001 | Training Loss: 0.004059025552123785 | Training Acc: 100.0 | Testing Loss: 0.005258181132376194 | Testing Acc: 100.0\n",
            "Epoch: 1002 | Training Loss: 0.004053830169141293 | Training Acc: 100.0 | Testing Loss: 0.005253770388662815 | Testing Acc: 100.0\n",
            "Epoch: 1003 | Training Loss: 0.0040487265214324 | Training Acc: 100.0 | Testing Loss: 0.005246574990451336 | Testing Acc: 100.0\n",
            "Epoch: 1004 | Training Loss: 0.004043455235660076 | Training Acc: 100.0 | Testing Loss: 0.0052482024766504765 | Testing Acc: 100.0\n",
            "Epoch: 1005 | Training Loss: 0.004038484301418066 | Training Acc: 100.0 | Testing Loss: 0.005240174476057291 | Testing Acc: 100.0\n",
            "Epoch: 1006 | Training Loss: 0.0040331678465008736 | Training Acc: 100.0 | Testing Loss: 0.00523301912471652 | Testing Acc: 100.0\n",
            "Epoch: 1007 | Training Loss: 0.004028179682791233 | Training Acc: 100.0 | Testing Loss: 0.005229108966886997 | Testing Acc: 100.0\n",
            "Epoch: 1008 | Training Loss: 0.0040229735895991325 | Training Acc: 100.0 | Testing Loss: 0.0052224742248654366 | Testing Acc: 100.0\n",
            "Epoch: 1009 | Training Loss: 0.004017953760921955 | Training Acc: 100.0 | Testing Loss: 0.005224558524787426 | Testing Acc: 100.0\n",
            "Epoch: 1010 | Training Loss: 0.00401281425729394 | Training Acc: 100.0 | Testing Loss: 0.005216691177338362 | Testing Acc: 100.0\n",
            "Epoch: 1011 | Training Loss: 0.00400769105181098 | Training Acc: 100.0 | Testing Loss: 0.005212358199059963 | Testing Acc: 100.0\n",
            "Epoch: 1012 | Training Loss: 0.004002732690423727 | Training Acc: 100.0 | Testing Loss: 0.005204159766435623 | Testing Acc: 100.0\n",
            "Epoch: 1013 | Training Loss: 0.003997525200247765 | Training Acc: 100.0 | Testing Loss: 0.005206113215535879 | Testing Acc: 100.0\n",
            "Epoch: 1014 | Training Loss: 0.003992666956037283 | Training Acc: 100.0 | Testing Loss: 0.005197235383093357 | Testing Acc: 100.0\n",
            "Epoch: 1015 | Training Loss: 0.003987449221313 | Training Acc: 100.0 | Testing Loss: 0.005190688651055098 | Testing Acc: 100.0\n",
            "Epoch: 1016 | Training Loss: 0.003982576075941324 | Training Acc: 100.0 | Testing Loss: 0.005186102353036404 | Testing Acc: 100.0\n",
            "Epoch: 1017 | Training Loss: 0.003977448213845491 | Training Acc: 100.0 | Testing Loss: 0.005180039908736944 | Testing Acc: 100.0\n",
            "Epoch: 1018 | Training Loss: 0.003972537349909544 | Training Acc: 100.0 | Testing Loss: 0.005181232001632452 | Testing Acc: 100.0\n",
            "Epoch: 1019 | Training Loss: 0.0039674933068454266 | Training Acc: 100.0 | Testing Loss: 0.0051687634550035 | Testing Acc: 100.0\n",
            "Epoch: 1020 | Training Loss: 0.003962525632232428 | Training Acc: 100.0 | Testing Loss: 0.005170467309653759 | Testing Acc: 100.0\n",
            "Epoch: 1021 | Training Loss: 0.003957570064812899 | Training Acc: 100.0 | Testing Loss: 0.005162669811397791 | Testing Acc: 100.0\n",
            "Epoch: 1022 | Training Loss: 0.003952495753765106 | Training Acc: 100.0 | Testing Loss: 0.0051647974178195 | Testing Acc: 100.0\n",
            "Epoch: 1023 | Training Loss: 0.003947692923247814 | Training Acc: 100.0 | Testing Loss: 0.005150845739990473 | Testing Acc: 100.0\n",
            "Epoch: 1024 | Training Loss: 0.0039426363073289394 | Training Acc: 100.0 | Testing Loss: 0.005145781673491001 | Testing Acc: 100.0\n",
            "Epoch: 1025 | Training Loss: 0.003937811590731144 | Training Acc: 100.0 | Testing Loss: 0.005147824063897133 | Testing Acc: 100.0\n",
            "Epoch: 1026 | Training Loss: 0.003932822495698929 | Training Acc: 100.0 | Testing Loss: 0.005141259636729956 | Testing Acc: 100.0\n",
            "Epoch: 1027 | Training Loss: 0.00392789114266634 | Training Acc: 100.0 | Testing Loss: 0.0051420326344668865 | Testing Acc: 100.0\n",
            "Epoch: 1028 | Training Loss: 0.003923067357391119 | Training Acc: 100.0 | Testing Loss: 0.005129426252096891 | Testing Acc: 100.0\n",
            "Epoch: 1029 | Training Loss: 0.003918076865375042 | Training Acc: 100.0 | Testing Loss: 0.0051308791153132915 | Testing Acc: 100.0\n",
            "Epoch: 1030 | Training Loss: 0.0039132931269705296 | Training Acc: 100.0 | Testing Loss: 0.00512410793453455 | Testing Acc: 100.0\n",
            "Epoch: 1031 | Training Loss: 0.003908262588083744 | Training Acc: 100.0 | Testing Loss: 0.005116914864629507 | Testing Acc: 100.0\n",
            "Epoch: 1032 | Training Loss: 0.0039035524241626263 | Training Acc: 100.0 | Testing Loss: 0.0051182955503463745 | Testing Acc: 100.0\n",
            "Epoch: 1033 | Training Loss: 0.0038985982537269592 | Training Acc: 100.0 | Testing Loss: 0.005105132237076759 | Testing Acc: 100.0\n",
            "Epoch: 1034 | Training Loss: 0.0038938396610319614 | Training Acc: 100.0 | Testing Loss: 0.005108234006911516 | Testing Acc: 100.0\n",
            "Epoch: 1035 | Training Loss: 0.003888945560902357 | Training Acc: 100.0 | Testing Loss: 0.005100573413074017 | Testing Acc: 100.0\n",
            "Epoch: 1036 | Training Loss: 0.0038841101340949535 | Training Acc: 100.0 | Testing Loss: 0.0051026055589318275 | Testing Acc: 100.0\n",
            "Epoch: 1037 | Training Loss: 0.0038793659768998623 | Training Acc: 100.0 | Testing Loss: 0.005089007783681154 | Testing Acc: 100.0\n",
            "Epoch: 1038 | Training Loss: 0.0038744460325688124 | Training Acc: 100.0 | Testing Loss: 0.005091696046292782 | Testing Acc: 100.0\n",
            "Epoch: 1039 | Training Loss: 0.0038697938434779644 | Training Acc: 100.0 | Testing Loss: 0.0050838179886341095 | Testing Acc: 100.0\n",
            "Epoch: 1040 | Training Loss: 0.0038648482877761126 | Training Acc: 100.0 | Testing Loss: 0.005076952278614044 | Testing Acc: 100.0\n",
            "Epoch: 1041 | Training Loss: 0.0038602198474109173 | Training Acc: 100.0 | Testing Loss: 0.005078521557152271 | Testing Acc: 100.0\n",
            "Epoch: 1042 | Training Loss: 0.0038554228376597166 | Training Acc: 100.0 | Testing Loss: 0.005066718440502882 | Testing Acc: 100.0\n",
            "Epoch: 1043 | Training Loss: 0.003850658657029271 | Training Acc: 100.0 | Testing Loss: 0.0050684311427176 | Testing Acc: 100.0\n",
            "Epoch: 1044 | Training Loss: 0.003845910308882594 | Training Acc: 100.0 | Testing Loss: 0.005062117241322994 | Testing Acc: 100.0\n",
            "Epoch: 1045 | Training Loss: 0.0038411126006394625 | Training Acc: 100.0 | Testing Loss: 0.005062801763415337 | Testing Acc: 100.0\n",
            "Epoch: 1046 | Training Loss: 0.0038365162909030914 | Training Acc: 100.0 | Testing Loss: 0.005050634033977985 | Testing Acc: 100.0\n",
            "Epoch: 1047 | Training Loss: 0.0038316540885716677 | Training Acc: 100.0 | Testing Loss: 0.005052002612501383 | Testing Acc: 100.0\n",
            "Epoch: 1048 | Training Loss: 0.003827090607956052 | Training Acc: 100.0 | Testing Loss: 0.005045424215495586 | Testing Acc: 100.0\n",
            "Epoch: 1049 | Training Loss: 0.0038222491275519133 | Training Acc: 100.0 | Testing Loss: 0.0050383941270411015 | Testing Acc: 100.0\n",
            "Epoch: 1050 | Training Loss: 0.003817654447630048 | Training Acc: 100.0 | Testing Loss: 0.005039718467742205 | Testing Acc: 100.0\n",
            "Epoch: 1051 | Training Loss: 0.003812931478023529 | Training Acc: 100.0 | Testing Loss: 0.005026985425502062 | Testing Acc: 100.0\n",
            "Epoch: 1052 | Training Loss: 0.0038082886021584272 | Training Acc: 100.0 | Testing Loss: 0.00502995727583766 | Testing Acc: 100.0\n",
            "Epoch: 1053 | Training Loss: 0.0038036219775676727 | Training Acc: 100.0 | Testing Loss: 0.005022532306611538 | Testing Acc: 100.0\n",
            "Epoch: 1054 | Training Loss: 0.0037988959811627865 | Training Acc: 100.0 | Testing Loss: 0.005024472251534462 | Testing Acc: 100.0\n",
            "Epoch: 1055 | Training Loss: 0.0037943795323371887 | Training Acc: 100.0 | Testing Loss: 0.00501132570207119 | Testing Acc: 100.0\n",
            "Epoch: 1056 | Training Loss: 0.0037895888090133667 | Training Acc: 100.0 | Testing Loss: 0.005012905690819025 | Testing Acc: 100.0\n",
            "Epoch: 1057 | Training Loss: 0.0037851363886147738 | Training Acc: 100.0 | Testing Loss: 0.0050054881721735 | Testing Acc: 100.0\n",
            "Epoch: 1058 | Training Loss: 0.003780402708798647 | Training Acc: 100.0 | Testing Loss: 0.004999959841370583 | Testing Acc: 100.0\n",
            "Epoch: 1059 | Training Loss: 0.0037758753169327974 | Training Acc: 100.0 | Testing Loss: 0.00500135961920023 | Testing Acc: 100.0\n",
            "Epoch: 1060 | Training Loss: 0.0037712741177529097 | Training Acc: 100.0 | Testing Loss: 0.004989848472177982 | Testing Acc: 100.0\n",
            "Epoch: 1061 | Training Loss: 0.0037666577845811844 | Training Acc: 100.0 | Testing Loss: 0.004991423804312944 | Testing Acc: 100.0\n",
            "Epoch: 1062 | Training Loss: 0.0037621036171913147 | Training Acc: 100.0 | Testing Loss: 0.00498519791290164 | Testing Acc: 100.0\n",
            "Epoch: 1063 | Training Loss: 0.0037574588786810637 | Training Acc: 100.0 | Testing Loss: 0.004985877312719822 | Testing Acc: 100.0\n",
            "Epoch: 1064 | Training Loss: 0.0037530232220888138 | Training Acc: 100.0 | Testing Loss: 0.0049730874598026276 | Testing Acc: 100.0\n",
            "Epoch: 1065 | Training Loss: 0.003748317714780569 | Training Acc: 100.0 | Testing Loss: 0.004967282526195049 | Testing Acc: 100.0\n",
            "Epoch: 1066 | Training Loss: 0.0037439391016960144 | Training Acc: 100.0 | Testing Loss: 0.004970281384885311 | Testing Acc: 100.0\n",
            "Epoch: 1067 | Training Loss: 0.003739276435226202 | Training Acc: 100.0 | Testing Loss: 0.004963100887835026 | Testing Acc: 100.0\n",
            "Epoch: 1068 | Training Loss: 0.0037348181940615177 | Training Acc: 100.0 | Testing Loss: 0.0049650720320641994 | Testing Acc: 100.0\n",
            "Epoch: 1069 | Training Loss: 0.003730282187461853 | Training Acc: 100.0 | Testing Loss: 0.004952306859195232 | Testing Acc: 100.0\n",
            "Epoch: 1070 | Training Loss: 0.003725764574483037 | Training Acc: 100.0 | Testing Loss: 0.004953878931701183 | Testing Acc: 100.0\n",
            "Epoch: 1071 | Training Loss: 0.0037213177420198917 | Training Acc: 100.0 | Testing Loss: 0.004946642555296421 | Testing Acc: 100.0\n",
            "Epoch: 1072 | Training Loss: 0.003716739360243082 | Training Acc: 100.0 | Testing Loss: 0.004948534071445465 | Testing Acc: 100.0\n",
            "Epoch: 1073 | Training Loss: 0.0037123970687389374 | Training Acc: 100.0 | Testing Loss: 0.004940597806125879 | Testing Acc: 100.0\n",
            "Epoch: 1074 | Training Loss: 0.0037077940069139004 | Training Acc: 100.0 | Testing Loss: 0.004930068738758564 | Testing Acc: 100.0\n",
            "Epoch: 1075 | Training Loss: 0.0037034794222563505 | Training Acc: 100.0 | Testing Loss: 0.004931933246552944 | Testing Acc: 100.0\n",
            "Epoch: 1076 | Training Loss: 0.0036989145446568727 | Training Acc: 100.0 | Testing Loss: 0.004925142973661423 | Testing Acc: 100.0\n",
            "Epoch: 1077 | Training Loss: 0.0036945000756531954 | Training Acc: 100.0 | Testing Loss: 0.0049262200482189655 | Testing Acc: 100.0\n",
            "Epoch: 1078 | Training Loss: 0.003690097015351057 | Training Acc: 100.0 | Testing Loss: 0.0049197799526154995 | Testing Acc: 100.0\n",
            "Epoch: 1079 | Training Loss: 0.0036855905782431364 | Training Acc: 100.0 | Testing Loss: 0.004920146428048611 | Testing Acc: 100.0\n",
            "Epoch: 1080 | Training Loss: 0.0036813095211982727 | Training Acc: 100.0 | Testing Loss: 0.004908467642962933 | Testing Acc: 100.0\n",
            "Epoch: 1081 | Training Loss: 0.003676753956824541 | Training Acc: 100.0 | Testing Loss: 0.004909567069262266 | Testing Acc: 100.0\n",
            "Epoch: 1082 | Training Loss: 0.0036725059617310762 | Training Acc: 100.0 | Testing Loss: 0.004903202876448631 | Testing Acc: 100.0\n",
            "Epoch: 1083 | Training Loss: 0.003667979734018445 | Training Acc: 100.0 | Testing Loss: 0.0048964256420731544 | Testing Acc: 100.0\n",
            "Epoch: 1084 | Training Loss: 0.0036636728327721357 | Training Acc: 100.0 | Testing Loss: 0.004897581413388252 | Testing Acc: 100.0\n",
            "Epoch: 1085 | Training Loss: 0.0036592502146959305 | Training Acc: 100.0 | Testing Loss: 0.004885549657046795 | Testing Acc: 100.0\n",
            "Epoch: 1086 | Training Loss: 0.0036549102514982224 | Training Acc: 100.0 | Testing Loss: 0.0048882500268518925 | Testing Acc: 100.0\n",
            "Epoch: 1087 | Training Loss: 0.0036505665630102158 | Training Acc: 100.0 | Testing Loss: 0.004881176166236401 | Testing Acc: 100.0\n",
            "Epoch: 1088 | Training Loss: 0.003646139521151781 | Training Acc: 100.0 | Testing Loss: 0.004882936365902424 | Testing Acc: 100.0\n",
            "Epoch: 1089 | Training Loss: 0.003641920629888773 | Training Acc: 100.0 | Testing Loss: 0.004875211976468563 | Testing Acc: 100.0\n",
            "Epoch: 1090 | Training Loss: 0.003637439338490367 | Training Acc: 100.0 | Testing Loss: 0.004868661053478718 | Testing Acc: 100.0\n",
            "Epoch: 1091 | Training Loss: 0.0036332656163722277 | Training Acc: 100.0 | Testing Loss: 0.004869602154940367 | Testing Acc: 100.0\n",
            "Epoch: 1092 | Training Loss: 0.003628892358392477 | Training Acc: 100.0 | Testing Loss: 0.004858630243688822 | Testing Acc: 100.0\n",
            "Epoch: 1093 | Training Loss: 0.003624629694968462 | Training Acc: 100.0 | Testing Loss: 0.004860016517341137 | Testing Acc: 100.0\n",
            "Epoch: 1094 | Training Loss: 0.0036202799528837204 | Training Acc: 100.0 | Testing Loss: 0.004854009486734867 | Testing Acc: 100.0\n",
            "Epoch: 1095 | Training Loss: 0.0036159821320325136 | Training Acc: 100.0 | Testing Loss: 0.004854566417634487 | Testing Acc: 100.0\n",
            "Epoch: 1096 | Training Loss: 0.003611744847148657 | Training Acc: 100.0 | Testing Loss: 0.004843369126319885 | Testing Acc: 100.0\n",
            "Epoch: 1097 | Training Loss: 0.003607422113418579 | Training Acc: 100.0 | Testing Loss: 0.004844523034989834 | Testing Acc: 100.0\n",
            "Epoch: 1098 | Training Loss: 0.003603234188631177 | Training Acc: 100.0 | Testing Loss: 0.004837468732148409 | Testing Acc: 100.0\n",
            "Epoch: 1099 | Training Loss: 0.0035988539457321167 | Training Acc: 100.0 | Testing Loss: 0.0048311566933989525 | Testing Acc: 100.0\n",
            "Epoch: 1100 | Training Loss: 0.003594734240323305 | Training Acc: 100.0 | Testing Loss: 0.00483341421931982 | Testing Acc: 100.0\n",
            "Epoch: 1101 | Training Loss: 0.0035904161632061005 | Training Acc: 100.0 | Testing Loss: 0.004826178774237633 | Testing Acc: 100.0\n",
            "Epoch: 1102 | Training Loss: 0.0035862033255398273 | Training Acc: 100.0 | Testing Loss: 0.004826800432056189 | Testing Acc: 100.0\n",
            "Epoch: 1103 | Training Loss: 0.0035819862969219685 | Training Acc: 100.0 | Testing Loss: 0.004814769607037306 | Testing Acc: 100.0\n",
            "Epoch: 1104 | Training Loss: 0.0035777478478848934 | Training Acc: 100.0 | Testing Loss: 0.00481725949794054 | Testing Acc: 100.0\n",
            "Epoch: 1105 | Training Loss: 0.0035736076533794403 | Training Acc: 100.0 | Testing Loss: 0.004810132086277008 | Testing Acc: 100.0\n",
            "Epoch: 1106 | Training Loss: 0.003569274442270398 | Training Acc: 100.0 | Testing Loss: 0.00481165898963809 | Testing Acc: 100.0\n",
            "Epoch: 1107 | Training Loss: 0.0035652476362884045 | Training Acc: 100.0 | Testing Loss: 0.0048040053807199 | Testing Acc: 100.0\n",
            "Epoch: 1108 | Training Loss: 0.0035609230399131775 | Training Acc: 100.0 | Testing Loss: 0.004797358997166157 | Testing Acc: 100.0\n",
            "Epoch: 1109 | Training Loss: 0.0035568582825362682 | Training Acc: 100.0 | Testing Loss: 0.004798232112079859 | Testing Acc: 100.0\n",
            "Epoch: 1110 | Training Loss: 0.0035526524297893047 | Training Acc: 100.0 | Testing Loss: 0.004787561949342489 | Testing Acc: 100.0\n",
            "Epoch: 1111 | Training Loss: 0.0035485215485095978 | Training Acc: 100.0 | Testing Loss: 0.004788847174495459 | Testing Acc: 100.0\n",
            "Epoch: 1112 | Training Loss: 0.0035443496890366077 | Training Acc: 100.0 | Testing Loss: 0.004782964009791613 | Testing Acc: 100.0\n",
            "Epoch: 1113 | Training Loss: 0.0035401813220232725 | Training Acc: 100.0 | Testing Loss: 0.00478348508477211 | Testing Acc: 100.0\n",
            "Epoch: 1114 | Training Loss: 0.0035361037589609623 | Training Acc: 100.0 | Testing Loss: 0.004776185378432274 | Testing Acc: 100.0\n",
            "Epoch: 1115 | Training Loss: 0.003531848546117544 | Training Acc: 100.0 | Testing Loss: 0.004776478745043278 | Testing Acc: 100.0\n",
            "Epoch: 1116 | Training Loss: 0.0035279393196105957 | Training Acc: 100.0 | Testing Loss: 0.004765466321259737 | Testing Acc: 100.0\n",
            "Epoch: 1117 | Training Loss: 0.003523659659549594 | Training Acc: 100.0 | Testing Loss: 0.004759640898555517 | Testing Acc: 100.0\n",
            "Epoch: 1118 | Training Loss: 0.0035196468234062195 | Training Acc: 100.0 | Testing Loss: 0.004761326126754284 | Testing Acc: 100.0\n",
            "Epoch: 1119 | Training Loss: 0.0035154924262315035 | Training Acc: 100.0 | Testing Loss: 0.00475469883531332 | Testing Acc: 100.0\n",
            "Epoch: 1120 | Training Loss: 0.0035114213824272156 | Training Acc: 100.0 | Testing Loss: 0.004756434820592403 | Testing Acc: 100.0\n",
            "Epoch: 1121 | Training Loss: 0.0035073612816631794 | Training Acc: 100.0 | Testing Loss: 0.004749103914946318 | Testing Acc: 100.0\n",
            "Epoch: 1122 | Training Loss: 0.0035032243467867374 | Training Acc: 100.0 | Testing Loss: 0.004746000282466412 | Testing Acc: 100.0\n",
            "Epoch: 1123 | Training Loss: 0.0034992792643606663 | Training Acc: 100.0 | Testing Loss: 0.004739326424896717 | Testing Acc: 100.0\n",
            "Epoch: 1124 | Training Loss: 0.0034950703848153353 | Training Acc: 100.0 | Testing Loss: 0.004733473528176546 | Testing Acc: 100.0\n",
            "Epoch: 1125 | Training Loss: 0.0034911930561065674 | Training Acc: 100.0 | Testing Loss: 0.004734732210636139 | Testing Acc: 100.0\n",
            "Epoch: 1126 | Training Loss: 0.003487031441181898 | Training Acc: 100.0 | Testing Loss: 0.004729013424366713 | Testing Acc: 100.0\n",
            "Epoch: 1127 | Training Loss: 0.0034830737859010696 | Training Acc: 100.0 | Testing Loss: 0.0047295186668634415 | Testing Acc: 100.0\n",
            "Epoch: 1128 | Training Loss: 0.003479003207758069 | Training Acc: 100.0 | Testing Loss: 0.004718943499028683 | Testing Acc: 100.0\n",
            "Epoch: 1129 | Training Loss: 0.0034750259947031736 | Training Acc: 100.0 | Testing Loss: 0.004720008932054043 | Testing Acc: 100.0\n",
            "Epoch: 1130 | Training Loss: 0.0034709982573986053 | Training Acc: 100.0 | Testing Loss: 0.004713310860097408 | Testing Acc: 100.0\n",
            "Epoch: 1131 | Training Loss: 0.0034669465385377407 | Training Acc: 100.0 | Testing Loss: 0.004713902249932289 | Testing Acc: 100.0\n",
            "Epoch: 1132 | Training Loss: 0.0034630466252565384 | Training Acc: 100.0 | Testing Loss: 0.0047077094204723835 | Testing Acc: 100.0\n",
            "Epoch: 1133 | Training Loss: 0.0034589446149766445 | Training Acc: 100.0 | Testing Loss: 0.004701226484030485 | Testing Acc: 100.0\n",
            "Epoch: 1134 | Training Loss: 0.003455077763646841 | Training Acc: 100.0 | Testing Loss: 0.004702261183410883 | Testing Acc: 100.0\n",
            "Epoch: 1135 | Training Loss: 0.003451025579124689 | Training Acc: 100.0 | Testing Loss: 0.004695321898907423 | Testing Acc: 100.0\n",
            "Epoch: 1136 | Training Loss: 0.0034470960963517427 | Training Acc: 100.0 | Testing Loss: 0.004696554504334927 | Testing Acc: 100.0\n",
            "Epoch: 1137 | Training Loss: 0.003443139372393489 | Training Acc: 100.0 | Testing Loss: 0.004684871062636375 | Testing Acc: 100.0\n",
            "Epoch: 1138 | Training Loss: 0.003439137013629079 | Training Acc: 100.0 | Testing Loss: 0.0046859318390488625 | Testing Acc: 100.0\n",
            "Epoch: 1139 | Training Loss: 0.0034352480433881283 | Training Acc: 100.0 | Testing Loss: 0.0046791797503829 | Testing Acc: 100.0\n",
            "Epoch: 1140 | Training Loss: 0.003431223798543215 | Training Acc: 100.0 | Testing Loss: 0.004680636338889599 | Testing Acc: 100.0\n",
            "Epoch: 1141 | Training Loss: 0.0034274146892130375 | Training Acc: 100.0 | Testing Loss: 0.004673396237194538 | Testing Acc: 100.0\n",
            "Epoch: 1142 | Training Loss: 0.003423349466174841 | Training Acc: 100.0 | Testing Loss: 0.004667914938181639 | Testing Acc: 100.0\n",
            "Epoch: 1143 | Training Loss: 0.0034195829648524523 | Training Acc: 100.0 | Testing Loss: 0.004668509587645531 | Testing Acc: 100.0\n",
            "Epoch: 1144 | Training Loss: 0.003415559185668826 | Training Acc: 100.0 | Testing Loss: 0.004657466895878315 | Testing Acc: 100.0\n",
            "Epoch: 1145 | Training Loss: 0.003411731915548444 | Training Acc: 100.0 | Testing Loss: 0.0046587297692894936 | Testing Acc: 100.0\n",
            "Epoch: 1146 | Training Loss: 0.0034077968448400497 | Training Acc: 100.0 | Testing Loss: 0.004653193987905979 | Testing Acc: 100.0\n",
            "Epoch: 1147 | Training Loss: 0.003403905313462019 | Training Acc: 100.0 | Testing Loss: 0.00465369364246726 | Testing Acc: 100.0\n",
            "Epoch: 1148 | Training Loss: 0.0034000552259385586 | Training Acc: 100.0 | Testing Loss: 0.004646780900657177 | Testing Acc: 100.0\n",
            "Epoch: 1149 | Training Loss: 0.0033960998989641666 | Training Acc: 100.0 | Testing Loss: 0.0046472130343317986 | Testing Acc: 100.0\n",
            "Epoch: 1150 | Training Loss: 0.003392369020730257 | Training Acc: 100.0 | Testing Loss: 0.004640935454517603 | Testing Acc: 100.0\n",
            "Epoch: 1151 | Training Loss: 0.0033883850555866957 | Training Acc: 100.0 | Testing Loss: 0.004634440876543522 | Testing Acc: 100.0\n",
            "Epoch: 1152 | Training Loss: 0.0033846101723611355 | Training Acc: 100.0 | Testing Loss: 0.004635919816792011 | Testing Acc: 100.0\n",
            "Epoch: 1153 | Training Loss: 0.003380729351192713 | Training Acc: 100.0 | Testing Loss: 0.0046248240396380424 | Testing Acc: 100.0\n",
            "Epoch: 1154 | Training Loss: 0.0033769141882658005 | Training Acc: 100.0 | Testing Loss: 0.004625936038792133 | Testing Acc: 100.0\n",
            "Epoch: 1155 | Training Loss: 0.0033730906434357166 | Training Acc: 100.0 | Testing Loss: 0.004620381165295839 | Testing Acc: 100.0\n",
            "Epoch: 1156 | Training Loss: 0.0033692284487187862 | Training Acc: 100.0 | Testing Loss: 0.004620806314051151 | Testing Acc: 100.0\n",
            "Epoch: 1157 | Training Loss: 0.0033654775470495224 | Training Acc: 100.0 | Testing Loss: 0.0046147448010742664 | Testing Acc: 100.0\n",
            "Epoch: 1158 | Training Loss: 0.0033615597058087587 | Training Acc: 100.0 | Testing Loss: 0.004614760167896748 | Testing Acc: 100.0\n",
            "Epoch: 1159 | Training Loss: 0.003357894718647003 | Training Acc: 100.0 | Testing Loss: 0.004608425311744213 | Testing Acc: 100.0\n",
            "Epoch: 1160 | Training Loss: 0.003353971289470792 | Training Acc: 100.0 | Testing Loss: 0.004601923283189535 | Testing Acc: 100.0\n",
            "Epoch: 1161 | Training Loss: 0.0033502753358334303 | Training Acc: 100.0 | Testing Loss: 0.004602495580911636 | Testing Acc: 100.0\n",
            "Epoch: 1162 | Training Loss: 0.003346432000398636 | Training Acc: 100.0 | Testing Loss: 0.004591681063175201 | Testing Acc: 100.0\n",
            "Epoch: 1163 | Training Loss: 0.003342698561027646 | Training Acc: 100.0 | Testing Loss: 0.004593071062117815 | Testing Acc: 100.0\n",
            "Epoch: 1164 | Training Loss: 0.003338922979310155 | Training Acc: 100.0 | Testing Loss: 0.004586740396916866 | Testing Acc: 100.0\n",
            "Epoch: 1165 | Training Loss: 0.0033351294696331024 | Training Acc: 100.0 | Testing Loss: 0.004588255658745766 | Testing Acc: 100.0\n",
            "Epoch: 1166 | Training Loss: 0.003331455634906888 | Training Acc: 100.0 | Testing Loss: 0.00458140205591917 | Testing Acc: 100.0\n",
            "Epoch: 1167 | Training Loss: 0.0033275876194238663 | Training Acc: 100.0 | Testing Loss: 0.004582453519105911 | Testing Acc: 100.0\n",
            "Epoch: 1168 | Training Loss: 0.0033239969052374363 | Training Acc: 100.0 | Testing Loss: 0.004575259517878294 | Testing Acc: 100.0\n",
            "Epoch: 1169 | Training Loss: 0.0033201228361576796 | Training Acc: 100.0 | Testing Loss: 0.004569012206047773 | Testing Acc: 100.0\n",
            "Epoch: 1170 | Training Loss: 0.003316514892503619 | Training Acc: 100.0 | Testing Loss: 0.004569642245769501 | Testing Acc: 100.0\n",
            "Epoch: 1171 | Training Loss: 0.0033127337228506804 | Training Acc: 100.0 | Testing Loss: 0.004559914581477642 | Testing Acc: 100.0\n",
            "Epoch: 1172 | Training Loss: 0.00330906780436635 | Training Acc: 100.0 | Testing Loss: 0.00456093018874526 | Testing Acc: 100.0\n",
            "Epoch: 1173 | Training Loss: 0.003305331338196993 | Training Acc: 100.0 | Testing Loss: 0.004554699175059795 | Testing Acc: 100.0\n",
            "Epoch: 1174 | Training Loss: 0.0033016062807291746 | Training Acc: 100.0 | Testing Loss: 0.0045552728697657585 | Testing Acc: 100.0\n",
            "Epoch: 1175 | Training Loss: 0.0032979846000671387 | Training Acc: 100.0 | Testing Loss: 0.0045494819059967995 | Testing Acc: 100.0\n",
            "Epoch: 1176 | Training Loss: 0.003294202033430338 | Training Acc: 100.0 | Testing Loss: 0.004549632780253887 | Testing Acc: 100.0\n",
            "Epoch: 1177 | Training Loss: 0.0032906457781791687 | Training Acc: 100.0 | Testing Loss: 0.004542739596217871 | Testing Acc: 100.0\n",
            "Epoch: 1178 | Training Loss: 0.0032868601847440004 | Training Acc: 100.0 | Testing Loss: 0.004536615218967199 | Testing Acc: 100.0\n",
            "Epoch: 1179 | Training Loss: 0.003283283207565546 | Training Acc: 100.0 | Testing Loss: 0.004538280889391899 | Testing Acc: 100.0\n",
            "Epoch: 1180 | Training Loss: 0.0032795772422105074 | Training Acc: 100.0 | Testing Loss: 0.004531574435532093 | Testing Acc: 100.0\n",
            "Epoch: 1181 | Training Loss: 0.0032759278547018766 | Training Acc: 100.0 | Testing Loss: 0.0045318640768527985 | Testing Acc: 100.0\n",
            "Epoch: 1182 | Training Loss: 0.003272317349910736 | Training Acc: 100.0 | Testing Loss: 0.004521189257502556 | Testing Acc: 100.0\n",
            "Epoch: 1183 | Training Loss: 0.0032686323393136263 | Training Acc: 100.0 | Testing Loss: 0.004522967617958784 | Testing Acc: 100.0\n",
            "Epoch: 1184 | Training Loss: 0.0032650865614414215 | Training Acc: 100.0 | Testing Loss: 0.004516536835581064 | Testing Acc: 100.0\n",
            "Epoch: 1185 | Training Loss: 0.003261324018239975 | Training Acc: 100.0 | Testing Loss: 0.004517001565545797 | Testing Acc: 100.0\n",
            "Epoch: 1186 | Training Loss: 0.0032578702084720135 | Training Acc: 100.0 | Testing Loss: 0.0045103346928954124 | Testing Acc: 100.0\n",
            "Epoch: 1187 | Training Loss: 0.0032541428226977587 | Training Acc: 100.0 | Testing Loss: 0.004505281336605549 | Testing Acc: 100.0\n",
            "Epoch: 1188 | Training Loss: 0.0032506335992366076 | Training Acc: 100.0 | Testing Loss: 0.004505879245698452 | Testing Acc: 100.0\n",
            "Epoch: 1189 | Training Loss: 0.0032469588331878185 | Training Acc: 100.0 | Testing Loss: 0.004499494563788176 | Testing Acc: 100.0\n",
            "Epoch: 1190 | Training Loss: 0.0032434011809527874 | Training Acc: 100.0 | Testing Loss: 0.0044998303055763245 | Testing Acc: 100.0\n",
            "Epoch: 1191 | Training Loss: 0.003239832818508148 | Training Acc: 100.0 | Testing Loss: 0.004494001157581806 | Testing Acc: 100.0\n",
            "Epoch: 1192 | Training Loss: 0.0032362081110477448 | Training Acc: 100.0 | Testing Loss: 0.004493997432291508 | Testing Acc: 100.0\n",
            "Epoch: 1193 | Training Loss: 0.0032327063381671906 | Training Acc: 100.0 | Testing Loss: 0.004487940110266209 | Testing Acc: 100.0\n",
            "Epoch: 1194 | Training Loss: 0.003229054156690836 | Training Acc: 100.0 | Testing Loss: 0.004481822717934847 | Testing Acc: 100.0\n",
            "Epoch: 1195 | Training Loss: 0.003225605236366391 | Training Acc: 100.0 | Testing Loss: 0.004482261370867491 | Testing Acc: 100.0\n",
            "Epoch: 1196 | Training Loss: 0.0032219768036156893 | Training Acc: 100.0 | Testing Loss: 0.004472826141864061 | Testing Acc: 100.0\n",
            "Epoch: 1197 | Training Loss: 0.0032184987794607878 | Training Acc: 100.0 | Testing Loss: 0.004473713226616383 | Testing Acc: 100.0\n",
            "Epoch: 1198 | Training Loss: 0.003214893862605095 | Training Acc: 100.0 | Testing Loss: 0.004467608407139778 | Testing Acc: 100.0\n",
            "Epoch: 1199 | Training Loss: 0.0032113753259181976 | Training Acc: 100.0 | Testing Loss: 0.004468102473765612 | Testing Acc: 100.0\n",
            "Epoch: 1200 | Training Loss: 0.0032078742515295744 | Training Acc: 100.0 | Testing Loss: 0.004462483339011669 | Testing Acc: 100.0\n",
            "Epoch: 1201 | Training Loss: 0.0032042935490608215 | Training Acc: 100.0 | Testing Loss: 0.004462555982172489 | Testing Acc: 100.0\n",
            "Epoch: 1202 | Training Loss: 0.0032008630223572254 | Training Acc: 100.0 | Testing Loss: 0.004455938935279846 | Testing Acc: 100.0\n",
            "Epoch: 1203 | Training Loss: 0.0031972439028322697 | Training Acc: 100.0 | Testing Loss: 0.0044500092044472694 | Testing Acc: 100.0\n",
            "Epoch: 1204 | Training Loss: 0.003193876938894391 | Training Acc: 100.0 | Testing Loss: 0.004451441578567028 | Testing Acc: 100.0\n",
            "Epoch: 1205 | Training Loss: 0.0031902797054499388 | Training Acc: 100.0 | Testing Loss: 0.0044449688866734505 | Testing Acc: 100.0\n",
            "Epoch: 1206 | Training Loss: 0.003186844987794757 | Training Acc: 100.0 | Testing Loss: 0.004445244558155537 | Testing Acc: 100.0\n",
            "Epoch: 1207 | Training Loss: 0.003183336229994893 | Training Acc: 100.0 | Testing Loss: 0.004438663832843304 | Testing Acc: 100.0\n",
            "Epoch: 1208 | Training Loss: 0.003179865423589945 | Training Acc: 100.0 | Testing Loss: 0.004439714830368757 | Testing Acc: 100.0\n",
            "Epoch: 1209 | Training Loss: 0.003176433499902487 | Training Acc: 100.0 | Testing Loss: 0.004432881250977516 | Testing Acc: 100.0\n",
            "Epoch: 1210 | Training Loss: 0.0031728832982480526 | Training Acc: 100.0 | Testing Loss: 0.004432829096913338 | Testing Acc: 100.0\n",
            "Epoch: 1211 | Training Loss: 0.00316953519359231 | Training Acc: 100.0 | Testing Loss: 0.004426049999892712 | Testing Acc: 100.0\n",
            "Epoch: 1212 | Training Loss: 0.0031659819651395082 | Training Acc: 100.0 | Testing Loss: 0.00442085275426507 | Testing Acc: 100.0\n",
            "Epoch: 1213 | Training Loss: 0.0031626522541046143 | Training Acc: 100.0 | Testing Loss: 0.004421188496053219 | Testing Acc: 100.0\n",
            "Epoch: 1214 | Training Loss: 0.003159116255119443 | Training Acc: 100.0 | Testing Loss: 0.004414872266352177 | Testing Acc: 100.0\n",
            "Epoch: 1215 | Training Loss: 0.0031557423062622547 | Training Acc: 100.0 | Testing Loss: 0.0044150277972221375 | Testing Acc: 100.0\n",
            "Epoch: 1216 | Training Loss: 0.0031523089855909348 | Training Acc: 100.0 | Testing Loss: 0.004405689891427755 | Testing Acc: 100.0\n",
            "Epoch: 1217 | Training Loss: 0.003148892428725958 | Training Acc: 100.0 | Testing Loss: 0.004406371619552374 | Testing Acc: 100.0\n",
            "Epoch: 1218 | Training Loss: 0.0031454861164093018 | Training Acc: 100.0 | Testing Loss: 0.004400341771543026 | Testing Acc: 100.0\n",
            "Epoch: 1219 | Training Loss: 0.003142022993415594 | Training Acc: 100.0 | Testing Loss: 0.004400717560201883 | Testing Acc: 100.0\n",
            "Epoch: 1220 | Training Loss: 0.0031387056224048138 | Training Acc: 100.0 | Testing Loss: 0.00439519714564085 | Testing Acc: 100.0\n",
            "Epoch: 1221 | Training Loss: 0.003135211765766144 | Training Acc: 100.0 | Testing Loss: 0.004389407113194466 | Testing Acc: 100.0\n",
            "Epoch: 1222 | Training Loss: 0.0031319279223680496 | Training Acc: 100.0 | Testing Loss: 0.004390155430883169 | Testing Acc: 100.0\n",
            "Epoch: 1223 | Training Loss: 0.003128480864688754 | Training Acc: 100.0 | Testing Loss: 0.004384796135127544 | Testing Acc: 100.0\n",
            "Epoch: 1224 | Training Loss: 0.0031251348555088043 | Training Acc: 100.0 | Testing Loss: 0.004384941421449184 | Testing Acc: 100.0\n",
            "Epoch: 1225 | Training Loss: 0.0031217513605952263 | Training Acc: 100.0 | Testing Loss: 0.004379282705485821 | Testing Acc: 100.0\n",
            "Epoch: 1226 | Training Loss: 0.0031183629762381315 | Training Acc: 100.0 | Testing Loss: 0.004379182122647762 | Testing Acc: 100.0\n",
            "Epoch: 1227 | Training Loss: 0.003115035593509674 | Training Acc: 100.0 | Testing Loss: 0.004372608847916126 | Testing Acc: 100.0\n",
            "Epoch: 1228 | Training Loss: 0.0031116155441850424 | Training Acc: 100.0 | Testing Loss: 0.0043725185096263885 | Testing Acc: 100.0\n",
            "Epoch: 1229 | Training Loss: 0.0031083787325769663 | Training Acc: 100.0 | Testing Loss: 0.004366687498986721 | Testing Acc: 100.0\n",
            "Epoch: 1230 | Training Loss: 0.0031049216631799936 | Training Acc: 100.0 | Testing Loss: 0.00436073262244463 | Testing Acc: 100.0\n",
            "Epoch: 1231 | Training Loss: 0.0031016904395073652 | Training Acc: 100.0 | Testing Loss: 0.00436115637421608 | Testing Acc: 100.0\n",
            "Epoch: 1232 | Training Loss: 0.0030982824973762035 | Training Acc: 100.0 | Testing Loss: 0.004354964476078749 | Testing Acc: 100.0\n",
            "Epoch: 1233 | Training Loss: 0.003095007734373212 | Training Acc: 100.0 | Testing Loss: 0.004355224780738354 | Testing Acc: 100.0\n",
            "Epoch: 1234 | Training Loss: 0.003091666614636779 | Training Acc: 100.0 | Testing Loss: 0.004348923917859793 | Testing Acc: 100.0\n",
            "Epoch: 1235 | Training Loss: 0.003088350174948573 | Training Acc: 100.0 | Testing Loss: 0.004349751863628626 | Testing Acc: 100.0\n",
            "Epoch: 1236 | Training Loss: 0.0030850768089294434 | Training Acc: 100.0 | Testing Loss: 0.0043432204984128475 | Testing Acc: 100.0\n",
            "Epoch: 1237 | Training Loss: 0.0030816993676126003 | Training Acc: 100.0 | Testing Loss: 0.004343305714428425 | Testing Acc: 100.0\n",
            "Epoch: 1238 | Training Loss: 0.0030785086564719677 | Training Acc: 100.0 | Testing Loss: 0.004336784593760967 | Testing Acc: 100.0\n",
            "Epoch: 1239 | Training Loss: 0.003075122134760022 | Training Acc: 100.0 | Testing Loss: 0.004331783391535282 | Testing Acc: 100.0\n",
            "Epoch: 1240 | Training Loss: 0.0030719463247805834 | Training Acc: 100.0 | Testing Loss: 0.004332093987613916 | Testing Acc: 100.0\n",
            "Epoch: 1241 | Training Loss: 0.003068579826503992 | Training Acc: 100.0 | Testing Loss: 0.004325984977185726 | Testing Acc: 100.0\n",
            "Epoch: 1242 | Training Loss: 0.0030653593130409718 | Training Acc: 100.0 | Testing Loss: 0.0043261367827653885 | Testing Acc: 100.0\n",
            "Epoch: 1243 | Training Loss: 0.0030620754696428776 | Training Acc: 100.0 | Testing Loss: 0.0043206303380429745 | Testing Acc: 100.0\n",
            "Epoch: 1244 | Training Loss: 0.0030588065274059772 | Training Acc: 100.0 | Testing Loss: 0.004320536740124226 | Testing Acc: 100.0\n",
            "Epoch: 1245 | Training Loss: 0.00305558810941875 | Training Acc: 100.0 | Testing Loss: 0.004314159043133259 | Testing Acc: 100.0\n",
            "Epoch: 1246 | Training Loss: 0.003052269574254751 | Training Acc: 100.0 | Testing Loss: 0.0043140584602952 | Testing Acc: 100.0\n",
            "Epoch: 1247 | Training Loss: 0.003049127059057355 | Training Acc: 100.0 | Testing Loss: 0.004305011127144098 | Testing Acc: 100.0\n",
            "Epoch: 1248 | Training Loss: 0.0030457850079983473 | Training Acc: 100.0 | Testing Loss: 0.004299898166209459 | Testing Acc: 100.0\n",
            "Epoch: 1249 | Training Loss: 0.003042659955099225 | Training Acc: 100.0 | Testing Loss: 0.0043009496293962 | Testing Acc: 100.0\n",
            "Epoch: 1250 | Training Loss: 0.0030393588822335005 | Training Acc: 100.0 | Testing Loss: 0.004296100232750177 | Testing Acc: 100.0\n",
            "Epoch: 1251 | Training Loss: 0.0030361879616975784 | Training Acc: 100.0 | Testing Loss: 0.0042964378371834755 | Testing Acc: 100.0\n",
            "Epoch: 1252 | Training Loss: 0.003032948588952422 | Training Acc: 100.0 | Testing Loss: 0.004290485288947821 | Testing Acc: 100.0\n",
            "Epoch: 1253 | Training Loss: 0.0030297283083200455 | Training Acc: 100.0 | Testing Loss: 0.004290641285479069 | Testing Acc: 100.0\n",
            "Epoch: 1254 | Training Loss: 0.0030265606474131346 | Training Acc: 100.0 | Testing Loss: 0.004285241477191448 | Testing Acc: 100.0\n",
            "Epoch: 1255 | Training Loss: 0.003023294033482671 | Training Acc: 100.0 | Testing Loss: 0.004285168834030628 | Testing Acc: 100.0\n",
            "Epoch: 1256 | Training Loss: 0.0030201964545994997 | Training Acc: 100.0 | Testing Loss: 0.00427889684215188 | Testing Acc: 100.0\n",
            "Epoch: 1257 | Training Loss: 0.003016903530806303 | Training Acc: 100.0 | Testing Loss: 0.0042732832953333855 | Testing Acc: 100.0\n",
            "Epoch: 1258 | Training Loss: 0.00301383133046329 | Training Acc: 100.0 | Testing Loss: 0.004273847211152315 | Testing Acc: 100.0\n",
            "Epoch: 1259 | Training Loss: 0.0030105679761618376 | Training Acc: 100.0 | Testing Loss: 0.004267968703061342 | Testing Acc: 100.0\n",
            "Epoch: 1260 | Training Loss: 0.0030074543319642544 | Training Acc: 100.0 | Testing Loss: 0.0042689526453614235 | Testing Acc: 100.0\n",
            "Epoch: 1261 | Training Loss: 0.003004262689501047 | Training Acc: 100.0 | Testing Loss: 0.004262779373675585 | Testing Acc: 100.0\n",
            "Epoch: 1262 | Training Loss: 0.003001096425577998 | Training Acc: 100.0 | Testing Loss: 0.004262848757207394 | Testing Acc: 100.0\n",
            "Epoch: 1263 | Training Loss: 0.0029979716055095196 | Training Acc: 100.0 | Testing Loss: 0.004256663378328085 | Testing Acc: 100.0\n",
            "Epoch: 1264 | Training Loss: 0.0029947576113045216 | Training Acc: 100.0 | Testing Loss: 0.004257478751242161 | Testing Acc: 100.0\n",
            "Epoch: 1265 | Training Loss: 0.0029917138163000345 | Training Acc: 100.0 | Testing Loss: 0.0042511108331382275 | Testing Acc: 100.0\n",
            "Epoch: 1266 | Training Loss: 0.0029884702526032925 | Training Acc: 100.0 | Testing Loss: 0.004245533142238855 | Testing Acc: 100.0\n",
            "Epoch: 1267 | Training Loss: 0.002985450904816389 | Training Acc: 100.0 | Testing Loss: 0.00424590427428484 | Testing Acc: 100.0\n",
            "Epoch: 1268 | Training Loss: 0.0029822438955307007 | Training Acc: 100.0 | Testing Loss: 0.004240122623741627 | Testing Acc: 100.0\n",
            "Epoch: 1269 | Training Loss: 0.002979177050292492 | Training Acc: 100.0 | Testing Loss: 0.004240318667143583 | Testing Acc: 100.0\n",
            "Epoch: 1270 | Training Loss: 0.0029760391917079687 | Training Acc: 100.0 | Testing Loss: 0.004235068801790476 | Testing Acc: 100.0\n",
            "Epoch: 1271 | Training Loss: 0.0029729262460023165 | Training Acc: 100.0 | Testing Loss: 0.004234989173710346 | Testing Acc: 100.0\n",
            "Epoch: 1272 | Training Loss: 0.002969842404127121 | Training Acc: 100.0 | Testing Loss: 0.0042288824915885925 | Testing Acc: 100.0\n",
            "Epoch: 1273 | Training Loss: 0.002966680098325014 | Training Acc: 100.0 | Testing Loss: 0.004228809382766485 | Testing Acc: 100.0\n",
            "Epoch: 1274 | Training Loss: 0.002963682170957327 | Training Acc: 100.0 | Testing Loss: 0.004223354626446962 | Testing Acc: 100.0\n",
            "Epoch: 1275 | Training Loss: 0.002960496349260211 | Training Acc: 100.0 | Testing Loss: 0.004217780195176601 | Testing Acc: 100.0\n",
            "Epoch: 1276 | Training Loss: 0.0029574946966022253 | Training Acc: 100.0 | Testing Loss: 0.0042182463221251965 | Testing Acc: 100.0\n",
            "Epoch: 1277 | Training Loss: 0.002954373834654689 | Training Acc: 100.0 | Testing Loss: 0.004213140811771154 | Testing Acc: 100.0\n",
            "Epoch: 1278 | Training Loss: 0.0029513284098356962 | Training Acc: 100.0 | Testing Loss: 0.0042131515219807625 | Testing Acc: 100.0\n",
            "Epoch: 1279 | Training Loss: 0.0029482473619282246 | Training Acc: 100.0 | Testing Loss: 0.004207168705761433 | Testing Acc: 100.0\n",
            "Epoch: 1280 | Training Loss: 0.0029451651498675346 | Training Acc: 100.0 | Testing Loss: 0.004207140766084194 | Testing Acc: 100.0\n",
            "Epoch: 1281 | Training Loss: 0.002942159306257963 | Training Acc: 100.0 | Testing Loss: 0.004201787523925304 | Testing Acc: 100.0\n",
            "Epoch: 1282 | Training Loss: 0.0029390326235443354 | Training Acc: 100.0 | Testing Loss: 0.004201589617878199 | Testing Acc: 100.0\n",
            "Epoch: 1283 | Training Loss: 0.0029360796324908733 | Training Acc: 100.0 | Testing Loss: 0.0041954731568694115 | Testing Acc: 100.0\n",
            "Epoch: 1284 | Training Loss: 0.0029329368844628334 | Training Acc: 100.0 | Testing Loss: 0.004190015606582165 | Testing Acc: 100.0\n",
            "Epoch: 1285 | Training Loss: 0.0029300041496753693 | Training Acc: 100.0 | Testing Loss: 0.004190453328192234 | Testing Acc: 100.0\n",
            "Epoch: 1286 | Training Loss: 0.0029268870130181313 | Training Acc: 100.0 | Testing Loss: 0.004184748511761427 | Testing Acc: 100.0\n",
            "Epoch: 1287 | Training Loss: 0.002923914697021246 | Training Acc: 100.0 | Testing Loss: 0.0041856421157717705 | Testing Acc: 100.0\n",
            "Epoch: 1288 | Training Loss: 0.0029208685737103224 | Training Acc: 100.0 | Testing Loss: 0.004179701674729586 | Testing Acc: 100.0\n",
            "Epoch: 1289 | Training Loss: 0.002917839912697673 | Training Acc: 100.0 | Testing Loss: 0.004179710056632757 | Testing Acc: 100.0\n",
            "Epoch: 1290 | Training Loss: 0.0029148715548217297 | Training Acc: 100.0 | Testing Loss: 0.004173723049461842 | Testing Acc: 100.0\n",
            "Epoch: 1291 | Training Loss: 0.0029118102975189686 | Training Acc: 100.0 | Testing Loss: 0.00417387206107378 | Testing Acc: 100.0\n",
            "Epoch: 1292 | Training Loss: 0.002908885944634676 | Training Acc: 100.0 | Testing Loss: 0.004167874809354544 | Testing Acc: 100.0\n",
            "Epoch: 1293 | Training Loss: 0.00290580908767879 | Training Acc: 100.0 | Testing Loss: 0.004163226578384638 | Testing Acc: 100.0\n",
            "Epoch: 1294 | Training Loss: 0.002902919426560402 | Training Acc: 100.0 | Testing Loss: 0.004163528326898813 | Testing Acc: 100.0\n",
            "Epoch: 1295 | Training Loss: 0.002899843966588378 | Training Acc: 100.0 | Testing Loss: 0.004157909192144871 | Testing Acc: 100.0\n",
            "Epoch: 1296 | Training Loss: 0.002896915888413787 | Training Acc: 100.0 | Testing Loss: 0.004158046096563339 | Testing Acc: 100.0\n",
            "Epoch: 1297 | Training Loss: 0.0028939200565218925 | Training Acc: 100.0 | Testing Loss: 0.004152331966906786 | Testing Acc: 100.0\n",
            "Epoch: 1298 | Training Loss: 0.0028909468092024326 | Training Acc: 100.0 | Testing Loss: 0.00415236409753561 | Testing Acc: 100.0\n",
            "Epoch: 1299 | Training Loss: 0.00288802245631814 | Training Acc: 100.0 | Testing Loss: 0.004147222265601158 | Testing Acc: 100.0\n",
            "Epoch: 1300 | Training Loss: 0.002885000314563513 | Training Acc: 100.0 | Testing Loss: 0.004147072322666645 | Testing Acc: 100.0\n",
            "Epoch: 1301 | Training Loss: 0.002882117871195078 | Training Acc: 100.0 | Testing Loss: 0.004141153767704964 | Testing Acc: 100.0\n",
            "Epoch: 1302 | Training Loss: 0.002879089443013072 | Training Acc: 100.0 | Testing Loss: 0.0041358512826263905 | Testing Acc: 100.0\n",
            "Epoch: 1303 | Training Loss: 0.0028762356378138065 | Training Acc: 100.0 | Testing Loss: 0.004136422649025917 | Testing Acc: 100.0\n",
            "Epoch: 1304 | Training Loss: 0.002873223042115569 | Training Acc: 100.0 | Testing Loss: 0.004130877088755369 | Testing Acc: 100.0\n",
            "Epoch: 1305 | Training Loss: 0.002870328025892377 | Training Acc: 100.0 | Testing Loss: 0.004131739027798176 | Testing Acc: 100.0\n",
            "Epoch: 1306 | Training Loss: 0.0028673866763710976 | Training Acc: 100.0 | Testing Loss: 0.004125936422497034 | Testing Acc: 100.0\n",
            "Epoch: 1307 | Training Loss: 0.0028644423000514507 | Training Acc: 100.0 | Testing Loss: 0.004125968553125858 | Testing Acc: 100.0\n",
            "Epoch: 1308 | Training Loss: 0.002861573826521635 | Training Acc: 100.0 | Testing Loss: 0.004120113793760538 | Testing Acc: 100.0\n",
            "Epoch: 1309 | Training Loss: 0.002858577761799097 | Training Acc: 100.0 | Testing Loss: 0.004120106343179941 | Testing Acc: 100.0\n",
            "Epoch: 1310 | Training Loss: 0.0028557623736560345 | Training Acc: 100.0 | Testing Loss: 0.004114291165024042 | Testing Acc: 100.0\n",
            "Epoch: 1311 | Training Loss: 0.0028527628164738417 | Training Acc: 100.0 | Testing Loss: 0.004109167028218508 | Testing Acc: 100.0\n",
            "Epoch: 1312 | Training Loss: 0.0028499679174274206 | Training Acc: 100.0 | Testing Loss: 0.004109584726393223 | Testing Acc: 100.0\n",
            "Epoch: 1313 | Training Loss: 0.00284699653275311 | Training Acc: 100.0 | Testing Loss: 0.004104805178940296 | Testing Acc: 100.0\n",
            "Epoch: 1314 | Training Loss: 0.0028441702015697956 | Training Acc: 100.0 | Testing Loss: 0.004104928113520145 | Testing Acc: 100.0\n",
            "Epoch: 1315 | Training Loss: 0.002841237233951688 | Training Acc: 100.0 | Testing Loss: 0.004099278710782528 | Testing Acc: 100.0\n",
            "Epoch: 1316 | Training Loss: 0.0028383643366396427 | Training Acc: 100.0 | Testing Loss: 0.004099312238395214 | Testing Acc: 100.0\n",
            "Epoch: 1317 | Training Loss: 0.0028355014510452747 | Training Acc: 100.0 | Testing Loss: 0.004093647934496403 | Testing Acc: 100.0\n",
            "Epoch: 1318 | Training Loss: 0.0028325901366770267 | Training Acc: 100.0 | Testing Loss: 0.004093723837286234 | Testing Acc: 100.0\n",
            "Epoch: 1319 | Training Loss: 0.002829798264428973 | Training Acc: 100.0 | Testing Loss: 0.004088641609996557 | Testing Acc: 100.0\n",
            "Epoch: 1320 | Training Loss: 0.002826858311891556 | Training Acc: 100.0 | Testing Loss: 0.004083401523530483 | Testing Acc: 100.0\n",
            "Epoch: 1321 | Training Loss: 0.0028240876272320747 | Training Acc: 100.0 | Testing Loss: 0.004083720501512289 | Testing Acc: 100.0\n",
            "Epoch: 1322 | Training Loss: 0.002821167930960655 | Training Acc: 100.0 | Testing Loss: 0.0040789395570755005 | Testing Acc: 100.0\n",
            "Epoch: 1323 | Training Loss: 0.002818370470777154 | Training Acc: 100.0 | Testing Loss: 0.004078954923897982 | Testing Acc: 100.0\n",
            "Epoch: 1324 | Training Loss: 0.0028155080508440733 | Training Acc: 100.0 | Testing Loss: 0.0040733725763857365 | Testing Acc: 100.0\n",
            "Epoch: 1325 | Training Loss: 0.0028126605320721865 | Training Acc: 100.0 | Testing Loss: 0.004073344171047211 | Testing Acc: 100.0\n",
            "Epoch: 1326 | Training Loss: 0.0028098570182919502 | Training Acc: 100.0 | Testing Loss: 0.004067732486873865 | Testing Acc: 100.0\n",
            "Epoch: 1327 | Training Loss: 0.0028069564141333103 | Training Acc: 100.0 | Testing Loss: 0.0040676710195839405 | Testing Acc: 100.0\n",
            "Epoch: 1328 | Training Loss: 0.0028042278718203306 | Training Acc: 100.0 | Testing Loss: 0.004062646999955177 | Testing Acc: 100.0\n",
            "Epoch: 1329 | Training Loss: 0.0028013091068714857 | Training Acc: 100.0 | Testing Loss: 0.0040574404411017895 | Testing Acc: 100.0\n",
            "Epoch: 1330 | Training Loss: 0.0027985847555100918 | Training Acc: 100.0 | Testing Loss: 0.004057931713759899 | Testing Acc: 100.0\n",
            "Epoch: 1331 | Training Loss: 0.002795711625367403 | Training Acc: 100.0 | Testing Loss: 0.004052555188536644 | Testing Acc: 100.0\n",
            "Epoch: 1332 | Training Loss: 0.0027929407078772783 | Training Acc: 100.0 | Testing Loss: 0.004052736796438694 | Testing Acc: 100.0\n",
            "Epoch: 1333 | Training Loss: 0.0027901337016373873 | Training Acc: 100.0 | Testing Loss: 0.004047227092087269 | Testing Acc: 100.0\n",
            "Epoch: 1334 | Training Loss: 0.002787318080663681 | Training Acc: 100.0 | Testing Loss: 0.00404730811715126 | Testing Acc: 100.0\n",
            "Epoch: 1335 | Training Loss: 0.0027845632284879684 | Training Acc: 100.0 | Testing Loss: 0.004041754640638828 | Testing Acc: 100.0\n",
            "Epoch: 1336 | Training Loss: 0.0027817231602966785 | Training Acc: 100.0 | Testing Loss: 0.004042398650199175 | Testing Acc: 100.0\n",
            "Epoch: 1337 | Training Loss: 0.002779037458822131 | Training Acc: 100.0 | Testing Loss: 0.004036678466945887 | Testing Acc: 100.0\n",
            "Epoch: 1338 | Training Loss: 0.0027761494275182486 | Training Acc: 100.0 | Testing Loss: 0.00403166888281703 | Testing Acc: 100.0\n",
            "Epoch: 1339 | Training Loss: 0.002773488871753216 | Training Acc: 100.0 | Testing Loss: 0.004032017197459936 | Testing Acc: 100.0\n",
            "Epoch: 1340 | Training Loss: 0.0027706294786185026 | Training Acc: 100.0 | Testing Loss: 0.004026792012155056 | Testing Acc: 100.0\n",
            "Epoch: 1341 | Training Loss: 0.0027679279446601868 | Training Acc: 100.0 | Testing Loss: 0.00402694521471858 | Testing Acc: 100.0\n",
            "Epoch: 1342 | Training Loss: 0.00276514096185565 | Training Acc: 100.0 | Testing Loss: 0.004021609667688608 | Testing Acc: 100.0\n",
            "Epoch: 1343 | Training Loss: 0.0027623833157122135 | Training Acc: 100.0 | Testing Loss: 0.00402167160063982 | Testing Acc: 100.0\n",
            "Epoch: 1344 | Training Loss: 0.0027596582658588886 | Training Acc: 100.0 | Testing Loss: 0.004016852471977472 | Testing Acc: 100.0\n",
            "Epoch: 1345 | Training Loss: 0.0027568787336349487 | Training Acc: 100.0 | Testing Loss: 0.004016841761767864 | Testing Acc: 100.0\n",
            "Epoch: 1346 | Training Loss: 0.002754193963482976 | Training Acc: 100.0 | Testing Loss: 0.004011224489659071 | Testing Acc: 100.0\n",
            "Epoch: 1347 | Training Loss: 0.0027513932436704636 | Training Acc: 100.0 | Testing Loss: 0.004006874281913042 | Testing Acc: 100.0\n",
            "Epoch: 1348 | Training Loss: 0.0027487524785101414 | Training Acc: 100.0 | Testing Loss: 0.004007105715572834 | Testing Acc: 100.0\n",
            "Epoch: 1349 | Training Loss: 0.0027459454722702503 | Training Acc: 100.0 | Testing Loss: 0.004001856315881014 | Testing Acc: 100.0\n",
            "Epoch: 1350 | Training Loss: 0.0027432660572230816 | Training Acc: 100.0 | Testing Loss: 0.004001949913799763 | Testing Acc: 100.0\n",
            "Epoch: 1351 | Training Loss: 0.0027405242435634136 | Training Acc: 100.0 | Testing Loss: 0.003996616695076227 | Testing Acc: 100.0\n",
            "Epoch: 1352 | Training Loss: 0.0027378140948712826 | Training Acc: 100.0 | Testing Loss: 0.003996645100414753 | Testing Acc: 100.0\n",
            "Epoch: 1353 | Training Loss: 0.0027351328171789646 | Training Acc: 100.0 | Testing Loss: 0.003991267178207636 | Testing Acc: 100.0\n",
            "Epoch: 1354 | Training Loss: 0.002732363296672702 | Training Acc: 100.0 | Testing Loss: 0.003991234581917524 | Testing Acc: 100.0\n",
            "Epoch: 1355 | Training Loss: 0.0027297516353428364 | Training Acc: 100.0 | Testing Loss: 0.003986408468335867 | Testing Acc: 100.0\n",
            "Epoch: 1356 | Training Loss: 0.002726964419707656 | Training Acc: 100.0 | Testing Loss: 0.003981427289545536 | Testing Acc: 100.0\n",
            "Epoch: 1357 | Training Loss: 0.0027243555523455143 | Training Acc: 100.0 | Testing Loss: 0.003981908317655325 | Testing Acc: 100.0\n",
            "Epoch: 1358 | Training Loss: 0.0027216137386858463 | Training Acc: 100.0 | Testing Loss: 0.003976699896156788 | Testing Acc: 100.0\n",
            "Epoch: 1359 | Training Loss: 0.0027189645916223526 | Training Acc: 100.0 | Testing Loss: 0.003976894076913595 | Testing Acc: 100.0\n",
            "Epoch: 1360 | Training Loss: 0.0027162760961800814 | Training Acc: 100.0 | Testing Loss: 0.003971585538238287 | Testing Acc: 100.0\n",
            "Epoch: 1361 | Training Loss: 0.0027135899290442467 | Training Acc: 100.0 | Testing Loss: 0.003971691243350506 | Testing Acc: 100.0\n",
            "Epoch: 1362 | Training Loss: 0.002710955683141947 | Training Acc: 100.0 | Testing Loss: 0.003966350574046373 | Testing Acc: 100.0\n",
            "Epoch: 1363 | Training Loss: 0.0027082362212240696 | Training Acc: 100.0 | Testing Loss: 0.003966384567320347 | Testing Acc: 100.0\n",
            "Epoch: 1364 | Training Loss: 0.0027056396938860416 | Training Acc: 100.0 | Testing Loss: 0.003961016424000263 | Testing Acc: 100.0\n",
            "Epoch: 1365 | Training Loss: 0.0027028971817344427 | Training Acc: 100.0 | Testing Loss: 0.0039568329229950905 | Testing Acc: 100.0\n",
            "Epoch: 1366 | Training Loss: 0.0027003525756299496 | Training Acc: 100.0 | Testing Loss: 0.003957097418606281 | Testing Acc: 100.0\n",
            "Epoch: 1367 | Training Loss: 0.0026976161170750856 | Training Acc: 100.0 | Testing Loss: 0.003952025435864925 | Testing Acc: 100.0\n",
            "Epoch: 1368 | Training Loss: 0.0026950326282531023 | Training Acc: 100.0 | Testing Loss: 0.003952136263251305 | Testing Acc: 100.0\n",
            "Epoch: 1369 | Training Loss: 0.0026923527475446463 | Training Acc: 100.0 | Testing Loss: 0.003946992568671703 | Testing Acc: 100.0\n",
            "Epoch: 1370 | Training Loss: 0.0026897296775132418 | Training Acc: 100.0 | Testing Loss: 0.003947143442928791 | Testing Acc: 100.0\n",
            "Epoch: 1371 | Training Loss: 0.0026871233712881804 | Training Acc: 100.0 | Testing Loss: 0.003941903822124004 | Testing Acc: 100.0\n",
            "Epoch: 1372 | Training Loss: 0.0026844521053135395 | Training Acc: 100.0 | Testing Loss: 0.003941877745091915 | Testing Acc: 100.0\n",
            "Epoch: 1373 | Training Loss: 0.002681890968233347 | Training Acc: 100.0 | Testing Loss: 0.003936616703867912 | Testing Acc: 100.0\n",
            "Epoch: 1374 | Training Loss: 0.0026791966520249844 | Training Acc: 100.0 | Testing Loss: 0.003931869752705097 | Testing Acc: 100.0\n",
            "Epoch: 1375 | Training Loss: 0.0026766639202833176 | Training Acc: 100.0 | Testing Loss: 0.003932282794266939 | Testing Acc: 100.0\n",
            "Epoch: 1376 | Training Loss: 0.002673996612429619 | Training Acc: 100.0 | Testing Loss: 0.003927319310605526 | Testing Acc: 100.0\n",
            "Epoch: 1377 | Training Loss: 0.002671425696462393 | Training Acc: 100.0 | Testing Loss: 0.00392754515632987 | Testing Acc: 100.0\n",
            "Epoch: 1378 | Training Loss: 0.0026688084471970797 | Training Acc: 100.0 | Testing Loss: 0.003922461532056332 | Testing Acc: 100.0\n",
            "Epoch: 1379 | Training Loss: 0.002666193526238203 | Training Acc: 100.0 | Testing Loss: 0.003923117183148861 | Testing Acc: 100.0\n",
            "Epoch: 1380 | Training Loss: 0.0026636437978595495 | Training Acc: 100.0 | Testing Loss: 0.003917839378118515 | Testing Acc: 100.0\n",
            "Epoch: 1381 | Training Loss: 0.0026609916239976883 | Training Acc: 100.0 | Testing Loss: 0.0039177341386675835 | Testing Acc: 100.0\n",
            "Epoch: 1382 | Training Loss: 0.002658506389707327 | Training Acc: 100.0 | Testing Loss: 0.003913056571036577 | Testing Acc: 100.0\n",
            "Epoch: 1383 | Training Loss: 0.0026558293029665947 | Training Acc: 100.0 | Testing Loss: 0.003908224403858185 | Testing Acc: 100.0\n",
            "Epoch: 1384 | Training Loss: 0.0026533417403697968 | Training Acc: 100.0 | Testing Loss: 0.003908644896000624 | Testing Acc: 100.0\n",
            "Epoch: 1385 | Training Loss: 0.0026507098227739334 | Training Acc: 100.0 | Testing Loss: 0.003903618548065424 | Testing Acc: 100.0\n",
            "Epoch: 1386 | Training Loss: 0.002648171503096819 | Training Acc: 100.0 | Testing Loss: 0.00390377314761281 | Testing Acc: 100.0\n",
            "Epoch: 1387 | Training Loss: 0.0026455894112586975 | Training Acc: 100.0 | Testing Loss: 0.0038986809086054564 | Testing Acc: 100.0\n",
            "Epoch: 1388 | Training Loss: 0.0026430226862430573 | Training Acc: 100.0 | Testing Loss: 0.0038987533189356327 | Testing Acc: 100.0\n",
            "Epoch: 1389 | Training Loss: 0.0026405001990497112 | Training Acc: 100.0 | Testing Loss: 0.0038935821503400803 | Testing Acc: 100.0\n",
            "Epoch: 1390 | Training Loss: 0.0026378962211310863 | Training Acc: 100.0 | Testing Loss: 0.003893598448485136 | Testing Acc: 100.0\n",
            "Epoch: 1391 | Training Loss: 0.0026354254223406315 | Training Acc: 100.0 | Testing Loss: 0.003888411447405815 | Testing Acc: 100.0\n",
            "Epoch: 1392 | Training Loss: 0.002632799558341503 | Training Acc: 100.0 | Testing Loss: 0.003884366247802973 | Testing Acc: 100.0\n",
            "Epoch: 1393 | Training Loss: 0.0026303573977202177 | Training Acc: 100.0 | Testing Loss: 0.0038846153765916824 | Testing Acc: 100.0\n",
            "Epoch: 1394 | Training Loss: 0.002627735957503319 | Training Acc: 100.0 | Testing Loss: 0.0038797135930508375 | Testing Acc: 100.0\n",
            "Epoch: 1395 | Training Loss: 0.002625261200591922 | Training Acc: 100.0 | Testing Loss: 0.0038797869347035885 | Testing Acc: 100.0\n",
            "Epoch: 1396 | Training Loss: 0.002622700994834304 | Training Acc: 100.0 | Testing Loss: 0.003874692367389798 | Testing Acc: 100.0\n",
            "Epoch: 1397 | Training Loss: 0.0026201691944152117 | Training Acc: 100.0 | Testing Loss: 0.0038745931815356016 | Testing Acc: 100.0\n",
            "Epoch: 1398 | Training Loss: 0.0026176541578024626 | Training Acc: 100.0 | Testing Loss: 0.003869441570714116 | Testing Acc: 100.0\n",
            "Epoch: 1399 | Training Loss: 0.002615099074319005 | Training Acc: 100.0 | Testing Loss: 0.0038693598471581936 | Testing Acc: 100.0\n",
            "Epoch: 1400 | Training Loss: 0.002612636424601078 | Training Acc: 100.0 | Testing Loss: 0.0038642478175461292 | Testing Acc: 100.0\n",
            "Epoch: 1401 | Training Loss: 0.0026100564282387495 | Training Acc: 100.0 | Testing Loss: 0.0038596042431890965 | Testing Acc: 100.0\n",
            "Epoch: 1402 | Training Loss: 0.0026076172944158316 | Training Acc: 100.0 | Testing Loss: 0.003859963733702898 | Testing Acc: 100.0\n",
            "Epoch: 1403 | Training Loss: 0.002605064306408167 | Training Acc: 100.0 | Testing Loss: 0.0038551311008632183 | Testing Acc: 100.0\n",
            "Epoch: 1404 | Training Loss: 0.002602582098916173 | Training Acc: 100.0 | Testing Loss: 0.0038553483318537474 | Testing Acc: 100.0\n",
            "Epoch: 1405 | Training Loss: 0.0026000819634646177 | Training Acc: 100.0 | Testing Loss: 0.0038503953255712986 | Testing Acc: 100.0\n",
            "Epoch: 1406 | Training Loss: 0.0025975799653679132 | Training Acc: 100.0 | Testing Loss: 0.003850483102723956 | Testing Acc: 100.0\n",
            "Epoch: 1407 | Training Loss: 0.0025951252318918705 | Training Acc: 100.0 | Testing Loss: 0.003845474449917674 | Testing Acc: 100.0\n",
            "Epoch: 1408 | Training Loss: 0.0025925745721906424 | Training Acc: 100.0 | Testing Loss: 0.003845625091344118 | Testing Acc: 100.0\n",
            "Epoch: 1409 | Training Loss: 0.0025901789776980877 | Training Acc: 100.0 | Testing Loss: 0.003840566147118807 | Testing Acc: 100.0\n",
            "Epoch: 1410 | Training Loss: 0.0025876066647469997 | Training Acc: 100.0 | Testing Loss: 0.0038366063963621855 | Testing Acc: 100.0\n",
            "Epoch: 1411 | Training Loss: 0.002585242036730051 | Training Acc: 100.0 | Testing Loss: 0.0038368336390703917 | Testing Acc: 100.0\n",
            "Epoch: 1412 | Training Loss: 0.0025826909113675356 | Training Acc: 100.0 | Testing Loss: 0.0038319937884807587 | Testing Acc: 100.0\n",
            "Epoch: 1413 | Training Loss: 0.0025802699383348227 | Training Acc: 100.0 | Testing Loss: 0.0038321323227137327 | Testing Acc: 100.0\n",
            "Epoch: 1414 | Training Loss: 0.002577786101028323 | Training Acc: 100.0 | Testing Loss: 0.0038272161036729813 | Testing Acc: 100.0\n",
            "Epoch: 1415 | Training Loss: 0.0025753239169716835 | Training Acc: 100.0 | Testing Loss: 0.0038272831588983536 | Testing Acc: 100.0\n",
            "Epoch: 1416 | Training Loss: 0.002572889905422926 | Training Acc: 100.0 | Testing Loss: 0.0038222880102694035 | Testing Acc: 100.0\n",
            "Epoch: 1417 | Training Loss: 0.0025703918654471636 | Training Acc: 100.0 | Testing Loss: 0.0038228253833949566 | Testing Acc: 100.0\n",
            "Epoch: 1418 | Training Loss: 0.00256802374497056 | Training Acc: 100.0 | Testing Loss: 0.003817714285105467 | Testing Acc: 100.0\n",
            "Epoch: 1419 | Training Loss: 0.0025654900819063187 | Training Acc: 100.0 | Testing Loss: 0.0038132010959088802 | Testing Acc: 100.0\n",
            "Epoch: 1420 | Training Loss: 0.0025631531607359648 | Training Acc: 100.0 | Testing Loss: 0.0038135561626404524 | Testing Acc: 100.0\n",
            "Epoch: 1421 | Training Loss: 0.002560638589784503 | Training Acc: 100.0 | Testing Loss: 0.0038088648580014706 | Testing Acc: 100.0\n",
            "Epoch: 1422 | Training Loss: 0.0025582544039934874 | Training Acc: 100.0 | Testing Loss: 0.003808948677033186 | Testing Acc: 100.0\n",
            "Epoch: 1423 | Training Loss: 0.002555801998823881 | Training Acc: 100.0 | Testing Loss: 0.0038041570223867893 | Testing Acc: 100.0\n",
            "Epoch: 1424 | Training Loss: 0.002553381724283099 | Training Acc: 100.0 | Testing Loss: 0.0038041570223867893 | Testing Acc: 100.0\n",
            "Epoch: 1425 | Training Loss: 0.0025509686674922705 | Training Acc: 100.0 | Testing Loss: 0.003799292491748929 | Testing Acc: 100.0\n",
            "Epoch: 1426 | Training Loss: 0.0025485227815806866 | Training Acc: 100.0 | Testing Loss: 0.0037992610596120358 | Testing Acc: 100.0\n",
            "Epoch: 1427 | Training Loss: 0.00254617165774107 | Training Acc: 100.0 | Testing Loss: 0.0037943688221275806 | Testing Acc: 100.0\n",
            "Epoch: 1428 | Training Loss: 0.0025436910800635815 | Training Acc: 100.0 | Testing Loss: 0.003789930371567607 | Testing Acc: 100.0\n",
            "Epoch: 1429 | Training Loss: 0.0025413602124899626 | Training Acc: 100.0 | Testing Loss: 0.0037903140764683485 | Testing Acc: 100.0\n",
            "Epoch: 1430 | Training Loss: 0.0025389124639332294 | Training Acc: 100.0 | Testing Loss: 0.003785674925893545 | Testing Acc: 100.0\n",
            "Epoch: 1431 | Training Loss: 0.0025365378241986036 | Training Acc: 100.0 | Testing Loss: 0.003785880748182535 | Testing Acc: 100.0\n",
            "Epoch: 1432 | Training Loss: 0.0025341338478028774 | Training Acc: 100.0 | Testing Loss: 0.0037811086513102055 | Testing Acc: 100.0\n",
            "Epoch: 1433 | Training Loss: 0.0025317310355603695 | Training Acc: 100.0 | Testing Loss: 0.003781326115131378 | Testing Acc: 100.0\n",
            "Epoch: 1434 | Training Loss: 0.002529378514736891 | Training Acc: 100.0 | Testing Loss: 0.0037764888256788254 | Testing Acc: 100.0\n",
            "Epoch: 1435 | Training Loss: 0.0025269363541156054 | Training Acc: 100.0 | Testing Loss: 0.0037764948792755604 | Testing Acc: 100.0\n",
            "Epoch: 1436 | Training Loss: 0.0025246324948966503 | Training Acc: 100.0 | Testing Loss: 0.0037716112565249205 | Testing Acc: 100.0\n",
            "Epoch: 1437 | Training Loss: 0.0025221772957593203 | Training Acc: 100.0 | Testing Loss: 0.003767280373722315 | Testing Acc: 100.0\n",
            "Epoch: 1438 | Training Loss: 0.0025198988150805235 | Training Acc: 100.0 | Testing Loss: 0.0037676088977605104 | Testing Acc: 100.0\n",
            "Epoch: 1439 | Training Loss: 0.0025174652691930532 | Training Acc: 100.0 | Testing Loss: 0.0037630789447575808 | Testing Acc: 100.0\n",
            "Epoch: 1440 | Training Loss: 0.0025151551235467196 | Training Acc: 100.0 | Testing Loss: 0.003763218643143773 | Testing Acc: 100.0\n",
            "Epoch: 1441 | Training Loss: 0.0025127544067800045 | Training Acc: 100.0 | Testing Loss: 0.003758566454052925 | Testing Acc: 100.0\n",
            "Epoch: 1442 | Training Loss: 0.0025104181841015816 | Training Acc: 100.0 | Testing Loss: 0.0037586234975606203 | Testing Acc: 100.0\n",
            "Epoch: 1443 | Training Loss: 0.0025080821942538023 | Training Acc: 100.0 | Testing Loss: 0.0037538870237767696 | Testing Acc: 100.0\n",
            "Epoch: 1444 | Training Loss: 0.0025056859012693167 | Training Acc: 100.0 | Testing Loss: 0.0037538788747042418 | Testing Acc: 100.0\n",
            "Epoch: 1445 | Training Loss: 0.0025033927522599697 | Training Acc: 100.0 | Testing Loss: 0.0037491205148398876 | Testing Acc: 100.0\n",
            "Epoch: 1446 | Training Loss: 0.0025009862147271633 | Training Acc: 100.0 | Testing Loss: 0.0037449125666171312 | Testing Acc: 100.0\n",
            "Epoch: 1447 | Training Loss: 0.002498713554814458 | Training Acc: 100.0 | Testing Loss: 0.0037452136166393757 | Testing Acc: 100.0\n",
            "Epoch: 1448 | Training Loss: 0.002496344968676567 | Training Acc: 100.0 | Testing Loss: 0.003741224529221654 | Testing Acc: 100.0\n",
            "Epoch: 1449 | Training Loss: 0.0024940399453043938 | Training Acc: 100.0 | Testing Loss: 0.0037412759847939014 | Testing Acc: 100.0\n",
            "Epoch: 1450 | Training Loss: 0.0024916890542954206 | Training Acc: 100.0 | Testing Loss: 0.0037365879397839308 | Testing Acc: 100.0\n",
            "Epoch: 1451 | Training Loss: 0.0024893598165363073 | Training Acc: 100.0 | Testing Loss: 0.00373654393479228 | Testing Acc: 100.0\n",
            "Epoch: 1452 | Training Loss: 0.002487069694325328 | Training Acc: 100.0 | Testing Loss: 0.003731852862983942 | Testing Acc: 100.0\n",
            "Epoch: 1453 | Training Loss: 0.00248470576480031 | Training Acc: 100.0 | Testing Loss: 0.003731781616806984 | Testing Acc: 100.0\n",
            "Epoch: 1454 | Training Loss: 0.002482461277395487 | Training Acc: 100.0 | Testing Loss: 0.0037270612083375454 | Testing Acc: 100.0\n",
            "Epoch: 1455 | Training Loss: 0.0024800803512334824 | Training Acc: 100.0 | Testing Loss: 0.003722749650478363 | Testing Acc: 100.0\n",
            "Epoch: 1456 | Training Loss: 0.002477846574038267 | Training Acc: 100.0 | Testing Loss: 0.0037231892347335815 | Testing Acc: 100.0\n",
            "Epoch: 1457 | Training Loss: 0.0024754900950938463 | Training Acc: 100.0 | Testing Loss: 0.003718695370480418 | Testing Acc: 100.0\n",
            "Epoch: 1458 | Training Loss: 0.0024732190649956465 | Training Acc: 100.0 | Testing Loss: 0.0037188627757132053 | Testing Acc: 100.0\n",
            "Epoch: 1459 | Training Loss: 0.0024709231220185757 | Training Acc: 100.0 | Testing Loss: 0.003714263439178467 | Testing Acc: 100.0\n",
            "Epoch: 1460 | Training Loss: 0.0024686125107109547 | Training Acc: 100.0 | Testing Loss: 0.0037143423687666655 | Testing Acc: 100.0\n",
            "Epoch: 1461 | Training Loss: 0.002466363599523902 | Training Acc: 100.0 | Testing Loss: 0.0037096585147082806 | Testing Acc: 100.0\n",
            "Epoch: 1462 | Training Loss: 0.002464019227772951 | Training Acc: 100.0 | Testing Loss: 0.003709682496264577 | Testing Acc: 100.0\n",
            "Epoch: 1463 | Training Loss: 0.002461820375174284 | Training Acc: 100.0 | Testing Loss: 0.0037049646489322186 | Testing Acc: 100.0\n",
            "Epoch: 1464 | Training Loss: 0.002459458541125059 | Training Acc: 100.0 | Testing Loss: 0.003700802568346262 | Testing Acc: 100.0\n",
            "Epoch: 1465 | Training Loss: 0.0024572659749537706 | Training Acc: 100.0 | Testing Loss: 0.0037010866217315197 | Testing Acc: 100.0\n",
            "Epoch: 1466 | Training Loss: 0.002454933477565646 | Training Acc: 100.0 | Testing Loss: 0.003696718020364642 | Testing Acc: 100.0\n",
            "Epoch: 1467 | Training Loss: 0.0024527101777493954 | Training Acc: 100.0 | Testing Loss: 0.0036968637723475695 | Testing Acc: 100.0\n",
            "Epoch: 1468 | Training Loss: 0.002450422151014209 | Training Acc: 100.0 | Testing Loss: 0.0036923896986991167 | Testing Acc: 100.0\n",
            "Epoch: 1469 | Training Loss: 0.0024481636937707663 | Training Acc: 100.0 | Testing Loss: 0.0036925305612385273 | Testing Acc: 100.0\n",
            "Epoch: 1470 | Training Loss: 0.00244592921808362 | Training Acc: 100.0 | Testing Loss: 0.0036879617255181074 | Testing Acc: 100.0\n",
            "Epoch: 1471 | Training Loss: 0.0024436269886791706 | Training Acc: 100.0 | Testing Loss: 0.0036879186518490314 | Testing Acc: 100.0\n",
            "Epoch: 1472 | Training Loss: 0.0024414495564997196 | Training Acc: 100.0 | Testing Loss: 0.0036833330523222685 | Testing Acc: 100.0\n",
            "Epoch: 1473 | Training Loss: 0.0024391331244260073 | Training Acc: 100.0 | Testing Loss: 0.0036791462916880846 | Testing Acc: 100.0\n",
            "Epoch: 1474 | Training Loss: 0.0024369549937546253 | Training Acc: 100.0 | Testing Loss: 0.0036794308107346296 | Testing Acc: 100.0\n",
            "Epoch: 1475 | Training Loss: 0.0024346807040274143 | Training Acc: 100.0 | Testing Loss: 0.0036755814217031 | Testing Acc: 100.0\n",
            "Epoch: 1476 | Training Loss: 0.002432467881590128 | Training Acc: 100.0 | Testing Loss: 0.0036756217014044523 | Testing Acc: 100.0\n",
            "Epoch: 1477 | Training Loss: 0.0024302098900079727 | Training Acc: 100.0 | Testing Loss: 0.0036711127031594515 | Testing Acc: 100.0\n",
            "Epoch: 1478 | Training Loss: 0.0024279803037643433 | Training Acc: 100.0 | Testing Loss: 0.003671086160466075 | Testing Acc: 100.0\n",
            "Epoch: 1479 | Training Loss: 0.002425780287012458 | Training Acc: 100.0 | Testing Loss: 0.003666538745164871 | Testing Acc: 100.0\n",
            "Epoch: 1480 | Training Loss: 0.002423506695777178 | Training Acc: 100.0 | Testing Loss: 0.0036664684303104877 | Testing Acc: 100.0\n",
            "Epoch: 1481 | Training Loss: 0.0024213569704443216 | Training Acc: 100.0 | Testing Loss: 0.0036619137972593307 | Testing Acc: 100.0\n",
            "Epoch: 1482 | Training Loss: 0.0024190754629671574 | Training Acc: 100.0 | Testing Loss: 0.0036578483413904905 | Testing Acc: 100.0\n",
            "Epoch: 1483 | Training Loss: 0.002416929928585887 | Training Acc: 100.0 | Testing Loss: 0.003658143337816 | Testing Acc: 100.0\n",
            "Epoch: 1484 | Training Loss: 0.002414675895124674 | Training Acc: 100.0 | Testing Loss: 0.0036538108251988888 | Testing Acc: 100.0\n",
            "Epoch: 1485 | Training Loss: 0.0024124837946146727 | Training Acc: 100.0 | Testing Loss: 0.003653979627415538 | Testing Acc: 100.0\n",
            "Epoch: 1486 | Training Loss: 0.0024102821480482817 | Training Acc: 100.0 | Testing Loss: 0.003649529069662094 | Testing Acc: 100.0\n",
            "Epoch: 1487 | Training Loss: 0.0024080665316432714 | Training Acc: 100.0 | Testing Loss: 0.0036495919339358807 | Testing Acc: 100.0\n",
            "Epoch: 1488 | Training Loss: 0.002405910985544324 | Training Acc: 100.0 | Testing Loss: 0.0036450866609811783 | Testing Acc: 100.0\n",
            "Epoch: 1489 | Training Loss: 0.0024036606773734093 | Training Acc: 100.0 | Testing Loss: 0.0036450992338359356 | Testing Acc: 100.0\n",
            "Epoch: 1490 | Training Loss: 0.002401546109467745 | Training Acc: 100.0 | Testing Loss: 0.0036405492573976517 | Testing Acc: 100.0\n",
            "Epoch: 1491 | Training Loss: 0.0023992802016437054 | Training Acc: 100.0 | Testing Loss: 0.003636502893641591 | Testing Acc: 100.0\n",
            "Epoch: 1492 | Training Loss: 0.002397183794528246 | Training Acc: 100.0 | Testing Loss: 0.003636776702478528 | Testing Acc: 100.0\n",
            "Epoch: 1493 | Training Loss: 0.002394943032413721 | Training Acc: 100.0 | Testing Loss: 0.003632508683949709 | Testing Acc: 100.0\n",
            "Epoch: 1494 | Training Loss: 0.0023928103037178516 | Training Acc: 100.0 | Testing Loss: 0.003632782492786646 | Testing Acc: 100.0\n",
            "Epoch: 1495 | Training Loss: 0.002390617970377207 | Training Acc: 100.0 | Testing Loss: 0.003628415521234274 | Testing Acc: 100.0\n",
            "Epoch: 1496 | Training Loss: 0.002388434484601021 | Training Acc: 100.0 | Testing Loss: 0.003628472564741969 | Testing Acc: 100.0\n",
            "Epoch: 1497 | Training Loss: 0.002386299427598715 | Training Acc: 100.0 | Testing Loss: 0.003624026197940111 | Testing Acc: 100.0\n",
            "Epoch: 1498 | Training Loss: 0.0023840838111937046 | Training Acc: 100.0 | Testing Loss: 0.003624021541327238 | Testing Acc: 100.0\n",
            "Epoch: 1499 | Training Loss: 0.0023819939233362675 | Training Acc: 100.0 | Testing Loss: 0.0036195474676787853 | Testing Acc: 100.0\n",
            "Epoch: 1500 | Training Loss: 0.0023797794710844755 | Training Acc: 100.0 | Testing Loss: 0.0036155395209789276 | Testing Acc: 100.0\n",
            "Epoch: 1501 | Training Loss: 0.0023776921443641186 | Training Acc: 100.0 | Testing Loss: 0.003615801688283682 | Testing Acc: 100.0\n",
            "Epoch: 1502 | Training Loss: 0.002375498414039612 | Training Acc: 100.0 | Testing Loss: 0.0036116265691816807 | Testing Acc: 100.0\n",
            "Epoch: 1503 | Training Loss: 0.0023733656853437424 | Training Acc: 100.0 | Testing Loss: 0.0036117625422775745 | Testing Acc: 100.0\n",
            "Epoch: 1504 | Training Loss: 0.002371224109083414 | Training Acc: 100.0 | Testing Loss: 0.003607475431635976 | Testing Acc: 100.0\n",
            "Epoch: 1505 | Training Loss: 0.00236906623467803 | Training Acc: 100.0 | Testing Loss: 0.003607605816796422 | Testing Acc: 100.0\n",
            "Epoch: 1506 | Training Loss: 0.002366972155869007 | Training Acc: 100.0 | Testing Loss: 0.0036032467614859343 | Testing Acc: 100.0\n",
            "Epoch: 1507 | Training Loss: 0.0023647749330848455 | Training Acc: 100.0 | Testing Loss: 0.003603210672736168 | Testing Acc: 100.0\n",
            "Epoch: 1508 | Training Loss: 0.002362719504162669 | Training Acc: 100.0 | Testing Loss: 0.003598817391321063 | Testing Acc: 100.0\n",
            "Epoch: 1509 | Training Loss: 0.0023605194874107838 | Training Acc: 100.0 | Testing Loss: 0.003594813169911504 | Testing Acc: 100.0\n",
            "Epoch: 1510 | Training Loss: 0.0023584533482789993 | Training Acc: 100.0 | Testing Loss: 0.0035951025784015656 | Testing Acc: 100.0\n",
            "Epoch: 1511 | Training Loss: 0.0023562959395349026 | Training Acc: 100.0 | Testing Loss: 0.0035909204743802547 | Testing Acc: 100.0\n",
            "Epoch: 1512 | Training Loss: 0.002354186028242111 | Training Acc: 100.0 | Testing Loss: 0.003591066226363182 | Testing Acc: 100.0\n",
            "Epoch: 1513 | Training Loss: 0.0023520863614976406 | Training Acc: 100.0 | Testing Loss: 0.0035868012346327305 | Testing Acc: 100.0\n",
            "Epoch: 1514 | Training Loss: 0.002349936868995428 | Training Acc: 100.0 | Testing Loss: 0.0035868813283741474 | Testing Acc: 100.0\n",
            "Epoch: 1515 | Training Loss: 0.0023478821385651827 | Training Acc: 100.0 | Testing Loss: 0.003582531586289406 | Testing Acc: 100.0\n",
            "Epoch: 1516 | Training Loss: 0.002345713786780834 | Training Acc: 100.0 | Testing Loss: 0.0035825620871037245 | Testing Acc: 100.0\n",
            "Epoch: 1517 | Training Loss: 0.002343685831874609 | Training Acc: 100.0 | Testing Loss: 0.0035781729966402054 | Testing Acc: 100.0\n",
            "Epoch: 1518 | Training Loss: 0.0023415149189531803 | Training Acc: 100.0 | Testing Loss: 0.003574356436729431 | Testing Acc: 100.0\n",
            "Epoch: 1519 | Training Loss: 0.0023394909221678972 | Training Acc: 100.0 | Testing Loss: 0.0035746204666793346 | Testing Acc: 100.0\n",
            "Epoch: 1520 | Training Loss: 0.0023373484145849943 | Training Acc: 100.0 | Testing Loss: 0.0035704639740288258 | Testing Acc: 100.0\n",
            "Epoch: 1521 | Training Loss: 0.002335286932066083 | Training Acc: 100.0 | Testing Loss: 0.0035706330090761185 | Testing Acc: 100.0\n",
            "Epoch: 1522 | Training Loss: 0.002333192154765129 | Training Acc: 100.0 | Testing Loss: 0.003566398750990629 | Testing Acc: 100.0\n",
            "Epoch: 1523 | Training Loss: 0.002331088064238429 | Training Acc: 100.0 | Testing Loss: 0.0035664669703692198 | Testing Acc: 100.0\n",
            "Epoch: 1524 | Training Loss: 0.0023290482349693775 | Training Acc: 100.0 | Testing Loss: 0.0035621661227196455 | Testing Acc: 100.0\n",
            "Epoch: 1525 | Training Loss: 0.002326914807781577 | Training Acc: 100.0 | Testing Loss: 0.0035621735733002424 | Testing Acc: 100.0\n",
            "Epoch: 1526 | Training Loss: 0.002324917120859027 | Training Acc: 100.0 | Testing Loss: 0.0035578501410782337 | Testing Acc: 100.0\n",
            "Epoch: 1527 | Training Loss: 0.002322786720469594 | Training Acc: 100.0 | Testing Loss: 0.003553976071998477 | Testing Acc: 100.0\n",
            "Epoch: 1528 | Training Loss: 0.002320770174264908 | Training Acc: 100.0 | Testing Loss: 0.0035543229896575212 | Testing Acc: 100.0\n",
            "Epoch: 1529 | Training Loss: 0.0023186709731817245 | Training Acc: 100.0 | Testing Loss: 0.0035502766259014606 | Testing Acc: 100.0\n",
            "Epoch: 1530 | Training Loss: 0.002316616242751479 | Training Acc: 100.0 | Testing Loss: 0.0035503848921507597 | Testing Acc: 100.0\n",
            "Epoch: 1531 | Training Loss: 0.0023145617451518774 | Training Acc: 100.0 | Testing Loss: 0.0035462442319840193 | Testing Acc: 100.0\n",
            "Epoch: 1532 | Training Loss: 0.002312478143721819 | Training Acc: 100.0 | Testing Loss: 0.0035462514497339725 | Testing Acc: 100.0\n",
            "Epoch: 1533 | Training Loss: 0.002310477662831545 | Training Acc: 100.0 | Testing Loss: 0.0035420723725110292 | Testing Acc: 100.0\n",
            "Epoch: 1534 | Training Loss: 0.0023083642590790987 | Training Acc: 100.0 | Testing Loss: 0.003542007179930806 | Testing Acc: 100.0\n",
            "Epoch: 1535 | Training Loss: 0.002306389855220914 | Training Acc: 100.0 | Testing Loss: 0.0035377591848373413 | Testing Acc: 100.0\n",
            "Epoch: 1536 | Training Loss: 0.0023042894899845123 | Training Acc: 100.0 | Testing Loss: 0.0035338669549673796 | Testing Acc: 100.0\n",
            "Epoch: 1537 | Training Loss: 0.00230228528380394 | Training Acc: 100.0 | Testing Loss: 0.0035341759212315083 | Testing Acc: 100.0\n",
            "Epoch: 1538 | Training Loss: 0.0023002258967608213 | Training Acc: 100.0 | Testing Loss: 0.0035301330499351025 | Testing Acc: 100.0\n",
            "Epoch: 1539 | Training Loss: 0.0022981944493949413 | Training Acc: 100.0 | Testing Loss: 0.0035302292089909315 | Testing Acc: 100.0\n",
            "Epoch: 1540 | Training Loss: 0.002296177204698324 | Training Acc: 100.0 | Testing Loss: 0.003526153741404414 | Testing Acc: 100.0\n",
            "Epoch: 1541 | Training Loss: 0.002294110832735896 | Training Acc: 100.0 | Testing Loss: 0.0035262790042907 | Testing Acc: 100.0\n",
            "Epoch: 1542 | Training Loss: 0.0022921476047486067 | Training Acc: 100.0 | Testing Loss: 0.003522125305607915 | Testing Acc: 100.0\n",
            "Epoch: 1543 | Training Loss: 0.0022900414187461138 | Training Acc: 100.0 | Testing Loss: 0.0035182989668101072 | Testing Acc: 100.0\n",
            "Epoch: 1544 | Training Loss: 0.0022881138138473034 | Training Acc: 100.0 | Testing Loss: 0.0035186070017516613 | Testing Acc: 100.0\n",
            "Epoch: 1545 | Training Loss: 0.0022860197350382805 | Training Acc: 100.0 | Testing Loss: 0.003514625132083893 | Testing Acc: 100.0\n",
            "Epoch: 1546 | Training Loss: 0.0022840590681880713 | Training Acc: 100.0 | Testing Loss: 0.003514787880703807 | Testing Acc: 100.0\n",
            "Epoch: 1547 | Training Loss: 0.0022820201702415943 | Training Acc: 100.0 | Testing Loss: 0.003510706126689911 | Testing Acc: 100.0\n",
            "Epoch: 1548 | Training Loss: 0.002280015265569091 | Training Acc: 100.0 | Testing Loss: 0.003510774578899145 | Testing Acc: 100.0\n",
            "Epoch: 1549 | Training Loss: 0.002278032246977091 | Training Acc: 100.0 | Testing Loss: 0.0035066246055066586 | Testing Acc: 100.0\n",
            "Epoch: 1550 | Training Loss: 0.002275991952046752 | Training Acc: 100.0 | Testing Loss: 0.003506650682538748 | Testing Acc: 100.0\n",
            "Epoch: 1551 | Training Loss: 0.002274039899930358 | Training Acc: 100.0 | Testing Loss: 0.0035024501848965883 | Testing Acc: 100.0\n",
            "Epoch: 1552 | Training Loss: 0.0022719805128872395 | Training Acc: 100.0 | Testing Loss: 0.0034986946266144514 | Testing Acc: 100.0\n",
            "Epoch: 1553 | Training Loss: 0.0022700794506818056 | Training Acc: 100.0 | Testing Loss: 0.0034990492276847363 | Testing Acc: 100.0\n",
            "Epoch: 1554 | Training Loss: 0.002268014010041952 | Training Acc: 100.0 | Testing Loss: 0.003495103446766734 | Testing Acc: 100.0\n",
            "Epoch: 1555 | Training Loss: 0.0022660731337964535 | Training Acc: 100.0 | Testing Loss: 0.003495211945846677 | Testing Acc: 100.0\n",
            "Epoch: 1556 | Training Loss: 0.0022640638053417206 | Training Acc: 100.0 | Testing Loss: 0.003491206094622612 | Testing Acc: 100.0\n",
            "Epoch: 1557 | Training Loss: 0.00226208777166903 | Training Acc: 100.0 | Testing Loss: 0.00349121424369514 | Testing Acc: 100.0\n",
            "Epoch: 1558 | Training Loss: 0.002260115696117282 | Training Acc: 100.0 | Testing Loss: 0.0034871180541813374 | Testing Acc: 100.0\n",
            "Epoch: 1559 | Training Loss: 0.0022581233642995358 | Training Acc: 100.0 | Testing Loss: 0.00348709337413311 | Testing Acc: 100.0\n",
            "Epoch: 1560 | Training Loss: 0.0022561904042959213 | Training Acc: 100.0 | Testing Loss: 0.0034829298965632915 | Testing Acc: 100.0\n",
            "Epoch: 1561 | Training Loss: 0.002254164544865489 | Training Acc: 100.0 | Testing Loss: 0.0034792176447808743 | Testing Acc: 100.0\n",
            "Epoch: 1562 | Training Loss: 0.0022522606886923313 | Training Acc: 100.0 | Testing Loss: 0.0034794602543115616 | Testing Acc: 100.0\n",
            "Epoch: 1563 | Training Loss: 0.002250246237963438 | Training Acc: 100.0 | Testing Loss: 0.0034755864180624485 | Testing Acc: 100.0\n",
            "Epoch: 1564 | Training Loss: 0.0022483139764517546 | Training Acc: 100.0 | Testing Loss: 0.003475795267149806 | Testing Acc: 100.0\n",
            "Epoch: 1565 | Training Loss: 0.0022463430650532246 | Training Acc: 100.0 | Testing Loss: 0.0034718215465545654 | Testing Acc: 100.0\n",
            "Epoch: 1566 | Training Loss: 0.0022443770430982113 | Training Acc: 100.0 | Testing Loss: 0.0034718066453933716 | Testing Acc: 100.0\n",
            "Epoch: 1567 | Training Loss: 0.0022424482740461826 | Training Acc: 100.0 | Testing Loss: 0.003467794042080641 | Testing Acc: 100.0\n",
            "Epoch: 1568 | Training Loss: 0.002240454312413931 | Training Acc: 100.0 | Testing Loss: 0.003467746777459979 | Testing Acc: 100.0\n",
            "Epoch: 1569 | Training Loss: 0.002238560700789094 | Training Acc: 100.0 | Testing Loss: 0.003463671775534749 | Testing Acc: 100.0\n",
            "Epoch: 1570 | Training Loss: 0.0022365539334714413 | Training Acc: 100.0 | Testing Loss: 0.003459941130131483 | Testing Acc: 100.0\n",
            "Epoch: 1571 | Training Loss: 0.002234678016975522 | Training Acc: 100.0 | Testing Loss: 0.0034602172672748566 | Testing Acc: 100.0\n",
            "Epoch: 1572 | Training Loss: 0.002232698258012533 | Training Acc: 100.0 | Testing Loss: 0.00345633621327579 | Testing Acc: 100.0\n",
            "Epoch: 1573 | Training Loss: 0.002230776706710458 | Training Acc: 100.0 | Testing Loss: 0.0034564274828881025 | Testing Acc: 100.0\n",
            "Epoch: 1574 | Training Loss: 0.0022288435138761997 | Training Acc: 100.0 | Testing Loss: 0.0034525305964052677 | Testing Acc: 100.0\n",
            "Epoch: 1575 | Training Loss: 0.0022268935572355986 | Training Acc: 100.0 | Testing Loss: 0.0034526456147432327 | Testing Acc: 100.0\n",
            "Epoch: 1576 | Training Loss: 0.002225009724497795 | Training Acc: 100.0 | Testing Loss: 0.0034486292861402035 | Testing Acc: 100.0\n",
            "Epoch: 1577 | Training Loss: 0.002223031595349312 | Training Acc: 100.0 | Testing Loss: 0.003448581788688898 | Testing Acc: 100.0\n",
            "Epoch: 1578 | Training Loss: 0.0022211752366274595 | Training Acc: 100.0 | Testing Loss: 0.0034445493947714567 | Testing Acc: 100.0\n",
            "Epoch: 1579 | Training Loss: 0.0022191887255758047 | Training Acc: 100.0 | Testing Loss: 0.0034408688079565763 | Testing Acc: 100.0\n",
            "Epoch: 1580 | Training Loss: 0.002217330737039447 | Training Acc: 100.0 | Testing Loss: 0.0034411163069307804 | Testing Acc: 100.0\n",
            "Epoch: 1581 | Training Loss: 0.0022153703030198812 | Training Acc: 100.0 | Testing Loss: 0.0034373023081570864 | Testing Acc: 100.0\n",
            "Epoch: 1582 | Training Loss: 0.0022134785540401936 | Training Acc: 100.0 | Testing Loss: 0.003437438514083624 | Testing Acc: 100.0\n",
            "Epoch: 1583 | Training Loss: 0.0022115646861493587 | Training Acc: 100.0 | Testing Loss: 0.0034335001837462187 | Testing Acc: 100.0\n",
            "Epoch: 1584 | Training Loss: 0.0022096436005085707 | Training Acc: 100.0 | Testing Loss: 0.003433535574004054 | Testing Acc: 100.0\n",
            "Epoch: 1585 | Training Loss: 0.0022077772300690413 | Training Acc: 100.0 | Testing Loss: 0.0034295760560780764 | Testing Acc: 100.0\n",
            "Epoch: 1586 | Training Loss: 0.002205824013799429 | Training Acc: 100.0 | Testing Loss: 0.003429566975682974 | Testing Acc: 100.0\n",
            "Epoch: 1587 | Training Loss: 0.002203994197770953 | Training Acc: 100.0 | Testing Loss: 0.0034255508799105883 | Testing Acc: 100.0\n",
            "Epoch: 1588 | Training Loss: 0.00220203073695302 | Training Acc: 100.0 | Testing Loss: 0.0034220474772155285 | Testing Acc: 100.0\n",
            "Epoch: 1589 | Training Loss: 0.002200204646214843 | Training Acc: 100.0 | Testing Loss: 0.003422233508899808 | Testing Acc: 100.0\n",
            "Epoch: 1590 | Training Loss: 0.002198273316025734 | Training Acc: 100.0 | Testing Loss: 0.0034184507094323635 | Testing Acc: 100.0\n",
            "Epoch: 1591 | Training Loss: 0.0021964102052152157 | Training Acc: 100.0 | Testing Loss: 0.0034185932017862797 | Testing Acc: 100.0\n",
            "Epoch: 1592 | Training Loss: 0.00219452241435647 | Training Acc: 100.0 | Testing Loss: 0.0034146984107792377 | Testing Acc: 100.0\n",
            "Epoch: 1593 | Training Loss: 0.002192626940086484 | Training Acc: 100.0 | Testing Loss: 0.003414745209738612 | Testing Acc: 100.0\n",
            "Epoch: 1594 | Training Loss: 0.0021907712798565626 | Training Acc: 100.0 | Testing Loss: 0.003410806180909276 | Testing Acc: 100.0\n",
            "Epoch: 1595 | Training Loss: 0.0021888508927077055 | Training Acc: 100.0 | Testing Loss: 0.0034107971005141735 | Testing Acc: 100.0\n",
            "Epoch: 1596 | Training Loss: 0.0021870373748242855 | Training Acc: 100.0 | Testing Loss: 0.003406818490475416 | Testing Acc: 100.0\n",
            "Epoch: 1597 | Training Loss: 0.00218510371632874 | Training Acc: 100.0 | Testing Loss: 0.003403251990675926 | Testing Acc: 100.0\n",
            "Epoch: 1598 | Training Loss: 0.002183288335800171 | Training Acc: 100.0 | Testing Loss: 0.003403562353923917 | Testing Acc: 100.0\n",
            "Epoch: 1599 | Training Loss: 0.002181395422667265 | Training Acc: 100.0 | Testing Loss: 0.0033998568542301655 | Testing Acc: 100.0\n",
            "Epoch: 1600 | Training Loss: 0.0021795399952679873 | Training Acc: 100.0 | Testing Loss: 0.003399926470592618 | Testing Acc: 100.0\n",
            "Epoch: 1601 | Training Loss: 0.0021776873618364334 | Training Acc: 100.0 | Testing Loss: 0.0033961187582463026 | Testing Acc: 100.0\n",
            "Epoch: 1602 | Training Loss: 0.002175812376663089 | Training Acc: 100.0 | Testing Loss: 0.0033961341250687838 | Testing Acc: 100.0\n",
            "Epoch: 1603 | Training Loss: 0.00217399001121521 | Training Acc: 100.0 | Testing Loss: 0.003392270300537348 | Testing Acc: 100.0\n",
            "Epoch: 1604 | Training Loss: 0.002172084292396903 | Training Acc: 100.0 | Testing Loss: 0.0033922058064490557 | Testing Acc: 100.0\n",
            "Epoch: 1605 | Training Loss: 0.002170307096093893 | Training Acc: 100.0 | Testing Loss: 0.0033882930874824524 | Testing Acc: 100.0\n",
            "Epoch: 1606 | Training Loss: 0.002168404869735241 | Training Acc: 100.0 | Testing Loss: 0.0033847701270133257 | Testing Acc: 100.0\n",
            "Epoch: 1607 | Training Loss: 0.0021665948443114758 | Training Acc: 100.0 | Testing Loss: 0.003384979907423258 | Testing Acc: 100.0\n",
            "Epoch: 1608 | Training Loss: 0.002164734760299325 | Training Acc: 100.0 | Testing Loss: 0.0033813000191003084 | Testing Acc: 100.0\n",
            "Epoch: 1609 | Training Loss: 0.002162899589166045 | Training Acc: 100.0 | Testing Loss: 0.0033813740592449903 | Testing Acc: 100.0\n",
            "Epoch: 1610 | Training Loss: 0.00216106278821826 | Training Acc: 100.0 | Testing Loss: 0.003377628279849887 | Testing Acc: 100.0\n",
            "Epoch: 1611 | Training Loss: 0.0021592110861092806 | Training Acc: 100.0 | Testing Loss: 0.003377715591341257 | Testing Acc: 100.0\n",
            "Epoch: 1612 | Training Loss: 0.002157423412427306 | Training Acc: 100.0 | Testing Loss: 0.003373902291059494 | Testing Acc: 100.0\n",
            "Epoch: 1613 | Training Loss: 0.0021555302664637566 | Training Acc: 100.0 | Testing Loss: 0.0033703595399856567 | Testing Acc: 100.0\n",
            "Epoch: 1614 | Training Loss: 0.0021537691354751587 | Training Acc: 100.0 | Testing Loss: 0.003370625199750066 | Testing Acc: 100.0\n",
            "Epoch: 1615 | Training Loss: 0.0021519071888178587 | Training Acc: 100.0 | Testing Loss: 0.003366966964676976 | Testing Acc: 100.0\n",
            "Epoch: 1616 | Training Loss: 0.0021501153241842985 | Training Acc: 100.0 | Testing Loss: 0.0033670824486762285 | Testing Acc: 100.0\n",
            "Epoch: 1617 | Training Loss: 0.002148288767784834 | Training Acc: 100.0 | Testing Loss: 0.003363379742950201 | Testing Acc: 100.0\n",
            "Epoch: 1618 | Training Loss: 0.0021464666351675987 | Training Acc: 100.0 | Testing Loss: 0.0033633876591920853 | Testing Acc: 100.0\n",
            "Epoch: 1619 | Training Loss: 0.002144675003364682 | Training Acc: 100.0 | Testing Loss: 0.003359628841280937 | Testing Acc: 100.0\n",
            "Epoch: 1620 | Training Loss: 0.0021428235340863466 | Training Acc: 100.0 | Testing Loss: 0.00335957040078938 | Testing Acc: 100.0\n",
            "Epoch: 1621 | Training Loss: 0.002141067059710622 | Training Acc: 100.0 | Testing Loss: 0.0033557552378624678 | Testing Acc: 100.0\n",
            "Epoch: 1622 | Training Loss: 0.0021392027847468853 | Training Acc: 100.0 | Testing Loss: 0.003352342639118433 | Testing Acc: 100.0\n",
            "Epoch: 1623 | Training Loss: 0.0021374598145484924 | Training Acc: 100.0 | Testing Loss: 0.003352557076141238 | Testing Acc: 100.0\n",
            "Epoch: 1624 | Training Loss: 0.002135609509423375 | Training Acc: 100.0 | Testing Loss: 0.0033489211928099394 | Testing Acc: 100.0\n",
            "Epoch: 1625 | Training Loss: 0.002133834408596158 | Training Acc: 100.0 | Testing Loss: 0.0033490185160189867 | Testing Acc: 100.0\n",
            "Epoch: 1626 | Training Loss: 0.0021320295054465532 | Training Acc: 100.0 | Testing Loss: 0.0033452745992690325 | Testing Acc: 100.0\n",
            "Epoch: 1627 | Training Loss: 0.002130229724571109 | Training Acc: 100.0 | Testing Loss: 0.003345340024679899 | Testing Acc: 100.0\n",
            "Epoch: 1628 | Training Loss: 0.0021284683607518673 | Training Acc: 100.0 | Testing Loss: 0.003341556992381811 | Testing Acc: 100.0\n",
            "Epoch: 1629 | Training Loss: 0.0021266331896185875 | Training Acc: 100.0 | Testing Loss: 0.0033415101934224367 | Testing Acc: 100.0\n",
            "Epoch: 1630 | Training Loss: 0.0021249086130410433 | Training Acc: 100.0 | Testing Loss: 0.00333773298189044 | Testing Acc: 100.0\n",
            "Epoch: 1631 | Training Loss: 0.0021230499260127544 | Training Acc: 100.0 | Testing Loss: 0.003334262641146779 | Testing Acc: 100.0\n",
            "Epoch: 1632 | Training Loss: 0.0021213358268141747 | Training Acc: 100.0 | Testing Loss: 0.0033345897682011127 | Testing Acc: 100.0\n",
            "Epoch: 1633 | Training Loss: 0.0021195178851485252 | Training Acc: 100.0 | Testing Loss: 0.003330973442643881 | Testing Acc: 100.0\n",
            "Epoch: 1634 | Training Loss: 0.0021177539601922035 | Training Acc: 100.0 | Testing Loss: 0.0033310826402157545 | Testing Acc: 100.0\n",
            "Epoch: 1635 | Training Loss: 0.0021159786265343428 | Training Acc: 100.0 | Testing Loss: 0.003327376674860716 | Testing Acc: 100.0\n",
            "Epoch: 1636 | Training Loss: 0.0021141970064491034 | Training Acc: 100.0 | Testing Loss: 0.003327413694933057 | Testing Acc: 100.0\n",
            "Epoch: 1637 | Training Loss: 0.0021124582272022963 | Training Acc: 100.0 | Testing Loss: 0.0033236630260944366 | Testing Acc: 100.0\n",
            "Epoch: 1638 | Training Loss: 0.002110640984028578 | Training Acc: 100.0 | Testing Loss: 0.003323643235489726 | Testing Acc: 100.0\n",
            "Epoch: 1639 | Training Loss: 0.002108945045620203 | Training Acc: 100.0 | Testing Loss: 0.003319876966997981 | Testing Acc: 100.0\n",
            "Epoch: 1640 | Training Loss: 0.0021071177907288074 | Training Acc: 100.0 | Testing Loss: 0.0033164441119879484 | Testing Acc: 100.0\n",
            "Epoch: 1641 | Training Loss: 0.0021054253447800875 | Training Acc: 100.0 | Testing Loss: 0.0033166769426316023 | Testing Acc: 100.0\n",
            "Epoch: 1642 | Training Loss: 0.0021036171820014715 | Training Acc: 100.0 | Testing Loss: 0.003313115332275629 | Testing Acc: 100.0\n",
            "Epoch: 1643 | Training Loss: 0.002101893536746502 | Training Acc: 100.0 | Testing Loss: 0.003313236404210329 | Testing Acc: 100.0\n",
            "Epoch: 1644 | Training Loss: 0.00210013659670949 | Training Acc: 100.0 | Testing Loss: 0.0033095856197178364 | Testing Acc: 100.0\n",
            "Epoch: 1645 | Training Loss: 0.0020983745343983173 | Training Acc: 100.0 | Testing Loss: 0.003309729043394327 | Testing Acc: 100.0\n",
            "Epoch: 1646 | Training Loss: 0.0020966578740626574 | Training Acc: 100.0 | Testing Loss: 0.003306005150079727 | Testing Acc: 100.0\n",
            "Epoch: 1647 | Training Loss: 0.002094863448292017 | Training Acc: 100.0 | Testing Loss: 0.0033059916459023952 | Testing Acc: 100.0\n",
            "Epoch: 1648 | Training Loss: 0.0020931879989802837 | Training Acc: 100.0 | Testing Loss: 0.0033022514544427395 | Testing Acc: 100.0\n",
            "Epoch: 1649 | Training Loss: 0.0020913882181048393 | Training Acc: 100.0 | Testing Loss: 0.003298896597698331 | Testing Acc: 100.0\n",
            "Epoch: 1650 | Training Loss: 0.002089709509164095 | Training Acc: 100.0 | Testing Loss: 0.003299066796898842 | Testing Acc: 100.0\n",
            "Epoch: 1651 | Training Loss: 0.002087926957756281 | Training Acc: 100.0 | Testing Loss: 0.003295543836429715 | Testing Acc: 100.0\n",
            "Epoch: 1652 | Training Loss: 0.002086217515170574 | Training Acc: 100.0 | Testing Loss: 0.003295640693977475 | Testing Acc: 100.0\n",
            "Epoch: 1653 | Training Loss: 0.002084482926875353 | Training Acc: 100.0 | Testing Loss: 0.0032920222729444504 | Testing Acc: 100.0\n",
            "Epoch: 1654 | Training Loss: 0.0020827469415962696 | Training Acc: 100.0 | Testing Loss: 0.0032920707017183304 | Testing Acc: 100.0\n",
            "Epoch: 1655 | Training Loss: 0.00208104751072824 | Training Acc: 100.0 | Testing Loss: 0.0032884012907743454 | Testing Acc: 100.0\n",
            "Epoch: 1656 | Training Loss: 0.0020792726427316666 | Training Acc: 100.0 | Testing Loss: 0.003288466017693281 | Testing Acc: 100.0\n",
            "Epoch: 1657 | Training Loss: 0.0020776190795004368 | Training Acc: 100.0 | Testing Loss: 0.003284758422523737 | Testing Acc: 100.0\n",
            "Epoch: 1658 | Training Loss: 0.0020758460741490126 | Training Acc: 100.0 | Testing Loss: 0.0032814410515129566 | Testing Acc: 100.0\n",
            "Epoch: 1659 | Training Loss: 0.0020741778425872326 | Training Acc: 100.0 | Testing Loss: 0.0032816175371408463 | Testing Acc: 100.0\n",
            "Epoch: 1660 | Training Loss: 0.0020724288187921047 | Training Acc: 100.0 | Testing Loss: 0.0032781544141471386 | Testing Acc: 100.0\n",
            "Epoch: 1661 | Training Loss: 0.0020707377698272467 | Training Acc: 100.0 | Testing Loss: 0.0032782130874693394 | Testing Acc: 100.0\n",
            "Epoch: 1662 | Training Loss: 0.002069024369120598 | Training Acc: 100.0 | Testing Loss: 0.003274656133726239 | Testing Acc: 100.0\n",
            "Epoch: 1663 | Training Loss: 0.002067300956696272 | Training Acc: 100.0 | Testing Loss: 0.00327468104660511 | Testing Acc: 100.0\n",
            "Epoch: 1664 | Training Loss: 0.0020656283013522625 | Training Acc: 100.0 | Testing Loss: 0.0032710558734834194 | Testing Acc: 100.0\n",
            "Epoch: 1665 | Training Loss: 0.002063875086605549 | Training Acc: 100.0 | Testing Loss: 0.003267793683335185 | Testing Acc: 100.0\n",
            "Epoch: 1666 | Training Loss: 0.0020622452720999718 | Training Acc: 100.0 | Testing Loss: 0.003268083091825247 | Testing Acc: 100.0\n",
            "Epoch: 1667 | Training Loss: 0.002060504397377372 | Training Acc: 100.0 | Testing Loss: 0.00326465698890388 | Testing Acc: 100.0\n",
            "Epoch: 1668 | Training Loss: 0.0020588357001543045 | Training Acc: 100.0 | Testing Loss: 0.0032647333573549986 | Testing Acc: 100.0\n",
            "Epoch: 1669 | Training Loss: 0.0020571283530443907 | Training Acc: 100.0 | Testing Loss: 0.003261242061853409 | Testing Acc: 100.0\n",
            "Epoch: 1670 | Training Loss: 0.002055444521829486 | Training Acc: 100.0 | Testing Loss: 0.00326124532148242 | Testing Acc: 100.0\n",
            "Epoch: 1671 | Training Loss: 0.0020537637174129486 | Training Acc: 100.0 | Testing Loss: 0.00325770303606987 | Testing Acc: 100.0\n",
            "Epoch: 1672 | Training Loss: 0.0020520498510450125 | Training Acc: 100.0 | Testing Loss: 0.0032576564699411392 | Testing Acc: 100.0\n",
            "Epoch: 1673 | Training Loss: 0.0020504072308540344 | Training Acc: 100.0 | Testing Loss: 0.003254023613408208 | Testing Acc: 100.0\n",
            "Epoch: 1674 | Training Loss: 0.0020486824214458466 | Training Acc: 100.0 | Testing Loss: 0.0032507602591067553 | Testing Acc: 100.0\n",
            "Epoch: 1675 | Training Loss: 0.002047059591859579 | Training Acc: 100.0 | Testing Loss: 0.0032509490847587585 | Testing Acc: 100.0\n",
            "Epoch: 1676 | Training Loss: 0.0020453431643545628 | Training Acc: 100.0 | Testing Loss: 0.003247567918151617 | Testing Acc: 100.0\n",
            "Epoch: 1677 | Training Loss: 0.00204368750564754 | Training Acc: 100.0 | Testing Loss: 0.0032477222848683596 | Testing Acc: 100.0\n",
            "Epoch: 1678 | Training Loss: 0.002042014617472887 | Training Acc: 100.0 | Testing Loss: 0.003244251711294055 | Testing Acc: 100.0\n",
            "Epoch: 1679 | Training Loss: 0.002040339633822441 | Training Acc: 100.0 | Testing Loss: 0.003244244260713458 | Testing Acc: 100.0\n",
            "Epoch: 1680 | Training Loss: 0.0020386846736073494 | Training Acc: 100.0 | Testing Loss: 0.003240711987018585 | Testing Acc: 100.0\n",
            "Epoch: 1681 | Training Loss: 0.002036993158981204 | Training Acc: 100.0 | Testing Loss: 0.0032406658865511417 | Testing Acc: 100.0\n",
            "Epoch: 1682 | Training Loss: 0.0020353724248707294 | Training Acc: 100.0 | Testing Loss: 0.0032371103297919035 | Testing Acc: 100.0\n",
            "Epoch: 1683 | Training Loss: 0.0020336692687124014 | Training Acc: 100.0 | Testing Loss: 0.003233840689063072 | Testing Acc: 100.0\n",
            "Epoch: 1684 | Training Loss: 0.0020320608746260405 | Training Acc: 100.0 | Testing Loss: 0.003234000178053975 | Testing Acc: 100.0\n",
            "Epoch: 1685 | Training Loss: 0.0020303723867982626 | Training Acc: 100.0 | Testing Loss: 0.0032306467182934284 | Testing Acc: 100.0\n",
            "Epoch: 1686 | Training Loss: 0.0020287330262362957 | Training Acc: 100.0 | Testing Loss: 0.003230694215744734 | Testing Acc: 100.0\n",
            "Epoch: 1687 | Training Loss: 0.0020270782988518476 | Training Acc: 100.0 | Testing Loss: 0.00322728487662971 | Testing Acc: 100.0\n",
            "Epoch: 1688 | Training Loss: 0.0020254163537174463 | Training Acc: 100.0 | Testing Loss: 0.0032273493707180023 | Testing Acc: 100.0\n",
            "Epoch: 1689 | Training Loss: 0.002023793291300535 | Training Acc: 100.0 | Testing Loss: 0.0032238666899502277 | Testing Acc: 100.0\n",
            "Epoch: 1690 | Training Loss: 0.0020221087615936995 | Training Acc: 100.0 | Testing Loss: 0.0032238089479506016 | Testing Acc: 100.0\n",
            "Epoch: 1691 | Training Loss: 0.002020514803007245 | Training Acc: 100.0 | Testing Loss: 0.003220280632376671 | Testing Acc: 100.0\n",
            "Epoch: 1692 | Training Loss: 0.0020188328344374895 | Training Acc: 100.0 | Testing Loss: 0.0032170440535992384 | Testing Acc: 100.0\n",
            "Epoch: 1693 | Training Loss: 0.0020172332879155874 | Training Acc: 100.0 | Testing Loss: 0.00321720982901752 | Testing Acc: 100.0\n",
            "Epoch: 1694 | Training Loss: 0.002015571342781186 | Training Acc: 100.0 | Testing Loss: 0.003213900374248624 | Testing Acc: 100.0\n",
            "Epoch: 1695 | Training Loss: 0.002013956429436803 | Training Acc: 100.0 | Testing Loss: 0.0032139415852725506 | Testing Acc: 100.0\n",
            "Epoch: 1696 | Training Loss: 0.0020123240537941456 | Training Acc: 100.0 | Testing Loss: 0.0032105478458106518 | Testing Acc: 100.0\n",
            "Epoch: 1697 | Training Loss: 0.002010676544159651 | Training Acc: 100.0 | Testing Loss: 0.0032105459831655025 | Testing Acc: 100.0\n",
            "Epoch: 1698 | Training Loss: 0.002009078161790967 | Training Acc: 100.0 | Testing Loss: 0.003207094967365265 | Testing Acc: 100.0\n",
            "Epoch: 1699 | Training Loss: 0.0020074136555194855 | Training Acc: 100.0 | Testing Loss: 0.0032070365268737078 | Testing Acc: 100.0\n",
            "Epoch: 1700 | Training Loss: 0.002005848102271557 | Training Acc: 100.0 | Testing Loss: 0.003203570144250989 | Testing Acc: 100.0\n",
            "Epoch: 1701 | Training Loss: 0.0020041815005242825 | Training Acc: 100.0 | Testing Loss: 0.0032004439271986485 | Testing Acc: 100.0\n",
            "Epoch: 1702 | Training Loss: 0.002002597087994218 | Training Acc: 100.0 | Testing Loss: 0.0032005805987864733 | Testing Acc: 100.0\n",
            "Epoch: 1703 | Training Loss: 0.002000963082537055 | Training Acc: 100.0 | Testing Loss: 0.0031972918659448624 | Testing Acc: 100.0\n",
            "Epoch: 1704 | Training Loss: 0.0019993435125797987 | Training Acc: 100.0 | Testing Loss: 0.0031973342411220074 | Testing Acc: 100.0\n",
            "Epoch: 1705 | Training Loss: 0.001997749786823988 | Training Acc: 100.0 | Testing Loss: 0.0031939675100147724 | Testing Acc: 100.0\n",
            "Epoch: 1706 | Training Loss: 0.0019961120560765266 | Training Acc: 100.0 | Testing Loss: 0.0031939540058374405 | Testing Acc: 100.0\n",
            "Epoch: 1707 | Training Loss: 0.001994546502828598 | Training Acc: 100.0 | Testing Loss: 0.0031905476935207844 | Testing Acc: 100.0\n",
            "Epoch: 1708 | Training Loss: 0.001992893172428012 | Training Acc: 100.0 | Testing Loss: 0.003187409834936261 | Testing Acc: 100.0\n",
            "Epoch: 1709 | Training Loss: 0.001991341821849346 | Training Acc: 100.0 | Testing Loss: 0.0031876317225396633 | Testing Acc: 100.0\n",
            "Epoch: 1710 | Training Loss: 0.0019897071179002523 | Training Acc: 100.0 | Testing Loss: 0.003184353234246373 | Testing Acc: 100.0\n",
            "Epoch: 1711 | Training Loss: 0.0019881301559507847 | Training Acc: 100.0 | Testing Loss: 0.0031844910699874163 | Testing Acc: 100.0\n",
            "Epoch: 1712 | Training Loss: 0.001986523624509573 | Training Acc: 100.0 | Testing Loss: 0.003181185107678175 | Testing Acc: 100.0\n",
            "Epoch: 1713 | Training Loss: 0.001984917325899005 | Training Acc: 100.0 | Testing Loss: 0.003181177657097578 | Testing Acc: 100.0\n",
            "Epoch: 1714 | Training Loss: 0.0019833515398204327 | Training Acc: 100.0 | Testing Loss: 0.00317780370824039 | Testing Acc: 100.0\n",
            "Epoch: 1715 | Training Loss: 0.0019817238207906485 | Training Acc: 100.0 | Testing Loss: 0.0031777401454746723 | Testing Acc: 100.0\n",
            "Epoch: 1716 | Training Loss: 0.00198018504306674 | Training Acc: 100.0 | Testing Loss: 0.0031743324361741543 | Testing Acc: 100.0\n",
            "Epoch: 1717 | Training Loss: 0.001978557789698243 | Training Acc: 100.0 | Testing Loss: 0.003171187359839678 | Testing Acc: 100.0\n",
            "Epoch: 1718 | Training Loss: 0.0019770103972405195 | Training Acc: 100.0 | Testing Loss: 0.0031713638454675674 | Testing Acc: 100.0\n",
            "Epoch: 1719 | Training Loss: 0.001975401071831584 | Training Acc: 100.0 | Testing Loss: 0.003168124705553055 | Testing Acc: 100.0\n",
            "Epoch: 1720 | Training Loss: 0.0019738355185836554 | Training Acc: 100.0 | Testing Loss: 0.003168173134326935 | Testing Acc: 100.0\n",
            "Epoch: 1721 | Training Loss: 0.0019722599536180496 | Training Acc: 100.0 | Testing Loss: 0.0031649935990571976 | Testing Acc: 100.0\n",
            "Epoch: 1722 | Training Loss: 0.0019706678576767445 | Training Acc: 100.0 | Testing Loss: 0.0031651712488383055 | Testing Acc: 100.0\n",
            "Epoch: 1723 | Training Loss: 0.0019691232591867447 | Training Acc: 100.0 | Testing Loss: 0.003161908593028784 | Testing Acc: 100.0\n",
            "Epoch: 1724 | Training Loss: 0.0019675083458423615 | Training Acc: 100.0 | Testing Loss: 0.0031619183719158173 | Testing Acc: 100.0\n",
            "Epoch: 1725 | Training Loss: 0.0019659933168441057 | Training Acc: 100.0 | Testing Loss: 0.0031586047261953354 | Testing Acc: 100.0\n",
            "Epoch: 1726 | Training Loss: 0.0019643798004835844 | Training Acc: 100.0 | Testing Loss: 0.0031555206514894962 | Testing Acc: 100.0\n",
            "Epoch: 1727 | Training Loss: 0.0019628587178885937 | Training Acc: 100.0 | Testing Loss: 0.0031557034235447645 | Testing Acc: 100.0\n",
            "Epoch: 1728 | Training Loss: 0.001961272908374667 | Training Acc: 100.0 | Testing Loss: 0.0031525406520813704 | Testing Acc: 100.0\n",
            "Epoch: 1729 | Training Loss: 0.0019597222562879324 | Training Acc: 100.0 | Testing Loss: 0.0031526111997663975 | Testing Acc: 100.0\n",
            "Epoch: 1730 | Training Loss: 0.001958167180418968 | Training Acc: 100.0 | Testing Loss: 0.0031493816059082747 | Testing Acc: 100.0\n",
            "Epoch: 1731 | Training Loss: 0.0019565909169614315 | Training Acc: 100.0 | Testing Loss: 0.0031493790447711945 | Testing Acc: 100.0\n",
            "Epoch: 1732 | Training Loss: 0.00195506913587451 | Training Acc: 100.0 | Testing Loss: 0.0031460702884942293 | Testing Acc: 100.0\n",
            "Epoch: 1733 | Training Loss: 0.0019534744787961245 | Training Acc: 100.0 | Testing Loss: 0.0031461138278245926 | Testing Acc: 100.0\n",
            "Epoch: 1734 | Training Loss: 0.0019519792404025793 | Training Acc: 100.0 | Testing Loss: 0.0031427766662091017 | Testing Acc: 100.0\n",
            "Epoch: 1735 | Training Loss: 0.0019503891235217452 | Training Acc: 100.0 | Testing Loss: 0.003139698412269354 | Testing Acc: 100.0\n",
            "Epoch: 1736 | Training Loss: 0.0019488763064146042 | Training Acc: 100.0 | Testing Loss: 0.0031398304272443056 | Testing Acc: 100.0\n",
            "Epoch: 1737 | Training Loss: 0.0019473170395940542 | Training Acc: 100.0 | Testing Loss: 0.003136690240353346 | Testing Acc: 100.0\n",
            "Epoch: 1738 | Training Loss: 0.0019457719754427671 | Training Acc: 100.0 | Testing Loss: 0.0031367384362965822 | Testing Acc: 100.0\n",
            "Epoch: 1739 | Training Loss: 0.0019442472839727998 | Training Acc: 100.0 | Testing Loss: 0.003133502323180437 | Testing Acc: 100.0\n",
            "Epoch: 1740 | Training Loss: 0.001942685223184526 | Training Acc: 100.0 | Testing Loss: 0.0031334776431322098 | Testing Acc: 100.0\n",
            "Epoch: 1741 | Training Loss: 0.0019411767134442925 | Training Acc: 100.0 | Testing Loss: 0.003130184020847082 | Testing Acc: 100.0\n",
            "Epoch: 1742 | Training Loss: 0.001939602429047227 | Training Acc: 100.0 | Testing Loss: 0.0031301260460168123 | Testing Acc: 100.0\n",
            "Epoch: 1743 | Training Loss: 0.001938123838044703 | Training Acc: 100.0 | Testing Loss: 0.0031268279999494553 | Testing Acc: 100.0\n",
            "Epoch: 1744 | Training Loss: 0.0019365461776033044 | Training Acc: 100.0 | Testing Loss: 0.0031238612718880177 | Testing Acc: 100.0\n",
            "Epoch: 1745 | Training Loss: 0.0019350554794073105 | Training Acc: 100.0 | Testing Loss: 0.0031239993404597044 | Testing Acc: 100.0\n",
            "Epoch: 1746 | Training Loss: 0.0019335101824253798 | Training Acc: 100.0 | Testing Loss: 0.0031208512373268604 | Testing Acc: 100.0\n",
            "Epoch: 1747 | Training Loss: 0.0019319970160722733 | Training Acc: 100.0 | Testing Loss: 0.0031209003645926714 | Testing Acc: 100.0\n",
            "Epoch: 1748 | Training Loss: 0.0019304826855659485 | Training Acc: 100.0 | Testing Loss: 0.0031176963821053505 | Testing Acc: 100.0\n",
            "Epoch: 1749 | Training Loss: 0.0019289398333057761 | Training Acc: 100.0 | Testing Loss: 0.0031176782213151455 | Testing Acc: 100.0\n",
            "Epoch: 1750 | Training Loss: 0.001927455305121839 | Training Acc: 100.0 | Testing Loss: 0.0031144176609814167 | Testing Acc: 100.0\n",
            "Epoch: 1751 | Training Loss: 0.0019258891697973013 | Training Acc: 100.0 | Testing Loss: 0.0031114153098315 | Testing Acc: 100.0\n",
            "Epoch: 1752 | Training Loss: 0.0019244368886575103 | Training Acc: 100.0 | Testing Loss: 0.0031115817837417126 | Testing Acc: 100.0\n",
            "Epoch: 1753 | Training Loss: 0.0019228918245062232 | Training Acc: 100.0 | Testing Loss: 0.003108483739197254 | Testing Acc: 100.0\n",
            "Epoch: 1754 | Training Loss: 0.0019214022904634476 | Training Acc: 100.0 | Testing Loss: 0.003108616452664137 | Testing Acc: 100.0\n",
            "Epoch: 1755 | Training Loss: 0.001919884467497468 | Training Acc: 100.0 | Testing Loss: 0.0031054567079991102 | Testing Acc: 100.0\n",
            "Epoch: 1756 | Training Loss: 0.0019183693220838904 | Training Acc: 100.0 | Testing Loss: 0.0031054385472089052 | Testing Acc: 100.0\n",
            "Epoch: 1757 | Training Loss: 0.0019168792059645057 | Training Acc: 100.0 | Testing Loss: 0.0031022275798022747 | Testing Acc: 100.0\n",
            "Epoch: 1758 | Training Loss: 0.0019153591711074114 | Training Acc: 100.0 | Testing Loss: 0.0031021698378026485 | Testing Acc: 100.0\n",
            "Epoch: 1759 | Training Loss: 0.0019138917559757829 | Training Acc: 100.0 | Testing Loss: 0.0030989088118076324 | Testing Acc: 100.0\n",
            "Epoch: 1760 | Training Loss: 0.001912356005050242 | Training Acc: 100.0 | Testing Loss: 0.0030959113501012325 | Testing Acc: 100.0\n",
            "Epoch: 1761 | Training Loss: 0.0019109001150354743 | Training Acc: 100.0 | Testing Loss: 0.003096049651503563 | Testing Acc: 100.0\n",
            "Epoch: 1762 | Training Loss: 0.0019093791488558054 | Training Acc: 100.0 | Testing Loss: 0.003092984901741147 | Testing Acc: 100.0\n",
            "Epoch: 1763 | Training Loss: 0.0019079085905104876 | Training Acc: 100.0 | Testing Loss: 0.0030930335633456707 | Testing Acc: 100.0\n",
            "Epoch: 1764 | Training Loss: 0.00190640555229038 | Training Acc: 100.0 | Testing Loss: 0.003089883830398321 | Testing Acc: 100.0\n",
            "Epoch: 1765 | Training Loss: 0.0019049153197556734 | Training Acc: 100.0 | Testing Loss: 0.003089871257543564 | Testing Acc: 100.0\n",
            "Epoch: 1766 | Training Loss: 0.0019034467404708266 | Training Acc: 100.0 | Testing Loss: 0.0030866763554513454 | Testing Acc: 100.0\n",
            "Epoch: 1767 | Training Loss: 0.0019019407918676734 | Training Acc: 100.0 | Testing Loss: 0.003086686134338379 | Testing Acc: 100.0\n",
            "Epoch: 1768 | Training Loss: 0.0019004963105544448 | Training Acc: 100.0 | Testing Loss: 0.0030834570061415434 | Testing Acc: 100.0\n",
            "Epoch: 1769 | Training Loss: 0.0018989730160683393 | Training Acc: 100.0 | Testing Loss: 0.003080481430515647 | Testing Acc: 100.0\n",
            "Epoch: 1770 | Training Loss: 0.0018975331913679838 | Training Acc: 100.0 | Testing Loss: 0.0030806094873696566 | Testing Acc: 100.0\n",
            "Epoch: 1771 | Training Loss: 0.001896028988994658 | Training Acc: 100.0 | Testing Loss: 0.0030775717459619045 | Testing Acc: 100.0\n",
            "Epoch: 1772 | Training Loss: 0.0018945669289678335 | Training Acc: 100.0 | Testing Loss: 0.0030776034109294415 | Testing Acc: 100.0\n",
            "Epoch: 1773 | Training Loss: 0.0018930953228846192 | Training Acc: 100.0 | Testing Loss: 0.0030744983814656734 | Testing Acc: 100.0\n",
            "Epoch: 1774 | Training Loss: 0.0018916208064183593 | Training Acc: 100.0 | Testing Loss: 0.003074473701417446 | Testing Acc: 100.0\n",
            "Epoch: 1775 | Training Loss: 0.0018901675939559937 | Training Acc: 100.0 | Testing Loss: 0.003071306273341179 | Testing Acc: 100.0\n",
            "Epoch: 1776 | Training Loss: 0.0018886716570705175 | Training Acc: 100.0 | Testing Loss: 0.0030712545849382877 | Testing Acc: 100.0\n",
            "Epoch: 1777 | Training Loss: 0.0018872451037168503 | Training Acc: 100.0 | Testing Loss: 0.0030680366326123476 | Testing Acc: 100.0\n",
            "Epoch: 1778 | Training Loss: 0.001885745907202363 | Training Acc: 100.0 | Testing Loss: 0.0030652121640741825 | Testing Acc: 100.0\n",
            "Epoch: 1779 | Training Loss: 0.0018843181896954775 | Training Acc: 100.0 | Testing Loss: 0.0030653164722025394 | Testing Acc: 100.0\n",
            "Epoch: 1780 | Training Loss: 0.001882839249446988 | Training Acc: 100.0 | Testing Loss: 0.003062300384044647 | Testing Acc: 100.0\n",
            "Epoch: 1781 | Training Loss: 0.0018813887145370245 | Training Acc: 100.0 | Testing Loss: 0.003062325995415449 | Testing Acc: 100.0\n",
            "Epoch: 1782 | Training Loss: 0.001879936782643199 | Training Acc: 100.0 | Testing Loss: 0.003059231210500002 | Testing Acc: 100.0\n",
            "Epoch: 1783 | Training Loss: 0.0018784705316647887 | Training Acc: 100.0 | Testing Loss: 0.003059206996113062 | Testing Acc: 100.0\n",
            "Epoch: 1784 | Training Loss: 0.0018770430469885468 | Training Acc: 100.0 | Testing Loss: 0.003056077752262354 | Testing Acc: 100.0\n",
            "Epoch: 1785 | Training Loss: 0.0018755538621917367 | Training Acc: 100.0 | Testing Loss: 0.0030560146551579237 | Testing Acc: 100.0\n",
            "Epoch: 1786 | Training Loss: 0.0018741560634225607 | Training Acc: 100.0 | Testing Loss: 0.0030528355855494738 | Testing Acc: 100.0\n",
            "Epoch: 1787 | Training Loss: 0.0018726667622104287 | Training Acc: 100.0 | Testing Loss: 0.003049935679882765 | Testing Acc: 100.0\n",
            "Epoch: 1788 | Training Loss: 0.0018712558085098863 | Training Acc: 100.0 | Testing Loss: 0.0030501424334943295 | Testing Acc: 100.0\n",
            "Epoch: 1789 | Training Loss: 0.0018697973573580384 | Training Acc: 100.0 | Testing Loss: 0.003047159407287836 | Testing Acc: 100.0\n",
            "Epoch: 1790 | Training Loss: 0.0018683609087020159 | Training Acc: 100.0 | Testing Loss: 0.0030471973586827517 | Testing Acc: 100.0\n",
            "Epoch: 1791 | Training Loss: 0.0018669201526790857 | Training Acc: 100.0 | Testing Loss: 0.003044129116460681 | Testing Acc: 100.0\n",
            "Epoch: 1792 | Training Loss: 0.001865477068349719 | Training Acc: 100.0 | Testing Loss: 0.0030440993141382933 | Testing Acc: 100.0\n",
            "Epoch: 1793 | Training Loss: 0.0018640706548467278 | Training Acc: 100.0 | Testing Loss: 0.0030409698374569416 | Testing Acc: 100.0\n",
            "Epoch: 1794 | Training Loss: 0.0018625881057232618 | Training Acc: 100.0 | Testing Loss: 0.0030381260439753532 | Testing Acc: 100.0\n",
            "Epoch: 1795 | Training Loss: 0.0018612161511555314 | Training Acc: 100.0 | Testing Loss: 0.003038270166143775 | Testing Acc: 100.0\n",
            "Epoch: 1796 | Training Loss: 0.001859741285443306 | Training Acc: 100.0 | Testing Loss: 0.003035337198525667 | Testing Acc: 100.0\n",
            "Epoch: 1797 | Training Loss: 0.0018583523342385888 | Training Acc: 100.0 | Testing Loss: 0.003035380272194743 | Testing Acc: 100.0\n",
            "Epoch: 1798 | Training Loss: 0.001856898539699614 | Training Acc: 100.0 | Testing Loss: 0.0030323737300932407 | Testing Acc: 100.0\n",
            "Epoch: 1799 | Training Loss: 0.001855483977124095 | Training Acc: 100.0 | Testing Loss: 0.003032428678125143 | Testing Acc: 100.0\n",
            "Epoch: 1800 | Training Loss: 0.0018540674354881048 | Training Acc: 100.0 | Testing Loss: 0.0030293383169919252 | Testing Acc: 100.0\n",
            "Epoch: 1801 | Training Loss: 0.001852623769082129 | Training Acc: 100.0 | Testing Loss: 0.003029290586709976 | Testing Acc: 100.0\n",
            "Epoch: 1802 | Training Loss: 0.0018512473907321692 | Training Acc: 100.0 | Testing Loss: 0.0030261711217463017 | Testing Acc: 100.0\n",
            "Epoch: 1803 | Training Loss: 0.0018497795099392533 | Training Acc: 100.0 | Testing Loss: 0.0030233326833695173 | Testing Acc: 100.0\n",
            "Epoch: 1804 | Training Loss: 0.0018484173342585564 | Training Acc: 100.0 | Testing Loss: 0.0030234430450946093 | Testing Acc: 100.0\n",
            "Epoch: 1805 | Training Loss: 0.0018469669157639146 | Training Acc: 100.0 | Testing Loss: 0.003020514966920018 | Testing Acc: 100.0\n",
            "Epoch: 1806 | Training Loss: 0.0018455848330631852 | Training Acc: 100.0 | Testing Loss: 0.0030205524526536465 | Testing Acc: 100.0\n",
            "Epoch: 1807 | Training Loss: 0.0018441596766933799 | Training Acc: 100.0 | Testing Loss: 0.0030175168067216873 | Testing Acc: 100.0\n",
            "Epoch: 1808 | Training Loss: 0.0018427487229928374 | Training Acc: 100.0 | Testing Loss: 0.003017526352778077 | Testing Acc: 100.0\n",
            "Epoch: 1809 | Training Loss: 0.0018413581419736147 | Training Acc: 100.0 | Testing Loss: 0.0030144513584673405 | Testing Acc: 100.0\n",
            "Epoch: 1810 | Training Loss: 0.0018399201799184084 | Training Acc: 100.0 | Testing Loss: 0.0030144888442009687 | Testing Acc: 100.0\n",
            "Epoch: 1811 | Training Loss: 0.001838569762185216 | Training Acc: 100.0 | Testing Loss: 0.0030113854445517063 | Testing Acc: 100.0\n",
            "Epoch: 1812 | Training Loss: 0.001837120158597827 | Training Acc: 100.0 | Testing Loss: 0.003008562605828047 | Testing Acc: 100.0\n",
            "Epoch: 1813 | Training Loss: 0.0018357725348323584 | Training Acc: 100.0 | Testing Loss: 0.0030086853075772524 | Testing Acc: 100.0\n",
            "Epoch: 1814 | Training Loss: 0.0018343431875109673 | Training Acc: 100.0 | Testing Loss: 0.003005744656547904 | Testing Acc: 100.0\n",
            "Epoch: 1815 | Training Loss: 0.00183296506293118 | Training Acc: 100.0 | Testing Loss: 0.0030057933181524277 | Testing Acc: 100.0\n",
            "Epoch: 1816 | Training Loss: 0.0018315732013434172 | Training Acc: 100.0 | Testing Loss: 0.0030027967877686024 | Testing Acc: 100.0\n",
            "Epoch: 1817 | Training Loss: 0.0018301664385944605 | Training Acc: 100.0 | Testing Loss: 0.003002806566655636 | Testing Acc: 100.0\n",
            "Epoch: 1818 | Training Loss: 0.0018288043793290854 | Training Acc: 100.0 | Testing Loss: 0.0029997529927641153 | Testing Acc: 100.0\n",
            "Epoch: 1819 | Training Loss: 0.0018273722380399704 | Training Acc: 100.0 | Testing Loss: 0.0029997345991432667 | Testing Acc: 100.0\n",
            "Epoch: 1820 | Training Loss: 0.0018260336946696043 | Training Acc: 100.0 | Testing Loss: 0.0029966640286147594 | Testing Acc: 100.0\n",
            "Epoch: 1821 | Training Loss: 0.0018245987594127655 | Training Acc: 100.0 | Testing Loss: 0.0029939082451164722 | Testing Acc: 100.0\n",
            "Epoch: 1822 | Training Loss: 0.0018232561415061355 | Training Acc: 100.0 | Testing Loss: 0.0029940411914139986 | Testing Acc: 100.0\n",
            "Epoch: 1823 | Training Loss: 0.0018218557815998793 | Training Acc: 100.0 | Testing Loss: 0.0029911049641668797 | Testing Acc: 100.0\n",
            "Epoch: 1824 | Training Loss: 0.0018204847583547235 | Training Acc: 100.0 | Testing Loss: 0.0029911710880696774 | Testing Acc: 100.0\n",
            "Epoch: 1825 | Training Loss: 0.0018191200215369463 | Training Acc: 100.0 | Testing Loss: 0.002988207619637251 | Testing Acc: 100.0\n",
            "Epoch: 1826 | Training Loss: 0.0018177175661548972 | Training Acc: 100.0 | Testing Loss: 0.0029881999362260103 | Testing Acc: 100.0\n",
            "Epoch: 1827 | Training Loss: 0.0018163822824135423 | Training Acc: 100.0 | Testing Loss: 0.0029851733706891537 | Testing Acc: 100.0\n",
            "Epoch: 1828 | Training Loss: 0.0018149558454751968 | Training Acc: 100.0 | Testing Loss: 0.0029851163271814585 | Testing Acc: 100.0\n",
            "Epoch: 1829 | Training Loss: 0.0018136423313990235 | Training Acc: 100.0 | Testing Loss: 0.002982094883918762 | Testing Acc: 100.0\n",
            "Epoch: 1830 | Training Loss: 0.001812228001654148 | Training Acc: 100.0 | Testing Loss: 0.0029793106950819492 | Testing Acc: 100.0\n",
            "Epoch: 1831 | Training Loss: 0.0018108945805579424 | Training Acc: 100.0 | Testing Loss: 0.0029795283917337656 | Testing Acc: 100.0\n",
            "Epoch: 1832 | Training Loss: 0.0018095135455951095 | Training Acc: 100.0 | Testing Loss: 0.0029766543302685022 | Testing Acc: 100.0\n",
            "Epoch: 1833 | Training Loss: 0.001808146946132183 | Training Acc: 100.0 | Testing Loss: 0.0029766755178570747 | Testing Acc: 100.0\n",
            "Epoch: 1834 | Training Loss: 0.0018068067729473114 | Training Acc: 100.0 | Testing Loss: 0.002973743248730898 | Testing Acc: 100.0\n",
            "Epoch: 1835 | Training Loss: 0.0018054153770208359 | Training Acc: 100.0 | Testing Loss: 0.002973731141537428 | Testing Acc: 100.0\n",
            "Epoch: 1836 | Training Loss: 0.001804105006158352 | Training Acc: 100.0 | Testing Loss: 0.002970766043290496 | Testing Acc: 100.0\n",
            "Epoch: 1837 | Training Loss: 0.0018026947509497404 | Training Acc: 100.0 | Testing Loss: 0.00296802562661469 | Testing Acc: 100.0\n",
            "Epoch: 1838 | Training Loss: 0.00180139415897429 | Training Acc: 100.0 | Testing Loss: 0.002968181623145938 | Testing Acc: 100.0\n",
            "Epoch: 1839 | Training Loss: 0.0017999985720962286 | Training Acc: 100.0 | Testing Loss: 0.0029653399251401424 | Testing Acc: 100.0\n",
            "Epoch: 1840 | Training Loss: 0.0017986635211855173 | Training Acc: 100.0 | Testing Loss: 0.002965383231639862 | Testing Acc: 100.0\n",
            "Epoch: 1841 | Training Loss: 0.0017973205540329218 | Training Acc: 100.0 | Testing Loss: 0.002962507540360093 | Testing Acc: 100.0\n",
            "Epoch: 1842 | Training Loss: 0.001795957563444972 | Training Acc: 100.0 | Testing Loss: 0.0029625683091580868 | Testing Acc: 100.0\n",
            "Epoch: 1843 | Training Loss: 0.0017946327570825815 | Training Acc: 100.0 | Testing Loss: 0.0029596304520964622 | Testing Acc: 100.0\n",
            "Epoch: 1844 | Training Loss: 0.0017932543996721506 | Training Acc: 100.0 | Testing Loss: 0.0029595617670565844 | Testing Acc: 100.0\n",
            "Epoch: 1845 | Training Loss: 0.001791955204680562 | Training Acc: 100.0 | Testing Loss: 0.0029566071461886168 | Testing Acc: 100.0\n",
            "Epoch: 1846 | Training Loss: 0.001790579641237855 | Training Acc: 100.0 | Testing Loss: 0.0029538602102547884 | Testing Acc: 100.0\n",
            "Epoch: 1847 | Training Loss: 0.0017892714822664857 | Training Acc: 100.0 | Testing Loss: 0.0029539712704718113 | Testing Acc: 100.0\n",
            "Epoch: 1848 | Training Loss: 0.001787912449799478 | Training Acc: 100.0 | Testing Loss: 0.0029511568136513233 | Testing Acc: 100.0\n",
            "Epoch: 1849 | Training Loss: 0.0017865824047476053 | Training Acc: 100.0 | Testing Loss: 0.002951171714812517 | Testing Acc: 100.0\n",
            "Epoch: 1850 | Training Loss: 0.001785253407433629 | Training Acc: 100.0 | Testing Loss: 0.002948323730379343 | Testing Acc: 100.0\n",
            "Epoch: 1851 | Training Loss: 0.0017839139327406883 | Training Acc: 100.0 | Testing Loss: 0.0029482885729521513 | Testing Acc: 100.0\n",
            "Epoch: 1852 | Training Loss: 0.0017825986724346876 | Training Acc: 100.0 | Testing Loss: 0.0029453779570758343 | Testing Acc: 100.0\n",
            "Epoch: 1853 | Training Loss: 0.0017812388250604272 | Training Acc: 100.0 | Testing Loss: 0.002945388201624155 | Testing Acc: 100.0\n",
            "Epoch: 1854 | Training Loss: 0.0017799537163227797 | Training Acc: 100.0 | Testing Loss: 0.0029424275271594524 | Testing Acc: 100.0\n",
            "Epoch: 1855 | Training Loss: 0.0017785871168598533 | Training Acc: 100.0 | Testing Loss: 0.002939742524176836 | Testing Acc: 100.0\n",
            "Epoch: 1856 | Training Loss: 0.0017772915307432413 | Training Acc: 100.0 | Testing Loss: 0.002939836587756872 | Testing Acc: 100.0\n",
            "Epoch: 1857 | Training Loss: 0.0017759508918970823 | Training Acc: 100.0 | Testing Loss: 0.002937066601589322 | Testing Acc: 100.0\n",
            "Epoch: 1858 | Training Loss: 0.001774635398760438 | Training Acc: 100.0 | Testing Loss: 0.002937076613306999 | Testing Acc: 100.0\n",
            "Epoch: 1859 | Training Loss: 0.0017733217682689428 | Training Acc: 100.0 | Testing Loss: 0.0029342439956963062 | Testing Acc: 100.0\n",
            "Epoch: 1860 | Training Loss: 0.00177199044264853 | Training Acc: 100.0 | Testing Loss: 0.002934208605438471 | Testing Acc: 100.0\n",
            "Epoch: 1861 | Training Loss: 0.0017706953221932054 | Training Acc: 100.0 | Testing Loss: 0.0029313210397958755 | Testing Acc: 100.0\n",
            "Epoch: 1862 | Training Loss: 0.0017693417612463236 | Training Acc: 100.0 | Testing Loss: 0.0029286686331033707 | Testing Acc: 100.0\n",
            "Epoch: 1863 | Training Loss: 0.0017680758610367775 | Training Acc: 100.0 | Testing Loss: 0.002928864909335971 | Testing Acc: 100.0\n",
            "Epoch: 1864 | Training Loss: 0.0017667250940576196 | Training Acc: 100.0 | Testing Loss: 0.0029261279851198196 | Testing Acc: 100.0\n",
            "Epoch: 1865 | Training Loss: 0.001765445456840098 | Training Acc: 100.0 | Testing Loss: 0.0029261368326842785 | Testing Acc: 100.0\n",
            "Epoch: 1866 | Training Loss: 0.0017641218146309257 | Training Acc: 100.0 | Testing Loss: 0.0029233326204121113 | Testing Acc: 100.0\n",
            "Epoch: 1867 | Training Loss: 0.001762819243595004 | Training Acc: 100.0 | Testing Loss: 0.0029232914093881845 | Testing Acc: 100.0\n",
            "Epoch: 1868 | Training Loss: 0.0017615169053897262 | Training Acc: 100.0 | Testing Loss: 0.002920442493632436 | Testing Acc: 100.0\n",
            "Epoch: 1869 | Training Loss: 0.001760190585628152 | Training Acc: 100.0 | Testing Loss: 0.0029204010497778654 | Testing Acc: 100.0\n",
            "Epoch: 1870 | Training Loss: 0.001758925965987146 | Training Acc: 100.0 | Testing Loss: 0.0029175125528126955 | Testing Acc: 100.0\n",
            "Epoch: 1871 | Training Loss: 0.001757583930157125 | Training Acc: 100.0 | Testing Loss: 0.0029148587491363287 | Testing Acc: 100.0\n",
            "Epoch: 1872 | Training Loss: 0.0017563371220603585 | Training Acc: 100.0 | Testing Loss: 0.002914964687079191 | Testing Acc: 100.0\n",
            "Epoch: 1873 | Training Loss: 0.0017550078919157386 | Training Acc: 100.0 | Testing Loss: 0.002912210300564766 | Testing Acc: 100.0\n",
            "Epoch: 1874 | Training Loss: 0.0017537313979119062 | Training Acc: 100.0 | Testing Loss: 0.0029123499989509583 | Testing Acc: 100.0\n",
            "Epoch: 1875 | Training Loss: 0.0017524298746138811 | Training Acc: 100.0 | Testing Loss: 0.0029095220379531384 | Testing Acc: 100.0\n",
            "Epoch: 1876 | Training Loss: 0.0017511218320578337 | Training Acc: 100.0 | Testing Loss: 0.0029095092322677374 | Testing Acc: 100.0\n",
            "Epoch: 1877 | Training Loss: 0.0017498604720458388 | Training Acc: 100.0 | Testing Loss: 0.002906659385189414 | Testing Acc: 100.0\n",
            "Epoch: 1878 | Training Loss: 0.0017485326388850808 | Training Acc: 100.0 | Testing Loss: 0.0029066128190606833 | Testing Acc: 100.0\n",
            "Epoch: 1879 | Training Loss: 0.0017472952604293823 | Training Acc: 100.0 | Testing Loss: 0.0029037287458777428 | Testing Acc: 100.0\n",
            "Epoch: 1880 | Training Loss: 0.0017459677765145898 | Training Acc: 100.0 | Testing Loss: 0.002901074942201376 | Testing Acc: 100.0\n",
            "Epoch: 1881 | Training Loss: 0.0017447180580347776 | Training Acc: 100.0 | Testing Loss: 0.0029011971782892942 | Testing Acc: 100.0\n",
            "Epoch: 1882 | Training Loss: 0.0017434138571843505 | Training Acc: 100.0 | Testing Loss: 0.002898475620895624 | Testing Acc: 100.0\n",
            "Epoch: 1883 | Training Loss: 0.001742138876579702 | Training Acc: 100.0 | Testing Loss: 0.0028985135722905397 | Testing Acc: 100.0\n",
            "Epoch: 1884 | Training Loss: 0.0017408585408702493 | Training Acc: 100.0 | Testing Loss: 0.0028957307804375887 | Testing Acc: 100.0\n",
            "Epoch: 1885 | Training Loss: 0.0017395683098584414 | Training Acc: 100.0 | Testing Loss: 0.0028957691974937916 | Testing Acc: 100.0\n",
            "Epoch: 1886 | Training Loss: 0.0017383325612172484 | Training Acc: 100.0 | Testing Loss: 0.002892968710511923 | Testing Acc: 100.0\n",
            "Epoch: 1887 | Training Loss: 0.0017370072891935706 | Training Acc: 100.0 | Testing Loss: 0.0028903712518513203 | Testing Acc: 100.0\n",
            "Epoch: 1888 | Training Loss: 0.0017357822507619858 | Training Acc: 100.0 | Testing Loss: 0.00289049930870533 | Testing Acc: 100.0\n",
            "Epoch: 1889 | Training Loss: 0.0017344765365123749 | Training Acc: 100.0 | Testing Loss: 0.002887793816626072 | Testing Acc: 100.0\n",
            "Epoch: 1890 | Training Loss: 0.0017332287970930338 | Training Acc: 100.0 | Testing Loss: 0.0028878487646579742 | Testing Acc: 100.0\n",
            "Epoch: 1891 | Training Loss: 0.001731948577798903 | Training Acc: 100.0 | Testing Loss: 0.0028850992675870657 | Testing Acc: 100.0\n",
            "Epoch: 1892 | Training Loss: 0.0017306808149442077 | Training Acc: 100.0 | Testing Loss: 0.0028850750532001257 | Testing Acc: 100.0\n",
            "Epoch: 1893 | Training Loss: 0.0017294309800490737 | Training Acc: 100.0 | Testing Loss: 0.002882330911234021 | Testing Acc: 100.0\n",
            "Epoch: 1894 | Training Loss: 0.0017281444743275642 | Training Acc: 100.0 | Testing Loss: 0.002882244298234582 | Testing Acc: 100.0\n",
            "Epoch: 1895 | Training Loss: 0.0017269158270210028 | Training Acc: 100.0 | Testing Loss: 0.002879460109397769 | Testing Acc: 100.0\n",
            "Epoch: 1896 | Training Loss: 0.001725611975416541 | Training Acc: 100.0 | Testing Loss: 0.0028769182972609997 | Testing Acc: 100.0\n",
            "Epoch: 1897 | Training Loss: 0.0017243999755010009 | Training Acc: 100.0 | Testing Loss: 0.0028769955970346928 | Testing Acc: 100.0\n",
            "Epoch: 1898 | Training Loss: 0.0017231159145012498 | Training Acc: 100.0 | Testing Loss: 0.0028743299189954996 | Testing Acc: 100.0\n",
            "Epoch: 1899 | Training Loss: 0.0017218764405697584 | Training Acc: 100.0 | Testing Loss: 0.00287435669451952 | Testing Acc: 100.0\n",
            "Epoch: 1900 | Training Loss: 0.0017206132179126143 | Training Acc: 100.0 | Testing Loss: 0.002871633507311344 | Testing Acc: 100.0\n",
            "Epoch: 1901 | Training Loss: 0.0017193525563925505 | Training Acc: 100.0 | Testing Loss: 0.0028715988155454397 | Testing Acc: 100.0\n",
            "Epoch: 1902 | Training Loss: 0.0017181182047352195 | Training Acc: 100.0 | Testing Loss: 0.0028688532765954733 | Testing Acc: 100.0\n",
            "Epoch: 1903 | Training Loss: 0.0017168388003483415 | Training Acc: 100.0 | Testing Loss: 0.002868767362087965 | Testing Acc: 100.0\n",
            "Epoch: 1904 | Training Loss: 0.0017156300600618124 | Training Acc: 100.0 | Testing Loss: 0.002865999471396208 | Testing Acc: 100.0\n",
            "Epoch: 1905 | Training Loss: 0.0017143537988886237 | Training Acc: 100.0 | Testing Loss: 0.0028634232003241777 | Testing Acc: 100.0\n",
            "Epoch: 1906 | Training Loss: 0.0017131405184045434 | Training Acc: 100.0 | Testing Loss: 0.0028635687194764614 | Testing Acc: 100.0\n",
            "Epoch: 1907 | Training Loss: 0.0017118729883804917 | Training Acc: 100.0 | Testing Loss: 0.0028609358705580235 | Testing Acc: 100.0\n",
            "Epoch: 1908 | Training Loss: 0.0017106377054005861 | Training Acc: 100.0 | Testing Loss: 0.002860939595848322 | Testing Acc: 100.0\n",
            "Epoch: 1909 | Training Loss: 0.0017093917122110724 | Training Acc: 100.0 | Testing Loss: 0.0028582559898495674 | Testing Acc: 100.0\n",
            "Epoch: 1910 | Training Loss: 0.001708145486190915 | Training Acc: 100.0 | Testing Loss: 0.0028582154773175716 | Testing Acc: 100.0\n",
            "Epoch: 1911 | Training Loss: 0.0017069252207875252 | Training Acc: 100.0 | Testing Loss: 0.002855457831174135 | Testing Acc: 100.0\n",
            "Epoch: 1912 | Training Loss: 0.0017056561773642898 | Training Acc: 100.0 | Testing Loss: 0.0028554112650454044 | Testing Acc: 100.0\n",
            "Epoch: 1913 | Training Loss: 0.001704464084468782 | Training Acc: 100.0 | Testing Loss: 0.0028526373207569122 | Testing Acc: 100.0\n",
            "Epoch: 1914 | Training Loss: 0.0017031922470778227 | Training Acc: 100.0 | Testing Loss: 0.00285009341314435 | Testing Acc: 100.0\n",
            "Epoch: 1915 | Training Loss: 0.001701991306617856 | Training Acc: 100.0 | Testing Loss: 0.002850256161764264 | Testing Acc: 100.0\n",
            "Epoch: 1916 | Training Loss: 0.0017007424030452967 | Training Acc: 100.0 | Testing Loss: 0.0028476170264184475 | Testing Acc: 100.0\n",
            "Epoch: 1917 | Training Loss: 0.001699525280855596 | Training Acc: 100.0 | Testing Loss: 0.0028476258739829063 | Testing Acc: 100.0\n",
            "Epoch: 1918 | Training Loss: 0.0016983002424240112 | Training Acc: 100.0 | Testing Loss: 0.0028449473902583122 | Testing Acc: 100.0\n",
            "Epoch: 1919 | Training Loss: 0.0016970541328191757 | Training Acc: 100.0 | Testing Loss: 0.0028449404053390026 | Testing Acc: 100.0\n",
            "Epoch: 1920 | Training Loss: 0.0016958710039034486 | Training Acc: 100.0 | Testing Loss: 0.0028421992901712656 | Testing Acc: 100.0\n",
            "Epoch: 1921 | Training Loss: 0.0016946003306657076 | Training Acc: 100.0 | Testing Loss: 0.0028397063724696636 | Testing Acc: 100.0\n",
            "Epoch: 1922 | Training Loss: 0.0016934250015765429 | Training Acc: 100.0 | Testing Loss: 0.002839800901710987 | Testing Acc: 100.0\n",
            "Epoch: 1923 | Training Loss: 0.0016921708593145013 | Training Acc: 100.0 | Testing Loss: 0.0028371945954859257 | Testing Acc: 100.0\n",
            "Epoch: 1924 | Training Loss: 0.0016909732948988676 | Training Acc: 100.0 | Testing Loss: 0.0028372376691550016 | Testing Acc: 100.0\n",
            "Epoch: 1925 | Training Loss: 0.0016897441819310188 | Training Acc: 100.0 | Testing Loss: 0.0028345929458737373 | Testing Acc: 100.0\n",
            "Epoch: 1926 | Training Loss: 0.0016885253135114908 | Training Acc: 100.0 | Testing Loss: 0.002834653714671731 | Testing Acc: 100.0\n",
            "Epoch: 1927 | Training Loss: 0.0016873283311724663 | Training Acc: 100.0 | Testing Loss: 0.0028319519478827715 | Testing Acc: 100.0\n",
            "Epoch: 1928 | Training Loss: 0.0016860900213941932 | Training Acc: 100.0 | Testing Loss: 0.0028318886179476976 | Testing Acc: 100.0\n",
            "Epoch: 1929 | Training Loss: 0.0016849280800670385 | Training Acc: 100.0 | Testing Loss: 0.00282919779419899 | Testing Acc: 100.0\n",
            "Epoch: 1930 | Training Loss: 0.0016836731228977442 | Training Acc: 100.0 | Testing Loss: 0.0028266701847314835 | Testing Acc: 100.0\n",
            "Epoch: 1931 | Training Loss: 0.001682494068518281 | Training Acc: 100.0 | Testing Loss: 0.00282678147777915 | Testing Acc: 100.0\n",
            "Epoch: 1932 | Training Loss: 0.001681270427070558 | Training Acc: 100.0 | Testing Loss: 0.0028241919353604317 | Testing Acc: 100.0\n",
            "Epoch: 1933 | Training Loss: 0.0016800735611468554 | Training Acc: 100.0 | Testing Loss: 0.0028241905383765697 | Testing Acc: 100.0\n",
            "Epoch: 1934 | Training Loss: 0.0016788741340860724 | Training Acc: 100.0 | Testing Loss: 0.002821583766490221 | Testing Acc: 100.0\n",
            "Epoch: 1935 | Training Loss: 0.0016776521224528551 | Training Acc: 100.0 | Testing Loss: 0.0028215544298291206 | Testing Acc: 100.0\n",
            "Epoch: 1936 | Training Loss: 0.0016764833126217127 | Training Acc: 100.0 | Testing Loss: 0.0028189080767333508 | Testing Acc: 100.0\n",
            "Epoch: 1937 | Training Loss: 0.001675244071520865 | Training Acc: 100.0 | Testing Loss: 0.002818890381604433 | Testing Acc: 100.0\n",
            "Epoch: 1938 | Training Loss: 0.0016740888822823763 | Training Acc: 100.0 | Testing Loss: 0.0028162277303636074 | Testing Acc: 100.0\n",
            "Epoch: 1939 | Training Loss: 0.0016728626796975732 | Training Acc: 100.0 | Testing Loss: 0.0028137220069766045 | Testing Acc: 100.0\n",
            "Epoch: 1940 | Training Loss: 0.00167168234474957 | Training Acc: 100.0 | Testing Loss: 0.002813782310113311 | Testing Acc: 100.0\n",
            "Epoch: 1941 | Training Loss: 0.0016704790759831667 | Training Acc: 100.0 | Testing Loss: 0.0028112311847507954 | Testing Acc: 100.0\n",
            "Epoch: 1942 | Training Loss: 0.0016692945500835776 | Training Acc: 100.0 | Testing Loss: 0.002811247482895851 | Testing Acc: 100.0\n",
            "Epoch: 1943 | Training Loss: 0.0016681080451235175 | Training Acc: 100.0 | Testing Loss: 0.002808657009154558 | Testing Acc: 100.0\n",
            "Epoch: 1944 | Training Loss: 0.0016668944153934717 | Training Acc: 100.0 | Testing Loss: 0.0028086218517273664 | Testing Acc: 100.0\n",
            "Epoch: 1945 | Training Loss: 0.0016657353844493628 | Training Acc: 100.0 | Testing Loss: 0.0028059803880751133 | Testing Acc: 100.0\n",
            "Epoch: 1946 | Training Loss: 0.001664507552050054 | Training Acc: 100.0 | Testing Loss: 0.0028035251889377832 | Testing Acc: 100.0\n",
            "Epoch: 1947 | Training Loss: 0.001663372153416276 | Training Acc: 100.0 | Testing Loss: 0.002803670009598136 | Testing Acc: 100.0\n",
            "Epoch: 1948 | Training Loss: 0.0016621642280369997 | Training Acc: 100.0 | Testing Loss: 0.002801164286211133 | Testing Acc: 100.0\n",
            "Epoch: 1949 | Training Loss: 0.001660990878008306 | Training Acc: 100.0 | Testing Loss: 0.002801168244332075 | Testing Acc: 100.0\n",
            "Epoch: 1950 | Training Loss: 0.0016598061192780733 | Training Acc: 100.0 | Testing Loss: 0.0027985938359051943 | Testing Acc: 100.0\n",
            "Epoch: 1951 | Training Loss: 0.0016586227575317025 | Training Acc: 100.0 | Testing Loss: 0.0027985707856714725 | Testing Acc: 100.0\n",
            "Epoch: 1952 | Training Loss: 0.0016574548790231347 | Training Acc: 100.0 | Testing Loss: 0.0027959400322288275 | Testing Acc: 100.0\n",
            "Epoch: 1953 | Training Loss: 0.0016562577802687883 | Training Acc: 100.0 | Testing Loss: 0.0027959225699305534 | Testing Acc: 100.0\n",
            "Epoch: 1954 | Training Loss: 0.0016551197040826082 | Training Acc: 100.0 | Testing Loss: 0.0027932636439800262 | Testing Acc: 100.0\n",
            "Epoch: 1955 | Training Loss: 0.0016539066564291716 | Training Acc: 100.0 | Testing Loss: 0.0027908359188586473 | Testing Acc: 100.0\n",
            "Epoch: 1956 | Training Loss: 0.0016527778934687376 | Training Acc: 100.0 | Testing Loss: 0.0027909076306968927 | Testing Acc: 100.0\n",
            "Epoch: 1957 | Training Loss: 0.0016515773022547364 | Training Acc: 100.0 | Testing Loss: 0.0027883839793503284 | Testing Acc: 100.0\n",
            "Epoch: 1958 | Training Loss: 0.0016504157101735473 | Training Acc: 100.0 | Testing Loss: 0.0027884847950190306 | Testing Acc: 100.0\n",
            "Epoch: 1959 | Training Loss: 0.0016492533031851053 | Training Acc: 100.0 | Testing Loss: 0.0027859045658260584 | Testing Acc: 100.0\n",
            "Epoch: 1960 | Training Loss: 0.001648062840104103 | Training Acc: 100.0 | Testing Loss: 0.002785886637866497 | Testing Acc: 100.0\n",
            "Epoch: 1961 | Training Loss: 0.001646926044486463 | Training Acc: 100.0 | Testing Loss: 0.0027832782361656427 | Testing Acc: 100.0\n",
            "Epoch: 1962 | Training Loss: 0.0016457248711958528 | Training Acc: 100.0 | Testing Loss: 0.0027808556333184242 | Testing Acc: 100.0\n",
            "Epoch: 1963 | Training Loss: 0.0016446104273200035 | Training Acc: 100.0 | Testing Loss: 0.0027809676248580217 | Testing Acc: 100.0\n",
            "Epoch: 1964 | Training Loss: 0.0016434213612228632 | Training Acc: 100.0 | Testing Loss: 0.0027784772682935 | Testing Acc: 100.0\n",
            "Epoch: 1965 | Training Loss: 0.0016422842163592577 | Training Acc: 100.0 | Testing Loss: 0.002778532449156046 | Testing Acc: 100.0\n",
            "Epoch: 1966 | Training Loss: 0.0016411163378506899 | Training Acc: 100.0 | Testing Loss: 0.002775974338874221 | Testing Acc: 100.0\n",
            "Epoch: 1967 | Training Loss: 0.0016399577725678682 | Training Acc: 100.0 | Testing Loss: 0.0027759335935115814 | Testing Acc: 100.0\n",
            "Epoch: 1968 | Training Loss: 0.0016388166695833206 | Training Acc: 100.0 | Testing Loss: 0.00277335150167346 | Testing Acc: 100.0\n",
            "Epoch: 1969 | Training Loss: 0.001637635170482099 | Training Acc: 100.0 | Testing Loss: 0.0027733624447137117 | Testing Acc: 100.0\n",
            "Epoch: 1970 | Training Loss: 0.0016365181654691696 | Training Acc: 100.0 | Testing Loss: 0.0027707875706255436 | Testing Acc: 100.0\n",
            "Epoch: 1971 | Training Loss: 0.001635327935218811 | Training Acc: 100.0 | Testing Loss: 0.002768369857221842 | Testing Acc: 100.0\n",
            "Epoch: 1972 | Training Loss: 0.001634201966226101 | Training Acc: 100.0 | Testing Loss: 0.002768459264189005 | Testing Acc: 100.0\n",
            "Epoch: 1973 | Training Loss: 0.001633038162253797 | Training Acc: 100.0 | Testing Loss: 0.0027659679763019085 | Testing Acc: 100.0\n",
            "Epoch: 1974 | Training Loss: 0.0016318925190716982 | Training Acc: 100.0 | Testing Loss: 0.0027659726329147816 | Testing Acc: 100.0\n",
            "Epoch: 1975 | Training Loss: 0.0016307530459016562 | Training Acc: 100.0 | Testing Loss: 0.0027634704019874334 | Testing Acc: 100.0\n",
            "Epoch: 1976 | Training Loss: 0.001629597507417202 | Training Acc: 100.0 | Testing Loss: 0.0027634187135845423 | Testing Acc: 100.0\n",
            "Epoch: 1977 | Training Loss: 0.0016284693265333772 | Training Acc: 100.0 | Testing Loss: 0.0027608703821897507 | Testing Acc: 100.0\n",
            "Epoch: 1978 | Training Loss: 0.0016273049404844642 | Training Acc: 100.0 | Testing Loss: 0.0027608638629317284 | Testing Acc: 100.0\n",
            "Epoch: 1979 | Training Loss: 0.0016262041172012687 | Training Acc: 100.0 | Testing Loss: 0.0027582834009081125 | Testing Acc: 100.0\n",
            "Epoch: 1980 | Training Loss: 0.001625029370188713 | Training Acc: 100.0 | Testing Loss: 0.0027559048030525446 | Testing Acc: 100.0\n",
            "Epoch: 1981 | Training Loss: 0.0016239180695265532 | Training Acc: 100.0 | Testing Loss: 0.002755953697487712 | Testing Acc: 100.0\n",
            "Epoch: 1982 | Training Loss: 0.0016227628802880645 | Training Acc: 100.0 | Testing Loss: 0.002753507113084197 | Testing Acc: 100.0\n",
            "Epoch: 1983 | Training Loss: 0.0016216386575251818 | Training Acc: 100.0 | Testing Loss: 0.0027535180561244488 | Testing Acc: 100.0\n",
            "Epoch: 1984 | Training Loss: 0.0016204960411414504 | Training Acc: 100.0 | Testing Loss: 0.0027509978972375393 | Testing Acc: 100.0\n",
            "Epoch: 1985 | Training Loss: 0.0016193505143746734 | Training Acc: 100.0 | Testing Loss: 0.0027509857900440693 | Testing Acc: 100.0\n",
            "Epoch: 1986 | Training Loss: 0.0016182452673092484 | Training Acc: 100.0 | Testing Loss: 0.0027484376914799213 | Testing Acc: 100.0\n",
            "Epoch: 1987 | Training Loss: 0.0016170823946595192 | Training Acc: 100.0 | Testing Loss: 0.0027461096178740263 | Testing Acc: 100.0\n",
            "Epoch: 1988 | Training Loss: 0.001615996821783483 | Training Acc: 100.0 | Testing Loss: 0.002746255835518241 | Testing Acc: 100.0\n",
            "Epoch: 1989 | Training Loss: 0.0016148325521498919 | Training Acc: 100.0 | Testing Loss: 0.0027438371907919645 | Testing Acc: 100.0\n",
            "Epoch: 1990 | Training Loss: 0.0016137358034029603 | Training Acc: 100.0 | Testing Loss: 0.0027438418474048376 | Testing Acc: 100.0\n",
            "Epoch: 1991 | Training Loss: 0.001612593070603907 | Training Acc: 100.0 | Testing Loss: 0.0027413666248321533 | Testing Acc: 100.0\n",
            "Epoch: 1992 | Training Loss: 0.0016114661702886224 | Training Acc: 100.0 | Testing Loss: 0.002741365460678935 | Testing Acc: 100.0\n",
            "Epoch: 1993 | Training Loss: 0.0016103594098240137 | Training Acc: 100.0 | Testing Loss: 0.0027388392481952906 | Testing Acc: 100.0\n",
            "Epoch: 1994 | Training Loss: 0.0016092124860733747 | Training Acc: 100.0 | Testing Loss: 0.0027388271410018206 | Testing Acc: 100.0\n",
            "Epoch: 1995 | Training Loss: 0.0016081280773505569 | Training Acc: 100.0 | Testing Loss: 0.0027362839318811893 | Testing Acc: 100.0\n",
            "Epoch: 1996 | Training Loss: 0.0016069728881120682 | Training Acc: 100.0 | Testing Loss: 0.002733938628807664 | Testing Acc: 100.0\n",
            "Epoch: 1997 | Training Loss: 0.0016058903420343995 | Training Acc: 100.0 | Testing Loss: 0.0027340275701135397 | Testing Acc: 100.0\n",
            "Epoch: 1998 | Training Loss: 0.0016047607641667128 | Training Acc: 100.0 | Testing Loss: 0.0027316201012581587 | Testing Acc: 100.0\n",
            "Epoch: 1999 | Training Loss: 0.0016036552842706442 | Training Acc: 100.0 | Testing Loss: 0.002731726737692952 | Testing Acc: 100.0\n",
            "Epoch: 2000 | Training Loss: 0.0016025451477617025 | Training Acc: 100.0 | Testing Loss: 0.002729239873588085 | Testing Acc: 100.0\n",
            "Epoch: 2001 | Training Loss: 0.0016014082357287407 | Training Acc: 100.0 | Testing Loss: 0.002729204948991537 | Testing Acc: 100.0\n",
            "Epoch: 2002 | Training Loss: 0.0016003359341993928 | Training Acc: 100.0 | Testing Loss: 0.0027267178520560265 | Testing Acc: 100.0\n",
            "Epoch: 2003 | Training Loss: 0.0015991872642189264 | Training Acc: 100.0 | Testing Loss: 0.0027244056109339 | Testing Acc: 100.0\n",
            "Epoch: 2004 | Training Loss: 0.0015981204342097044 | Training Acc: 100.0 | Testing Loss: 0.002724506426602602 | Testing Acc: 100.0\n",
            "Epoch: 2005 | Training Loss: 0.0015969894593581557 | Training Acc: 100.0 | Testing Loss: 0.0027221092022955418 | Testing Acc: 100.0\n",
            "Epoch: 2006 | Training Loss: 0.0015958998119458556 | Training Acc: 100.0 | Testing Loss: 0.0027221194468438625 | Testing Acc: 100.0\n",
            "Epoch: 2007 | Training Loss: 0.0015947872307151556 | Training Acc: 100.0 | Testing Loss: 0.002719699637964368 | Testing Acc: 100.0\n",
            "Epoch: 2008 | Training Loss: 0.001593681750819087 | Training Acc: 100.0 | Testing Loss: 0.0027196763549000025 | Testing Acc: 100.0\n",
            "Epoch: 2009 | Training Loss: 0.0015925935003906488 | Training Acc: 100.0 | Testing Loss: 0.0027172169648110867 | Testing Acc: 100.0\n",
            "Epoch: 2010 | Training Loss: 0.001591469394043088 | Training Acc: 100.0 | Testing Loss: 0.0027172095142304897 | Testing Acc: 100.0\n",
            "Epoch: 2011 | Training Loss: 0.0015903994208201766 | Training Acc: 100.0 | Testing Loss: 0.0027146998327225447 | Testing Acc: 100.0\n",
            "Epoch: 2012 | Training Loss: 0.0015892708906903863 | Training Acc: 100.0 | Testing Loss: 0.0027124162297695875 | Testing Acc: 100.0\n",
            "Epoch: 2013 | Training Loss: 0.0015882101142778993 | Training Acc: 100.0 | Testing Loss: 0.00271247117780149 | Testing Acc: 100.0\n",
            "Epoch: 2014 | Training Loss: 0.0015870872884988785 | Training Acc: 100.0 | Testing Loss: 0.0027101137675344944 | Testing Acc: 100.0\n",
            "Epoch: 2015 | Training Loss: 0.0015860076528042555 | Training Acc: 100.0 | Testing Loss: 0.00271011283621192 | Testing Acc: 100.0\n",
            "Epoch: 2016 | Training Loss: 0.0015849110204726458 | Training Acc: 100.0 | Testing Loss: 0.0027076699770987034 | Testing Acc: 100.0\n",
            "Epoch: 2017 | Training Loss: 0.0015838125254958868 | Training Acc: 100.0 | Testing Loss: 0.002707663457840681 | Testing Acc: 100.0\n",
            "Epoch: 2018 | Training Loss: 0.001582734053954482 | Training Acc: 100.0 | Testing Loss: 0.0027051980141550303 | Testing Acc: 100.0\n",
            "Epoch: 2019 | Training Loss: 0.0015816173981875181 | Training Acc: 100.0 | Testing Loss: 0.0027051575016230345 | Testing Acc: 100.0\n",
            "Epoch: 2020 | Training Loss: 0.001580574898980558 | Training Acc: 100.0 | Testing Loss: 0.002702651545405388 | Testing Acc: 100.0\n",
            "Epoch: 2021 | Training Loss: 0.0015794479986652732 | Training Acc: 100.0 | Testing Loss: 0.002700429642572999 | Testing Acc: 100.0\n",
            "Epoch: 2022 | Training Loss: 0.001578398048877716 | Training Acc: 100.0 | Testing Loss: 0.0027005081064999104 | Testing Acc: 100.0\n",
            "Epoch: 2023 | Training Loss: 0.0015772997867316008 | Training Acc: 100.0 | Testing Loss: 0.002698126481845975 | Testing Acc: 100.0\n",
            "Epoch: 2024 | Training Loss: 0.001576217240653932 | Training Acc: 100.0 | Testing Loss: 0.0026981481350958347 | Testing Acc: 100.0\n",
            "Epoch: 2025 | Training Loss: 0.0015751447062939405 | Training Acc: 100.0 | Testing Loss: 0.0026957213412970304 | Testing Acc: 100.0\n",
            "Epoch: 2026 | Training Loss: 0.0015740407397970557 | Training Acc: 100.0 | Testing Loss: 0.002695675240829587 | Testing Acc: 100.0\n",
            "Epoch: 2027 | Training Loss: 0.0015730008017271757 | Training Acc: 100.0 | Testing Loss: 0.00269324891269207 | Testing Acc: 100.0\n",
            "Epoch: 2028 | Training Loss: 0.0015718797221779823 | Training Acc: 100.0 | Testing Loss: 0.0026909983716905117 | Testing Acc: 100.0\n",
            "Epoch: 2029 | Training Loss: 0.001570843392983079 | Training Acc: 100.0 | Testing Loss: 0.0026911557652056217 | Testing Acc: 100.0\n",
            "Epoch: 2030 | Training Loss: 0.0015697460621595383 | Training Acc: 100.0 | Testing Loss: 0.0026888081338256598 | Testing Acc: 100.0\n",
            "Epoch: 2031 | Training Loss: 0.0015686808619648218 | Training Acc: 100.0 | Testing Loss: 0.002688812790438533 | Testing Acc: 100.0\n",
            "Epoch: 2032 | Training Loss: 0.0015676182229071856 | Training Acc: 100.0 | Testing Loss: 0.0026864423416554928 | Testing Acc: 100.0\n",
            "Epoch: 2033 | Training Loss: 0.0015665345126762986 | Training Acc: 100.0 | Testing Loss: 0.002686407882720232 | Testing Acc: 100.0\n",
            "Epoch: 2034 | Training Loss: 0.0015654803719371557 | Training Acc: 100.0 | Testing Loss: 0.002684015082195401 | Testing Acc: 100.0\n",
            "Epoch: 2035 | Training Loss: 0.001564385020174086 | Training Acc: 100.0 | Testing Loss: 0.0026839287020266056 | Testing Acc: 100.0\n",
            "Epoch: 2036 | Training Loss: 0.00156334787607193 | Training Acc: 100.0 | Testing Loss: 0.002681512851268053 | Testing Acc: 100.0\n",
            "Epoch: 2037 | Training Loss: 0.0015622526407241821 | Training Acc: 100.0 | Testing Loss: 0.002679249970242381 | Testing Acc: 100.0\n",
            "Epoch: 2038 | Training Loss: 0.0015612103743478656 | Training Acc: 100.0 | Testing Loss: 0.0026793053839355707 | Testing Acc: 100.0\n",
            "Epoch: 2039 | Training Loss: 0.0015601261984556913 | Training Acc: 100.0 | Testing Loss: 0.0026770085096359253 | Testing Acc: 100.0\n",
            "Epoch: 2040 | Training Loss: 0.0015590769471600652 | Training Acc: 100.0 | Testing Loss: 0.00267706997692585 | Testing Acc: 100.0\n",
            "Epoch: 2041 | Training Loss: 0.0015580195467919111 | Training Acc: 100.0 | Testing Loss: 0.0026747044175863266 | Testing Acc: 100.0\n",
            "Epoch: 2042 | Training Loss: 0.0015569457318633795 | Training Acc: 100.0 | Testing Loss: 0.002674664603546262 | Testing Acc: 100.0\n",
            "Epoch: 2043 | Training Loss: 0.0015559103339910507 | Training Acc: 100.0 | Testing Loss: 0.0026722601614892483 | Testing Acc: 100.0\n",
            "Epoch: 2044 | Training Loss: 0.0015548221999779344 | Training Acc: 100.0 | Testing Loss: 0.002670041285455227 | Testing Acc: 100.0\n",
            "Epoch: 2045 | Training Loss: 0.0015538043808192015 | Training Acc: 100.0 | Testing Loss: 0.002670096233487129 | Testing Acc: 100.0\n",
            "Epoch: 2046 | Training Loss: 0.0015527083305642009 | Training Acc: 100.0 | Testing Loss: 0.002667826833203435 | Testing Acc: 100.0\n",
            "Epoch: 2047 | Training Loss: 0.0015516807325184345 | Training Acc: 100.0 | Testing Loss: 0.0026678433641791344 | Testing Acc: 100.0\n",
            "Epoch: 2048 | Training Loss: 0.0015506120398640633 | Training Acc: 100.0 | Testing Loss: 0.002665483159944415 | Testing Acc: 100.0\n",
            "Epoch: 2049 | Training Loss: 0.0015495645347982645 | Training Acc: 100.0 | Testing Loss: 0.0026654882822185755 | Testing Acc: 100.0\n",
            "Epoch: 2050 | Training Loss: 0.0015485261101275682 | Training Acc: 100.0 | Testing Loss: 0.0026631176006048918 | Testing Acc: 100.0\n",
            "Epoch: 2051 | Training Loss: 0.001547447987832129 | Training Acc: 100.0 | Testing Loss: 0.002663110615685582 | Testing Acc: 100.0\n",
            "Epoch: 2052 | Training Loss: 0.0015464362222701311 | Training Acc: 100.0 | Testing Loss: 0.0026607115287333727 | Testing Acc: 100.0\n",
            "Epoch: 2053 | Training Loss: 0.0015453555388376117 | Training Acc: 100.0 | Testing Loss: 0.0026584756560623646 | Testing Acc: 100.0\n",
            "Epoch: 2054 | Training Loss: 0.0015443435404449701 | Training Acc: 100.0 | Testing Loss: 0.0026585364248603582 | Testing Acc: 100.0\n",
            "Epoch: 2055 | Training Loss: 0.0015432776417583227 | Training Acc: 100.0 | Testing Loss: 0.0026562560815364122 | Testing Acc: 100.0\n",
            "Epoch: 2056 | Training Loss: 0.0015422459691762924 | Training Acc: 100.0 | Testing Loss: 0.0026562490966171026 | Testing Acc: 100.0\n",
            "Epoch: 2057 | Training Loss: 0.0015412084758281708 | Training Acc: 100.0 | Testing Loss: 0.002653940115123987 | Testing Acc: 100.0\n",
            "Epoch: 2058 | Training Loss: 0.0015401433920487761 | Training Acc: 100.0 | Testing Loss: 0.0026539054233580828 | Testing Acc: 100.0\n",
            "Epoch: 2059 | Training Loss: 0.0015391319757327437 | Training Acc: 100.0 | Testing Loss: 0.0026515673380345106 | Testing Acc: 100.0\n",
            "Epoch: 2060 | Training Loss: 0.0015380645636469126 | Training Acc: 100.0 | Testing Loss: 0.002649433445185423 | Testing Acc: 100.0\n",
            "Epoch: 2061 | Training Loss: 0.001537069445475936 | Training Acc: 100.0 | Testing Loss: 0.002649483270943165 | Testing Acc: 100.0\n",
            "Epoch: 2062 | Training Loss: 0.0015360091347247362 | Training Acc: 100.0 | Testing Loss: 0.0026472588069736958 | Testing Acc: 100.0\n",
            "Epoch: 2063 | Training Loss: 0.0015349829336628318 | Training Acc: 100.0 | Testing Loss: 0.0026472406461834908 | Testing Acc: 100.0\n",
            "Epoch: 2064 | Training Loss: 0.0015339473029598594 | Training Acc: 100.0 | Testing Loss: 0.0026449591387063265 | Testing Acc: 100.0\n",
            "Epoch: 2065 | Training Loss: 0.0015329110901802778 | Training Acc: 100.0 | Testing Loss: 0.002644919091835618 | Testing Acc: 100.0\n",
            "Epoch: 2066 | Training Loss: 0.0015318852383643389 | Training Acc: 100.0 | Testing Loss: 0.002642580308020115 | Testing Acc: 100.0\n",
            "Epoch: 2067 | Training Loss: 0.0015308359870687127 | Training Acc: 100.0 | Testing Loss: 0.0026425118558108807 | Testing Acc: 100.0\n",
            "Epoch: 2068 | Training Loss: 0.0015298316720873117 | Training Acc: 100.0 | Testing Loss: 0.002640156541019678 | Testing Acc: 100.0\n",
            "Epoch: 2069 | Training Loss: 0.0015287752030417323 | Training Acc: 100.0 | Testing Loss: 0.0026379877235740423 | Testing Acc: 100.0\n",
            "Epoch: 2070 | Training Loss: 0.0015277747297659516 | Training Acc: 100.0 | Testing Loss: 0.0026380945928394794 | Testing Acc: 100.0\n",
            "Epoch: 2071 | Training Loss: 0.001526733161881566 | Training Acc: 100.0 | Testing Loss: 0.0026358463801443577 | Testing Acc: 100.0\n",
            "Epoch: 2072 | Training Loss: 0.001525719533674419 | Training Acc: 100.0 | Testing Loss: 0.0026358456816524267 | Testing Acc: 100.0\n",
            "Epoch: 2073 | Training Loss: 0.0015246907714754343 | Training Acc: 100.0 | Testing Loss: 0.002633534837514162 | Testing Acc: 100.0\n",
            "Epoch: 2074 | Training Loss: 0.0015236545586958528 | Training Acc: 100.0 | Testing Loss: 0.002633511321619153 | Testing Acc: 100.0\n",
            "Epoch: 2075 | Training Loss: 0.0015226620016619563 | Training Acc: 100.0 | Testing Loss: 0.0026311841793358326 | Testing Acc: 100.0\n",
            "Epoch: 2076 | Training Loss: 0.0015216070460155606 | Training Acc: 100.0 | Testing Loss: 0.002629021182656288 | Testing Acc: 100.0\n",
            "Epoch: 2077 | Training Loss: 0.0015206369571387768 | Training Acc: 100.0 | Testing Loss: 0.0026291050016880035 | Testing Acc: 100.0\n",
            "Epoch: 2078 | Training Loss: 0.001519585377536714 | Training Acc: 100.0 | Testing Loss: 0.0026268677320331335 | Testing Acc: 100.0\n",
            "Epoch: 2079 | Training Loss: 0.0015185832744464278 | Training Acc: 100.0 | Testing Loss: 0.002626907080411911 | Testing Acc: 100.0\n",
            "Epoch: 2080 | Training Loss: 0.0015175651060417295 | Training Acc: 100.0 | Testing Loss: 0.00262463022954762 | Testing Acc: 100.0\n",
            "Epoch: 2081 | Training Loss: 0.0015165475197136402 | Training Acc: 100.0 | Testing Loss: 0.0026246579363942146 | Testing Acc: 100.0\n",
            "Epoch: 2082 | Training Loss: 0.0015155525179579854 | Training Acc: 100.0 | Testing Loss: 0.0026223757304251194 | Testing Acc: 100.0\n",
            "Epoch: 2083 | Training Loss: 0.0015145106008276343 | Training Acc: 100.0 | Testing Loss: 0.002622312866151333 | Testing Acc: 100.0\n",
            "Epoch: 2084 | Training Loss: 0.0015135335270315409 | Training Acc: 100.0 | Testing Loss: 0.002619984094053507 | Testing Acc: 100.0\n",
            "Epoch: 2085 | Training Loss: 0.0015124918427318335 | Training Acc: 100.0 | Testing Loss: 0.002617820631712675 | Testing Acc: 100.0\n",
            "Epoch: 2086 | Training Loss: 0.0015115126734599471 | Training Acc: 100.0 | Testing Loss: 0.002617864403873682 | Testing Acc: 100.0\n",
            "Epoch: 2087 | Training Loss: 0.0015104868216440082 | Training Acc: 100.0 | Testing Loss: 0.002615666948258877 | Testing Acc: 100.0\n",
            "Epoch: 2088 | Training Loss: 0.0015094880945980549 | Training Acc: 100.0 | Testing Loss: 0.002615642733871937 | Testing Acc: 100.0\n",
            "Epoch: 2089 | Training Loss: 0.0015084838960319757 | Training Acc: 100.0 | Testing Loss: 0.002613405231386423 | Testing Acc: 100.0\n",
            "Epoch: 2090 | Training Loss: 0.0015074764378368855 | Training Acc: 100.0 | Testing Loss: 0.002613364951685071 | Testing Acc: 100.0\n",
            "Epoch: 2091 | Training Loss: 0.0015064907493069768 | Training Acc: 100.0 | Testing Loss: 0.0026110762264579535 | Testing Acc: 100.0\n",
            "Epoch: 2092 | Training Loss: 0.0015054557006806135 | Training Acc: 100.0 | Testing Loss: 0.002609025686979294 | Testing Acc: 100.0\n",
            "Epoch: 2093 | Training Loss: 0.0015044966712594032 | Training Acc: 100.0 | Testing Loss: 0.0026090932078659534 | Testing Acc: 100.0\n",
            "Epoch: 2094 | Training Loss: 0.0015034620882943273 | Training Acc: 100.0 | Testing Loss: 0.0026069057639688253 | Testing Acc: 100.0\n",
            "Epoch: 2095 | Training Loss: 0.0015024847816675901 | Training Acc: 100.0 | Testing Loss: 0.002606899244710803 | Testing Acc: 100.0\n",
            "Epoch: 2096 | Training Loss: 0.0015014702221378684 | Training Acc: 100.0 | Testing Loss: 0.002604655921459198 | Testing Acc: 100.0\n",
            "Epoch: 2097 | Training Loss: 0.0015004770830273628 | Training Acc: 100.0 | Testing Loss: 0.0026046494022011757 | Testing Acc: 100.0\n",
            "Epoch: 2098 | Training Loss: 0.0014994913944974542 | Training Acc: 100.0 | Testing Loss: 0.0026023718528449535 | Testing Acc: 100.0\n",
            "Epoch: 2099 | Training Loss: 0.001498479745350778 | Training Acc: 100.0 | Testing Loss: 0.00260232575237751 | Testing Acc: 100.0\n",
            "Epoch: 2100 | Training Loss: 0.0014975109370425344 | Training Acc: 100.0 | Testing Loss: 0.002600036794319749 | Testing Acc: 100.0\n",
            "Epoch: 2101 | Training Loss: 0.0014964862493798137 | Training Acc: 100.0 | Testing Loss: 0.002597969025373459 | Testing Acc: 100.0\n",
            "Epoch: 2102 | Training Loss: 0.0014955330407246947 | Training Acc: 100.0 | Testing Loss: 0.0025980130303651094 | Testing Acc: 100.0\n",
            "Epoch: 2103 | Training Loss: 0.0014945302391424775 | Training Acc: 100.0 | Testing Loss: 0.0025958255864679813 | Testing Acc: 100.0\n",
            "Epoch: 2104 | Training Loss: 0.001493539777584374 | Training Acc: 100.0 | Testing Loss: 0.0025958362966775894 | Testing Acc: 100.0\n",
            "Epoch: 2105 | Training Loss: 0.001492564333602786 | Training Acc: 100.0 | Testing Loss: 0.002593603217974305 | Testing Acc: 100.0\n",
            "Epoch: 2106 | Training Loss: 0.0014915624633431435 | Training Acc: 100.0 | Testing Loss: 0.0025935459416359663 | Testing Acc: 100.0\n",
            "Epoch: 2107 | Training Loss: 0.0014905965654179454 | Training Acc: 100.0 | Testing Loss: 0.0025913070421665907 | Testing Acc: 100.0\n",
            "Epoch: 2108 | Training Loss: 0.001489585149101913 | Training Acc: 100.0 | Testing Loss: 0.002591238822788 | Testing Acc: 100.0\n",
            "Epoch: 2109 | Training Loss: 0.0014886392746120691 | Training Acc: 100.0 | Testing Loss: 0.002588988747447729 | Testing Acc: 100.0\n",
            "Epoch: 2110 | Training Loss: 0.00148763507604599 | Training Acc: 100.0 | Testing Loss: 0.0025868751108646393 | Testing Acc: 100.0\n",
            "Epoch: 2111 | Training Loss: 0.0014866676647216082 | Training Acc: 100.0 | Testing Loss: 0.0025869528762996197 | Testing Acc: 100.0\n",
            "Epoch: 2112 | Training Loss: 0.001485679647885263 | Training Acc: 100.0 | Testing Loss: 0.002584810834378004 | Testing Acc: 100.0\n",
            "Epoch: 2113 | Training Loss: 0.0014846967533230782 | Training Acc: 100.0 | Testing Loss: 0.0025847929064184427 | Testing Acc: 100.0\n",
            "Epoch: 2114 | Training Loss: 0.0014837265480309725 | Training Acc: 100.0 | Testing Loss: 0.0025826110504567623 | Testing Acc: 100.0\n",
            "Epoch: 2115 | Training Loss: 0.0014827337581664324 | Training Acc: 100.0 | Testing Loss: 0.002582553308457136 | Testing Acc: 100.0\n",
            "Epoch: 2116 | Training Loss: 0.001481784856878221 | Training Acc: 100.0 | Testing Loss: 0.0025803204625844955 | Testing Acc: 100.0\n",
            "Epoch: 2117 | Training Loss: 0.0014807834522798657 | Training Acc: 100.0 | Testing Loss: 0.002578245010226965 | Testing Acc: 100.0\n",
            "Epoch: 2118 | Training Loss: 0.0014798407210037112 | Training Acc: 100.0 | Testing Loss: 0.0025783125311136246 | Testing Acc: 100.0\n",
            "Epoch: 2119 | Training Loss: 0.0014788417611271143 | Training Acc: 100.0 | Testing Loss: 0.0025761467404663563 | Testing Acc: 100.0\n",
            "Epoch: 2120 | Training Loss: 0.001477892161346972 | Training Acc: 100.0 | Testing Loss: 0.0025762314908206463 | Testing Acc: 100.0\n",
            "Epoch: 2121 | Training Loss: 0.0014769185800105333 | Training Acc: 100.0 | Testing Loss: 0.00257405498996377 | Testing Acc: 100.0\n",
            "Epoch: 2122 | Training Loss: 0.0014759383630007505 | Training Acc: 100.0 | Testing Loss: 0.0025740317068994045 | Testing Acc: 100.0\n",
            "Epoch: 2123 | Training Loss: 0.001474986900575459 | Training Acc: 100.0 | Testing Loss: 0.002571803517639637 | Testing Acc: 100.0\n",
            "Epoch: 2124 | Training Loss: 0.0014739881735295057 | Training Acc: 100.0 | Testing Loss: 0.002569751348346472 | Testing Acc: 100.0\n",
            "Epoch: 2125 | Training Loss: 0.0014730669790878892 | Training Acc: 100.0 | Testing Loss: 0.002569818403571844 | Testing Acc: 100.0\n",
            "Epoch: 2126 | Training Loss: 0.0014720798935741186 | Training Acc: 100.0 | Testing Loss: 0.002567698247730732 | Testing Acc: 100.0\n",
            "Epoch: 2127 | Training Loss: 0.001471128547564149 | Training Acc: 100.0 | Testing Loss: 0.0025677140802145004 | Testing Acc: 100.0\n",
            "Epoch: 2128 | Training Loss: 0.0014701553154736757 | Training Acc: 100.0 | Testing Loss: 0.002565548522397876 | Testing Acc: 100.0\n",
            "Epoch: 2129 | Training Loss: 0.0014691881369799376 | Training Acc: 100.0 | Testing Loss: 0.00256550800986588 | Testing Acc: 100.0\n",
            "Epoch: 2130 | Training Loss: 0.0014682451728731394 | Training Acc: 100.0 | Testing Loss: 0.0025633308105170727 | Testing Acc: 100.0\n",
            "Epoch: 2131 | Training Loss: 0.0014672623947262764 | Training Acc: 100.0 | Testing Loss: 0.0025633301120251417 | Testing Acc: 100.0\n",
            "Epoch: 2132 | Training Loss: 0.0014663361944258213 | Training Acc: 100.0 | Testing Loss: 0.0025611186865717173 | Testing Acc: 100.0\n",
            "Epoch: 2133 | Training Loss: 0.0014653491089120507 | Training Acc: 100.0 | Testing Loss: 0.002559049054980278 | Testing Acc: 100.0\n",
            "Epoch: 2134 | Training Loss: 0.0014644147595390677 | Training Acc: 100.0 | Testing Loss: 0.002559070475399494 | Testing Acc: 100.0\n",
            "Epoch: 2135 | Training Loss: 0.001463444554246962 | Training Acc: 100.0 | Testing Loss: 0.002556960564106703 | Testing Acc: 100.0\n",
            "Epoch: 2136 | Training Loss: 0.0014624918112531304 | Training Acc: 100.0 | Testing Loss: 0.002556954510509968 | Testing Acc: 100.0\n",
            "Epoch: 2137 | Training Loss: 0.0014615446561947465 | Training Acc: 100.0 | Testing Loss: 0.0025548168923705816 | Testing Acc: 100.0\n",
            "Epoch: 2138 | Training Loss: 0.0014605848118662834 | Training Acc: 100.0 | Testing Loss: 0.002554758684709668 | Testing Acc: 100.0\n",
            "Epoch: 2139 | Training Loss: 0.0014596504624933004 | Training Acc: 100.0 | Testing Loss: 0.0025525810196995735 | Testing Acc: 100.0\n",
            "Epoch: 2140 | Training Loss: 0.0014586762990802526 | Training Acc: 100.0 | Testing Loss: 0.002550562145188451 | Testing Acc: 100.0\n",
            "Epoch: 2141 | Training Loss: 0.0014577594120055437 | Training Acc: 100.0 | Testing Loss: 0.0025506631936877966 | Testing Acc: 100.0\n",
            "Epoch: 2142 | Training Loss: 0.001456781174056232 | Training Acc: 100.0 | Testing Loss: 0.002548581687733531 | Testing Acc: 100.0\n",
            "Epoch: 2143 | Training Loss: 0.0014558590482920408 | Training Acc: 100.0 | Testing Loss: 0.0025485812220722437 | Testing Acc: 100.0\n",
            "Epoch: 2144 | Training Loss: 0.001454903045669198 | Training Acc: 100.0 | Testing Loss: 0.002546419855207205 | Testing Acc: 100.0\n",
            "Epoch: 2145 | Training Loss: 0.001453953213058412 | Training Acc: 100.0 | Testing Loss: 0.0025464079808443785 | Testing Acc: 100.0\n",
            "Epoch: 2146 | Training Loss: 0.0014530230546370149 | Training Acc: 100.0 | Testing Loss: 0.002544235670939088 | Testing Acc: 100.0\n",
            "Epoch: 2147 | Training Loss: 0.0014520561089739203 | Training Acc: 100.0 | Testing Loss: 0.0025441551115363836 | Testing Acc: 100.0\n",
            "Epoch: 2148 | Training Loss: 0.0014511516783386469 | Training Acc: 100.0 | Testing Loss: 0.0025420000310987234 | Testing Acc: 100.0\n",
            "Epoch: 2149 | Training Loss: 0.001450177631340921 | Training Acc: 100.0 | Testing Loss: 0.0025399518199265003 | Testing Acc: 100.0\n",
            "Epoch: 2150 | Training Loss: 0.001449260744266212 | Training Acc: 100.0 | Testing Loss: 0.002540081739425659 | Testing Acc: 100.0\n",
            "Epoch: 2151 | Training Loss: 0.0014483159175142646 | Training Acc: 100.0 | Testing Loss: 0.002537971129640937 | Testing Acc: 100.0\n",
            "Epoch: 2152 | Training Loss: 0.0014473760966211557 | Training Acc: 100.0 | Testing Loss: 0.002537953667342663 | Testing Acc: 100.0\n",
            "Epoch: 2153 | Training Loss: 0.0014464501291513443 | Training Acc: 100.0 | Testing Loss: 0.002535843290388584 | Testing Acc: 100.0\n",
            "Epoch: 2154 | Training Loss: 0.001445499248802662 | Training Acc: 100.0 | Testing Loss: 0.0025357974227517843 | Testing Acc: 100.0\n",
            "Epoch: 2155 | Training Loss: 0.0014445859706029296 | Training Acc: 100.0 | Testing Loss: 0.0025336584076285362 | Testing Acc: 100.0\n",
            "Epoch: 2156 | Training Loss: 0.001443632529117167 | Training Acc: 100.0 | Testing Loss: 0.002531650010496378 | Testing Acc: 100.0\n",
            "Epoch: 2157 | Training Loss: 0.001442727167159319 | Training Acc: 100.0 | Testing Loss: 0.002531706355512142 | Testing Acc: 100.0\n",
            "Epoch: 2158 | Training Loss: 0.0014417737256735563 | Training Acc: 100.0 | Testing Loss: 0.0025296637322753668 | Testing Acc: 100.0\n",
            "Epoch: 2159 | Training Loss: 0.0014408602146431804 | Training Acc: 100.0 | Testing Loss: 0.0025296455714851618 | Testing Acc: 100.0\n",
            "Epoch: 2160 | Training Loss: 0.0014399169012904167 | Training Acc: 100.0 | Testing Loss: 0.0025275691878050566 | Testing Acc: 100.0\n",
            "Epoch: 2161 | Training Loss: 0.001438990468159318 | Training Acc: 100.0 | Testing Loss: 0.0025275740772485733 | Testing Acc: 100.0\n",
            "Epoch: 2162 | Training Loss: 0.0014380768407136202 | Training Acc: 100.0 | Testing Loss: 0.0025254464708268642 | Testing Acc: 100.0\n",
            "Epoch: 2163 | Training Loss: 0.0014371254947036505 | Training Acc: 100.0 | Testing Loss: 0.0025253945495933294 | Testing Acc: 100.0\n",
            "Epoch: 2164 | Training Loss: 0.0014362328220158815 | Training Acc: 100.0 | Testing Loss: 0.0025232385378330946 | Testing Acc: 100.0\n",
            "Epoch: 2165 | Training Loss: 0.0014352727448567748 | Training Acc: 100.0 | Testing Loss: 0.0025212178006768227 | Testing Acc: 100.0\n",
            "Epoch: 2166 | Training Loss: 0.0014343776274472475 | Training Acc: 100.0 | Testing Loss: 0.0025212739128619432 | Testing Acc: 100.0\n",
            "Epoch: 2167 | Training Loss: 0.0014334418810904026 | Training Acc: 100.0 | Testing Loss: 0.002519219880923629 | Testing Acc: 100.0\n",
            "Epoch: 2168 | Training Loss: 0.0014325238298624754 | Training Acc: 100.0 | Testing Loss: 0.002519224537536502 | Testing Acc: 100.0\n",
            "Epoch: 2169 | Training Loss: 0.0014316125307232141 | Training Acc: 100.0 | Testing Loss: 0.0025171248707920313 | Testing Acc: 100.0\n",
            "Epoch: 2170 | Training Loss: 0.001430675620213151 | Training Acc: 100.0 | Testing Loss: 0.002517090644687414 | Testing Acc: 100.0\n",
            "Epoch: 2171 | Training Loss: 0.0014297824818640947 | Training Acc: 100.0 | Testing Loss: 0.0025149849243462086 | Testing Acc: 100.0\n",
            "Epoch: 2172 | Training Loss: 0.0014288434758782387 | Training Acc: 100.0 | Testing Loss: 0.0025130552239716053 | Testing Acc: 100.0\n",
            "Epoch: 2173 | Training Loss: 0.0014279547613114119 | Training Acc: 100.0 | Testing Loss: 0.00251311669126153 | Testing Acc: 100.0\n",
            "Epoch: 2174 | Training Loss: 0.0014270144747570157 | Training Acc: 100.0 | Testing Loss: 0.002511067781597376 | Testing Acc: 100.0\n",
            "Epoch: 2175 | Training Loss: 0.0014261121395975351 | Training Acc: 100.0 | Testing Loss: 0.002511067781597376 | Testing Acc: 100.0\n",
            "Epoch: 2176 | Training Loss: 0.0014251990942284465 | Training Acc: 100.0 | Testing Loss: 0.002509017940610647 | Testing Acc: 100.0\n",
            "Epoch: 2177 | Training Loss: 0.0014242797624319792 | Training Acc: 100.0 | Testing Loss: 0.0025089606642723083 | Testing Acc: 100.0\n",
            "Epoch: 2178 | Training Loss: 0.0014233780093491077 | Training Acc: 100.0 | Testing Loss: 0.0025068947579711676 | Testing Acc: 100.0\n",
            "Epoch: 2179 | Training Loss: 0.0014224457554519176 | Training Acc: 100.0 | Testing Loss: 0.002506803022697568 | Testing Acc: 100.0\n",
            "Epoch: 2180 | Training Loss: 0.0014215657720342278 | Training Acc: 100.0 | Testing Loss: 0.0025046917144209146 | Testing Acc: 100.0\n",
            "Epoch: 2181 | Training Loss: 0.0014206317719072104 | Training Acc: 100.0 | Testing Loss: 0.002502778545022011 | Testing Acc: 100.0\n",
            "Epoch: 2182 | Training Loss: 0.0014197478303685784 | Training Acc: 100.0 | Testing Loss: 0.0025028001982718706 | Testing Acc: 100.0\n",
            "Epoch: 2183 | Training Loss: 0.001418824540451169 | Training Acc: 100.0 | Testing Loss: 0.0025007680524140596 | Testing Acc: 100.0\n",
            "Epoch: 2184 | Training Loss: 0.0014179220888763666 | Training Acc: 100.0 | Testing Loss: 0.0025007552467286587 | Testing Acc: 100.0\n",
            "Epoch: 2185 | Training Loss: 0.0014170205686241388 | Training Acc: 100.0 | Testing Loss: 0.002498677233234048 | Testing Acc: 100.0\n",
            "Epoch: 2186 | Training Loss: 0.0014161057770252228 | Training Acc: 100.0 | Testing Loss: 0.0024986423086375 | Testing Acc: 100.0\n",
            "Epoch: 2187 | Training Loss: 0.0014152403455227613 | Training Acc: 100.0 | Testing Loss: 0.00249656499363482 | Testing Acc: 100.0\n",
            "Epoch: 2188 | Training Loss: 0.0014143005246296525 | Training Acc: 100.0 | Testing Loss: 0.00249460618942976 | Testing Acc: 100.0\n",
            "Epoch: 2189 | Training Loss: 0.0014134313678368926 | Training Acc: 100.0 | Testing Loss: 0.002494684886187315 | Testing Acc: 100.0\n",
            "Epoch: 2190 | Training Loss: 0.0014125022571533918 | Training Acc: 100.0 | Testing Loss: 0.002492663450539112 | Testing Acc: 100.0\n",
            "Epoch: 2191 | Training Loss: 0.0014116204110905528 | Training Acc: 100.0 | Testing Loss: 0.0024927197955548763 | Testing Acc: 100.0\n",
            "Epoch: 2192 | Training Loss: 0.0014107280876487494 | Training Acc: 100.0 | Testing Loss: 0.0024906988255679607 | Testing Acc: 100.0\n",
            "Epoch: 2193 | Training Loss: 0.0014098072424530983 | Training Acc: 100.0 | Testing Loss: 0.0024906585458666086 | Testing Acc: 100.0\n",
            "Epoch: 2194 | Training Loss: 0.0014089399483054876 | Training Acc: 100.0 | Testing Loss: 0.002488597296178341 | Testing Acc: 100.0\n",
            "Epoch: 2195 | Training Loss: 0.001408018171787262 | Training Acc: 100.0 | Testing Loss: 0.002488528611138463 | Testing Acc: 100.0\n",
            "Epoch: 2196 | Training Loss: 0.001407156465575099 | Training Acc: 100.0 | Testing Loss: 0.002486439188942313 | Testing Acc: 100.0\n",
            "Epoch: 2197 | Training Loss: 0.0014062414411455393 | Training Acc: 100.0 | Testing Loss: 0.0024845078587532043 | Testing Acc: 100.0\n",
            "Epoch: 2198 | Training Loss: 0.0014053682098165154 | Training Acc: 100.0 | Testing Loss: 0.002484535565599799 | Testing Acc: 100.0\n",
            "Epoch: 2199 | Training Loss: 0.0014044649433344603 | Training Acc: 100.0 | Testing Loss: 0.0024825534783303738 | Testing Acc: 100.0\n",
            "Epoch: 2200 | Training Loss: 0.0014035834465175867 | Training Acc: 100.0 | Testing Loss: 0.0024825814180076122 | Testing Acc: 100.0\n",
            "Epoch: 2201 | Training Loss: 0.0014027012512087822 | Training Acc: 100.0 | Testing Loss: 0.0024805429857224226 | Testing Acc: 100.0\n",
            "Epoch: 2202 | Training Loss: 0.0014017929788678885 | Training Acc: 100.0 | Testing Loss: 0.002480525290593505 | Testing Acc: 100.0\n",
            "Epoch: 2203 | Training Loss: 0.0014009360456839204 | Training Acc: 100.0 | Testing Loss: 0.002478452166542411 | Testing Acc: 100.0\n",
            "Epoch: 2204 | Training Loss: 0.0014000209048390388 | Training Acc: 100.0 | Testing Loss: 0.0024765385314822197 | Testing Acc: 100.0\n",
            "Epoch: 2205 | Training Loss: 0.001399161876179278 | Training Acc: 100.0 | Testing Loss: 0.0024765944108366966 | Testing Acc: 100.0\n",
            "Epoch: 2206 | Training Loss: 0.00139825907535851 | Training Acc: 100.0 | Testing Loss: 0.0024746181443333626 | Testing Acc: 100.0\n",
            "Epoch: 2207 | Training Loss: 0.0013973796740174294 | Training Acc: 100.0 | Testing Loss: 0.0024746230337768793 | Testing Acc: 100.0\n",
            "Epoch: 2208 | Training Loss: 0.0013965072575956583 | Training Acc: 100.0 | Testing Loss: 0.002472612541168928 | Testing Acc: 100.0\n",
            "Epoch: 2209 | Training Loss: 0.001395606086589396 | Training Acc: 100.0 | Testing Loss: 0.002472554799169302 | Testing Acc: 100.0\n",
            "Epoch: 2210 | Training Loss: 0.0013947489205747843 | Training Acc: 100.0 | Testing Loss: 0.0024705384857952595 | Testing Acc: 100.0\n",
            "Epoch: 2211 | Training Loss: 0.001393844373524189 | Training Acc: 100.0 | Testing Loss: 0.0024686807300895452 | Testing Acc: 100.0\n",
            "Epoch: 2212 | Training Loss: 0.001392996171489358 | Training Acc: 100.0 | Testing Loss: 0.002468713792040944 | Testing Acc: 100.0\n",
            "Epoch: 2213 | Training Loss: 0.0013921029167249799 | Training Acc: 100.0 | Testing Loss: 0.00246677128598094 | Testing Acc: 100.0\n",
            "Epoch: 2214 | Training Loss: 0.001391235738992691 | Training Acc: 100.0 | Testing Loss: 0.002466770587489009 | Testing Acc: 100.0\n",
            "Epoch: 2215 | Training Loss: 0.0013903597136959434 | Training Acc: 100.0 | Testing Loss: 0.002464782679453492 | Testing Acc: 100.0\n",
            "Epoch: 2216 | Training Loss: 0.001389478100463748 | Training Acc: 100.0 | Testing Loss: 0.0024647247046232224 | Testing Acc: 100.0\n",
            "Epoch: 2217 | Training Loss: 0.0013886208180338144 | Training Acc: 100.0 | Testing Loss: 0.002462714212015271 | Testing Acc: 100.0\n",
            "Epoch: 2218 | Training Loss: 0.0013877229066565633 | Training Acc: 100.0 | Testing Loss: 0.0024626448284834623 | Testing Acc: 100.0\n",
            "Epoch: 2219 | Training Loss: 0.0013868773821741343 | Training Acc: 100.0 | Testing Loss: 0.002460611518472433 | Testing Acc: 100.0\n",
            "Epoch: 2220 | Training Loss: 0.0013859823811799288 | Training Acc: 100.0 | Testing Loss: 0.0024587251245975494 | Testing Acc: 100.0\n",
            "Epoch: 2221 | Training Loss: 0.0013851382536813617 | Training Acc: 100.0 | Testing Loss: 0.002458792645484209 | Testing Acc: 100.0\n",
            "Epoch: 2222 | Training Loss: 0.0013842511689290404 | Training Acc: 100.0 | Testing Loss: 0.002456838497892022 | Testing Acc: 100.0\n",
            "Epoch: 2223 | Training Loss: 0.001383383758366108 | Training Acc: 100.0 | Testing Loss: 0.0024568375665694475 | Testing Acc: 100.0\n",
            "Epoch: 2224 | Training Loss: 0.0013825253117829561 | Training Acc: 100.0 | Testing Loss: 0.002454825909808278 | Testing Acc: 100.0\n",
            "Epoch: 2225 | Training Loss: 0.0013816358987241983 | Training Acc: 100.0 | Testing Loss: 0.0024547625798732042 | Testing Acc: 100.0\n",
            "Epoch: 2226 | Training Loss: 0.0013808044604957104 | Training Acc: 100.0 | Testing Loss: 0.0024527800269424915 | Testing Acc: 100.0\n",
            "Epoch: 2227 | Training Loss: 0.0013799129519611597 | Training Acc: 100.0 | Testing Loss: 0.0024509108625352383 | Testing Acc: 100.0\n",
            "Epoch: 2228 | Training Loss: 0.001379065215587616 | Training Acc: 100.0 | Testing Loss: 0.0024509611539542675 | Testing Acc: 100.0\n",
            "Epoch: 2229 | Training Loss: 0.001378186629153788 | Training Acc: 100.0 | Testing Loss: 0.0024490233045071363 | Testing Acc: 100.0\n",
            "Epoch: 2230 | Training Loss: 0.0013773379614576697 | Training Acc: 100.0 | Testing Loss: 0.002449056599289179 | Testing Acc: 100.0\n",
            "Epoch: 2231 | Training Loss: 0.0013764813775196671 | Training Acc: 100.0 | Testing Loss: 0.0024471136275678873 | Testing Acc: 100.0\n",
            "Epoch: 2232 | Training Loss: 0.0013756097760051489 | Training Acc: 100.0 | Testing Loss: 0.002447061939164996 | Testing Acc: 100.0\n",
            "Epoch: 2233 | Training Loss: 0.0013747637858614326 | Training Acc: 100.0 | Testing Loss: 0.0024450786877423525 | Testing Acc: 100.0\n",
            "Epoch: 2234 | Training Loss: 0.0013738820562139153 | Training Acc: 100.0 | Testing Loss: 0.0024432202335447073 | Testing Acc: 100.0\n",
            "Epoch: 2235 | Training Loss: 0.0013730548089370131 | Training Acc: 100.0 | Testing Loss: 0.0024432477075606585 | Testing Acc: 100.0\n",
            "Epoch: 2236 | Training Loss: 0.0013721728464588523 | Training Acc: 100.0 | Testing Loss: 0.0024413494393229485 | Testing Acc: 100.0\n",
            "Epoch: 2237 | Training Loss: 0.0013713318621739745 | Training Acc: 100.0 | Testing Loss: 0.002441337564960122 | Testing Acc: 100.0\n",
            "Epoch: 2238 | Training Loss: 0.001370467129163444 | Training Acc: 100.0 | Testing Loss: 0.002439376898109913 | Testing Acc: 100.0\n",
            "Epoch: 2239 | Training Loss: 0.0013696124078705907 | Training Acc: 100.0 | Testing Loss: 0.00243935314938426 | Testing Acc: 100.0\n",
            "Epoch: 2240 | Training Loss: 0.001368772704154253 | Training Acc: 100.0 | Testing Loss: 0.00243735802359879 | Testing Acc: 100.0\n",
            "Epoch: 2241 | Training Loss: 0.001367894932627678 | Training Acc: 100.0 | Testing Loss: 0.0024373577907681465 | Testing Acc: 100.0\n",
            "Epoch: 2242 | Training Loss: 0.0013670832850039005 | Training Acc: 100.0 | Testing Loss: 0.002435340080410242 | Testing Acc: 100.0\n",
            "Epoch: 2243 | Training Loss: 0.0013662043493241072 | Training Acc: 100.0 | Testing Loss: 0.0024334758054465055 | Testing Acc: 100.0\n",
            "Epoch: 2244 | Training Loss: 0.0013653816422447562 | Training Acc: 100.0 | Testing Loss: 0.0024335088673979044 | Testing Acc: 100.0\n",
            "Epoch: 2245 | Training Loss: 0.001364520052447915 | Training Acc: 100.0 | Testing Loss: 0.002431593369692564 | Testing Acc: 100.0\n",
            "Epoch: 2246 | Training Loss: 0.001363671151921153 | Training Acc: 100.0 | Testing Loss: 0.0024315756745636463 | Testing Acc: 100.0\n",
            "Epoch: 2247 | Training Loss: 0.0013628366868942976 | Training Acc: 100.0 | Testing Loss: 0.0024296315386891365 | Testing Acc: 100.0\n",
            "Epoch: 2248 | Training Loss: 0.0013619813835248351 | Training Acc: 100.0 | Testing Loss: 0.0024295912589877844 | Testing Acc: 100.0\n",
            "Epoch: 2249 | Training Loss: 0.001361147966235876 | Training Acc: 100.0 | Testing Loss: 0.002427624072879553 | Testing Acc: 100.0\n",
            "Epoch: 2250 | Training Loss: 0.001360280904918909 | Training Acc: 100.0 | Testing Loss: 0.0024258338380604982 | Testing Acc: 100.0\n",
            "Epoch: 2251 | Training Loss: 0.0013594659976661205 | Training Acc: 100.0 | Testing Loss: 0.0024258436169475317 | Testing Acc: 100.0\n",
            "Epoch: 2252 | Training Loss: 0.0013586024288088083 | Training Acc: 100.0 | Testing Loss: 0.0024239798076450825 | Testing Acc: 100.0\n",
            "Epoch: 2253 | Training Loss: 0.0013577730860561132 | Training Acc: 100.0 | Testing Loss: 0.0024239441845566034 | Testing Acc: 100.0\n",
            "Epoch: 2254 | Training Loss: 0.0013569298898801208 | Training Acc: 100.0 | Testing Loss: 0.0024220170453190804 | Testing Acc: 100.0\n",
            "Epoch: 2255 | Training Loss: 0.001356079475954175 | Training Acc: 100.0 | Testing Loss: 0.00242199981585145 | Testing Acc: 100.0\n",
            "Epoch: 2256 | Training Loss: 0.0013552553718909621 | Training Acc: 100.0 | Testing Loss: 0.0024200440384447575 | Testing Acc: 100.0\n",
            "Epoch: 2257 | Training Loss: 0.0013543921522796154 | Training Acc: 100.0 | Testing Loss: 0.0024199923500418663 | Testing Acc: 100.0\n",
            "Epoch: 2258 | Training Loss: 0.0013535928446799517 | Training Acc: 100.0 | Testing Loss: 0.0024180018808692694 | Testing Acc: 100.0\n",
            "Epoch: 2259 | Training Loss: 0.0013527311384677887 | Training Acc: 100.0 | Testing Loss: 0.002416165778413415 | Testing Acc: 100.0\n",
            "Epoch: 2260 | Training Loss: 0.0013519139029085636 | Training Acc: 100.0 | Testing Loss: 0.0024162617046386003 | Testing Acc: 100.0\n",
            "Epoch: 2261 | Training Loss: 0.0013510717544704676 | Training Acc: 100.0 | Testing Loss: 0.0024143685586750507 | Testing Acc: 100.0\n",
            "Epoch: 2262 | Training Loss: 0.0013502419460564852 | Training Acc: 100.0 | Testing Loss: 0.0024143450427800417 | Testing Acc: 100.0\n",
            "Epoch: 2263 | Training Loss: 0.0013494171435013413 | Training Acc: 100.0 | Testing Loss: 0.002412428380921483 | Testing Acc: 100.0\n",
            "Epoch: 2264 | Training Loss: 0.0013485632371157408 | Training Acc: 100.0 | Testing Loss: 0.0024123825132846832 | Testing Acc: 100.0\n",
            "Epoch: 2265 | Training Loss: 0.0013477586908265948 | Training Acc: 100.0 | Testing Loss: 0.002410460729151964 | Testing Acc: 100.0\n",
            "Epoch: 2266 | Training Loss: 0.001346901641227305 | Training Acc: 100.0 | Testing Loss: 0.002408646745607257 | Testing Acc: 100.0\n",
            "Epoch: 2267 | Training Loss: 0.0013460976770147681 | Training Acc: 100.0 | Testing Loss: 0.0024086511693894863 | Testing Acc: 100.0\n",
            "Epoch: 2268 | Training Loss: 0.0013452557614073157 | Training Acc: 100.0 | Testing Loss: 0.0024068139027804136 | Testing Acc: 100.0\n",
            "Epoch: 2269 | Training Loss: 0.001344433519989252 | Training Acc: 100.0 | Testing Loss: 0.0024068537168204784 | Testing Acc: 100.0\n",
            "Epoch: 2270 | Training Loss: 0.0013436097651720047 | Training Acc: 100.0 | Testing Loss: 0.002404959639534354 | Testing Acc: 100.0\n",
            "Epoch: 2271 | Training Loss: 0.001342778792604804 | Training Acc: 100.0 | Testing Loss: 0.0024049137718975544 | Testing Acc: 100.0\n",
            "Epoch: 2272 | Training Loss: 0.0013419744791463017 | Training Acc: 100.0 | Testing Loss: 0.002402980113402009 | Testing Acc: 100.0\n",
            "Epoch: 2273 | Training Loss: 0.0013411167310550809 | Training Acc: 100.0 | Testing Loss: 0.002401182195171714 | Testing Acc: 100.0\n",
            "Epoch: 2274 | Training Loss: 0.0013403305783867836 | Training Acc: 100.0 | Testing Loss: 0.0024012101348489523 | Testing Acc: 100.0\n",
            "Epoch: 2275 | Training Loss: 0.0013394815614446998 | Training Acc: 100.0 | Testing Loss: 0.0023993677459657192 | Testing Acc: 100.0\n",
            "Epoch: 2276 | Training Loss: 0.0013386700302362442 | Training Acc: 100.0 | Testing Loss: 0.002399361226707697 | Testing Acc: 100.0\n",
            "Epoch: 2277 | Training Loss: 0.001337847555987537 | Training Acc: 100.0 | Testing Loss: 0.0023974727373570204 | Testing Acc: 100.0\n",
            "Epoch: 2278 | Training Loss: 0.0013370249653235078 | Training Acc: 100.0 | Testing Loss: 0.0023974268697202206 | Testing Acc: 100.0\n",
            "Epoch: 2279 | Training Loss: 0.0013362247264012694 | Training Acc: 100.0 | Testing Loss: 0.0023955267388373613 | Testing Acc: 100.0\n",
            "Epoch: 2280 | Training Loss: 0.0013353823451325297 | Training Acc: 100.0 | Testing Loss: 0.0023955036886036396 | Testing Acc: 100.0\n",
            "Epoch: 2281 | Training Loss: 0.0013345928164198995 | Training Acc: 100.0 | Testing Loss: 0.0023935921490192413 | Testing Acc: 100.0\n",
            "Epoch: 2282 | Training Loss: 0.0013337517157196999 | Training Acc: 100.0 | Testing Loss: 0.0023917837534099817 | Testing Acc: 100.0\n",
            "Epoch: 2283 | Training Loss: 0.001332950429059565 | Training Acc: 100.0 | Testing Loss: 0.002391788410022855 | Testing Acc: 100.0\n",
            "Epoch: 2284 | Training Loss: 0.0013321286533027887 | Training Acc: 100.0 | Testing Loss: 0.002389962552115321 | Testing Acc: 100.0\n",
            "Epoch: 2285 | Training Loss: 0.0013313150266185403 | Training Acc: 100.0 | Testing Loss: 0.002389939036220312 | Testing Acc: 100.0\n",
            "Epoch: 2286 | Training Loss: 0.0013304988387972116 | Training Acc: 100.0 | Testing Loss: 0.00238807313144207 | Testing Acc: 100.0\n",
            "Epoch: 2287 | Training Loss: 0.0013296811375766993 | Training Acc: 100.0 | Testing Loss: 0.002388021443039179 | Testing Acc: 100.0\n",
            "Epoch: 2288 | Training Loss: 0.0013288939371705055 | Training Acc: 100.0 | Testing Loss: 0.0023861038498580456 | Testing Acc: 100.0\n",
            "Epoch: 2289 | Training Loss: 0.0013280611019581556 | Training Acc: 100.0 | Testing Loss: 0.002384403022006154 | Testing Acc: 100.0\n",
            "Epoch: 2290 | Training Loss: 0.001327279955148697 | Training Acc: 100.0 | Testing Loss: 0.0023844195529818535 | Testing Acc: 100.0\n",
            "Epoch: 2291 | Training Loss: 0.0013264508452266455 | Training Acc: 100.0 | Testing Loss: 0.00238258158788085 | Testing Acc: 100.0\n",
            "Epoch: 2292 | Training Loss: 0.001325651304796338 | Training Acc: 100.0 | Testing Loss: 0.002382580889388919 | Testing Acc: 100.0\n",
            "Epoch: 2293 | Training Loss: 0.0013248443137854338 | Training Acc: 100.0 | Testing Loss: 0.0023807145189493895 | Testing Acc: 100.0\n",
            "Epoch: 2294 | Training Loss: 0.001324022887274623 | Training Acc: 100.0 | Testing Loss: 0.0023806628305464983 | Testing Acc: 100.0\n",
            "Epoch: 2295 | Training Loss: 0.0013232473284006119 | Training Acc: 100.0 | Testing Loss: 0.0023787906393408775 | Testing Acc: 100.0\n",
            "Epoch: 2296 | Training Loss: 0.0013224149588495493 | Training Acc: 100.0 | Testing Loss: 0.0023770269472151995 | Testing Acc: 100.0\n",
            "Epoch: 2297 | Training Loss: 0.0013216339284554124 | Training Acc: 100.0 | Testing Loss: 0.0023770718835294247 | Testing Acc: 100.0\n",
            "Epoch: 2298 | Training Loss: 0.0013208098243921995 | Training Acc: 100.0 | Testing Loss: 0.002375262323766947 | Testing Acc: 100.0\n",
            "Epoch: 2299 | Training Loss: 0.0013200200628489256 | Training Acc: 100.0 | Testing Loss: 0.0023752786219120026 | Testing Acc: 100.0\n",
            "Epoch: 2300 | Training Loss: 0.0013192250626161695 | Training Acc: 100.0 | Testing Loss: 0.0023734462447464466 | Testing Acc: 100.0\n",
            "Epoch: 2301 | Training Loss: 0.0013184113195165992 | Training Acc: 100.0 | Testing Loss: 0.002373411785811186 | Testing Acc: 100.0\n",
            "Epoch: 2302 | Training Loss: 0.0013176300562918186 | Training Acc: 100.0 | Testing Loss: 0.0023715393617749214 | Testing Acc: 100.0\n",
            "Epoch: 2303 | Training Loss: 0.0013168075820431113 | Training Acc: 100.0 | Testing Loss: 0.0023714762646704912 | Testing Acc: 100.0\n",
            "Epoch: 2304 | Training Loss: 0.0013160377275198698 | Training Acc: 100.0 | Testing Loss: 0.0023695870768278837 | Testing Acc: 100.0\n",
            "Epoch: 2305 | Training Loss: 0.0013152208412066102 | Training Acc: 100.0 | Testing Loss: 0.0023678282741457224 | Testing Acc: 100.0\n",
            "Epoch: 2306 | Training Loss: 0.0013144506374374032 | Training Acc: 100.0 | Testing Loss: 0.002367850858718157 | Testing Acc: 100.0\n",
            "Epoch: 2307 | Training Loss: 0.0013136372435837984 | Training Acc: 100.0 | Testing Loss: 0.0023660236038267612 | Testing Acc: 100.0\n",
            "Epoch: 2308 | Training Loss: 0.0013128361897543073 | Training Acc: 100.0 | Testing Loss: 0.0023660685401409864 | Testing Acc: 100.0\n",
            "Epoch: 2309 | Training Loss: 0.0013120572548359632 | Training Acc: 100.0 | Testing Loss: 0.0023642191663384438 | Testing Acc: 100.0\n",
            "Epoch: 2310 | Training Loss: 0.001311247586272657 | Training Acc: 100.0 | Testing Loss: 0.002364149782806635 | Testing Acc: 100.0\n",
            "Epoch: 2311 | Training Loss: 0.001310477266088128 | Training Acc: 100.0 | Testing Loss: 0.002362299943342805 | Testing Acc: 100.0\n",
            "Epoch: 2312 | Training Loss: 0.0013096727197989821 | Training Acc: 100.0 | Testing Loss: 0.0023605525493621826 | Testing Acc: 100.0\n",
            "Epoch: 2313 | Training Loss: 0.0013088976265862584 | Training Acc: 100.0 | Testing Loss: 0.002360597485676408 | Testing Acc: 100.0\n",
            "Epoch: 2314 | Training Loss: 0.0013080949429422617 | Training Acc: 100.0 | Testing Loss: 0.0023588042240589857 | Testing Acc: 100.0\n",
            "Epoch: 2315 | Training Loss: 0.0013073082081973553 | Training Acc: 100.0 | Testing Loss: 0.002358787227421999 | Testing Acc: 100.0\n",
            "Epoch: 2316 | Training Loss: 0.001306526712141931 | Training Acc: 100.0 | Testing Loss: 0.002356982557103038 | Testing Acc: 100.0\n",
            "Epoch: 2317 | Training Loss: 0.0013057291507720947 | Training Acc: 100.0 | Testing Loss: 0.0023569189943373203 | Testing Acc: 100.0\n",
            "Epoch: 2318 | Training Loss: 0.0013049561530351639 | Training Acc: 100.0 | Testing Loss: 0.002355074742808938 | Testing Acc: 100.0\n",
            "Epoch: 2319 | Training Loss: 0.0013041491620242596 | Training Acc: 100.0 | Testing Loss: 0.002353400457650423 | Testing Acc: 100.0\n",
            "Epoch: 2320 | Training Loss: 0.0013033926952630281 | Training Acc: 100.0 | Testing Loss: 0.002353428862988949 | Testing Acc: 100.0\n",
            "Epoch: 2321 | Training Loss: 0.0013025877997279167 | Training Acc: 100.0 | Testing Loss: 0.002351675881072879 | Testing Acc: 100.0\n",
            "Epoch: 2322 | Training Loss: 0.0013018132885918021 | Training Acc: 100.0 | Testing Loss: 0.002351634670048952 | Testing Acc: 100.0\n",
            "Epoch: 2323 | Training Loss: 0.001301030395552516 | Training Acc: 100.0 | Testing Loss: 0.0023498248774558306 | Testing Acc: 100.0\n",
            "Epoch: 2324 | Training Loss: 0.0013002421474084258 | Training Acc: 100.0 | Testing Loss: 0.002349784132093191 | Testing Acc: 100.0\n",
            "Epoch: 2325 | Training Loss: 0.0012994784628972411 | Training Acc: 100.0 | Testing Loss: 0.0023479510564357042 | Testing Acc: 100.0\n",
            "Epoch: 2326 | Training Loss: 0.001298674731515348 | Training Acc: 100.0 | Testing Loss: 0.002347870497033 | Testing Acc: 100.0\n",
            "Epoch: 2327 | Training Loss: 0.0012979275779798627 | Training Acc: 100.0 | Testing Loss: 0.0023460430093109608 | Testing Acc: 100.0\n",
            "Epoch: 2328 | Training Loss: 0.0012971197720617056 | Training Acc: 100.0 | Testing Loss: 0.0023443580139428377 | Testing Acc: 100.0\n",
            "Epoch: 2329 | Training Loss: 0.0012963612098246813 | Training Acc: 100.0 | Testing Loss: 0.0023443454410880804 | Testing Acc: 100.0\n",
            "Epoch: 2330 | Training Loss: 0.0012955826241523027 | Training Acc: 100.0 | Testing Loss: 0.0023425978142768145 | Testing Acc: 100.0\n",
            "Epoch: 2331 | Training Loss: 0.0012947978684678674 | Training Acc: 100.0 | Testing Loss: 0.0023425687104463577 | Testing Acc: 100.0\n",
            "Epoch: 2332 | Training Loss: 0.0012940334854647517 | Training Acc: 100.0 | Testing Loss: 0.002340763108804822 | Testing Acc: 100.0\n",
            "Epoch: 2333 | Training Loss: 0.0012932445388287306 | Training Acc: 100.0 | Testing Loss: 0.0023407055996358395 | Testing Acc: 100.0\n",
            "Epoch: 2334 | Training Loss: 0.0012924887705594301 | Training Acc: 100.0 | Testing Loss: 0.0023388774134218693 | Testing Acc: 100.0\n",
            "Epoch: 2335 | Training Loss: 0.0012916993582621217 | Training Acc: 100.0 | Testing Loss: 0.0023371924180537462 | Testing Acc: 100.0\n",
            "Epoch: 2336 | Training Loss: 0.001290946383960545 | Training Acc: 100.0 | Testing Loss: 0.0023372143041342497 | Testing Acc: 100.0\n",
            "Epoch: 2337 | Training Loss: 0.0012901531299576163 | Training Acc: 100.0 | Testing Loss: 0.002335442928597331 | Testing Acc: 100.0\n",
            "Epoch: 2338 | Training Loss: 0.0012893941020593047 | Training Acc: 100.0 | Testing Loss: 0.0023354999721050262 | Testing Acc: 100.0\n",
            "Epoch: 2339 | Training Loss: 0.0012886294862255454 | Training Acc: 100.0 | Testing Loss: 0.0023336943704634905 | Testing Acc: 100.0\n",
            "Epoch: 2340 | Training Loss: 0.0012878411216661334 | Training Acc: 100.0 | Testing Loss: 0.002333659678697586 | Testing Acc: 100.0\n",
            "Epoch: 2341 | Training Loss: 0.0012871020007878542 | Training Acc: 100.0 | Testing Loss: 0.00233184895478189 | Testing Acc: 100.0\n",
            "Epoch: 2342 | Training Loss: 0.001286309794522822 | Training Acc: 100.0 | Testing Loss: 0.0023301574401557446 | Testing Acc: 100.0\n",
            "Epoch: 2343 | Training Loss: 0.0012855603126809 | Training Acc: 100.0 | Testing Loss: 0.0023301965557038784 | Testing Acc: 100.0\n",
            "Epoch: 2344 | Training Loss: 0.0012847778853029013 | Training Acc: 100.0 | Testing Loss: 0.0023284482304006815 | Testing Acc: 100.0\n",
            "Epoch: 2345 | Training Loss: 0.0012840230483561754 | Training Acc: 100.0 | Testing Loss: 0.002328424947336316 | Testing Acc: 100.0\n",
            "Epoch: 2346 | Training Loss: 0.0012832677457481623 | Training Acc: 100.0 | Testing Loss: 0.002326670801267028 | Testing Acc: 100.0\n",
            "Epoch: 2347 | Training Loss: 0.0012824914883822203 | Training Acc: 100.0 | Testing Loss: 0.002326664514839649 | Testing Acc: 100.0\n",
            "Epoch: 2348 | Training Loss: 0.0012817422393709421 | Training Acc: 100.0 | Testing Loss: 0.00232488801702857 | Testing Acc: 100.0\n",
            "Epoch: 2349 | Training Loss: 0.0012809589970856905 | Training Acc: 100.0 | Testing Loss: 0.0023231899831444025 | Testing Acc: 100.0\n",
            "Epoch: 2350 | Training Loss: 0.0012802244164049625 | Training Acc: 100.0 | Testing Loss: 0.002323206514120102 | Testing Acc: 100.0\n",
            "Epoch: 2351 | Training Loss: 0.0012794440845027566 | Training Acc: 100.0 | Testing Loss: 0.002321497770026326 | Testing Acc: 100.0\n",
            "Epoch: 2352 | Training Loss: 0.0012787014711648226 | Training Acc: 100.0 | Testing Loss: 0.002321468433365226 | Testing Acc: 100.0\n",
            "Epoch: 2353 | Training Loss: 0.0012779372045770288 | Training Acc: 100.0 | Testing Loss: 0.002319708000868559 | Testing Acc: 100.0\n",
            "Epoch: 2354 | Training Loss: 0.0012771690962836146 | Training Acc: 100.0 | Testing Loss: 0.002319679129868746 | Testing Acc: 100.0\n",
            "Epoch: 2355 | Training Loss: 0.0012764226412400603 | Training Acc: 100.0 | Testing Loss: 0.0023178905248641968 | Testing Acc: 100.0\n",
            "Epoch: 2356 | Training Loss: 0.0012756462674587965 | Training Acc: 100.0 | Testing Loss: 0.0023178216069936752 | Testing Acc: 100.0\n",
            "Epoch: 2357 | Training Loss: 0.0012749162269756198 | Training Acc: 100.0 | Testing Loss: 0.002316033001989126 | Testing Acc: 100.0\n",
            "Epoch: 2358 | Training Loss: 0.0012741389218717813 | Training Acc: 100.0 | Testing Loss: 0.0023143922444432974 | Testing Acc: 100.0\n",
            "Epoch: 2359 | Training Loss: 0.001273399218916893 | Training Acc: 100.0 | Testing Loss: 0.0023144143633544445 | Testing Acc: 100.0\n",
            "Epoch: 2360 | Training Loss: 0.001272636465728283 | Training Acc: 100.0 | Testing Loss: 0.002312671160325408 | Testing Acc: 100.0\n",
            "Epoch: 2361 | Training Loss: 0.001271883025765419 | Training Acc: 100.0 | Testing Loss: 0.0023126364685595036 | Testing Acc: 100.0\n",
            "Epoch: 2362 | Training Loss: 0.0012711433228105307 | Training Acc: 100.0 | Testing Loss: 0.002310898620635271 | Testing Acc: 100.0\n",
            "Epoch: 2363 | Training Loss: 0.001270374865271151 | Training Acc: 100.0 | Testing Loss: 0.0023108357563614845 | Testing Acc: 100.0\n",
            "Epoch: 2364 | Training Loss: 0.0012696434278041124 | Training Acc: 100.0 | Testing Loss: 0.0023090580943971872 | Testing Acc: 100.0\n",
            "Epoch: 2365 | Training Loss: 0.001268875552341342 | Training Acc: 100.0 | Testing Loss: 0.0023073996417224407 | Testing Acc: 100.0\n",
            "Epoch: 2366 | Training Loss: 0.0012681505177170038 | Training Acc: 100.0 | Testing Loss: 0.0023074676282703876 | Testing Acc: 100.0\n",
            "Epoch: 2367 | Training Loss: 0.0012673819437623024 | Training Acc: 100.0 | Testing Loss: 0.002305752132087946 | Testing Acc: 100.0\n",
            "Epoch: 2368 | Training Loss: 0.0012666392140090466 | Training Acc: 100.0 | Testing Loss: 0.002305723028257489 | Testing Acc: 100.0\n",
            "Epoch: 2369 | Training Loss: 0.0012658921768888831 | Training Acc: 100.0 | Testing Loss: 0.0023039854131639004 | Testing Acc: 100.0\n",
            "Epoch: 2370 | Training Loss: 0.001265138853341341 | Training Acc: 100.0 | Testing Loss: 0.002303944667801261 | Testing Acc: 100.0\n",
            "Epoch: 2371 | Training Loss: 0.0012644109083339572 | Training Acc: 100.0 | Testing Loss: 0.0023021784145385027 | Testing Acc: 100.0\n",
            "Epoch: 2372 | Training Loss: 0.0012636431492865086 | Training Acc: 100.0 | Testing Loss: 0.0023005143739283085 | Testing Acc: 100.0\n",
            "Epoch: 2373 | Training Loss: 0.001262924401089549 | Training Acc: 100.0 | Testing Loss: 0.002300565131008625 | Testing Acc: 100.0\n",
            "Epoch: 2374 | Training Loss: 0.0012621574569493532 | Training Acc: 100.0 | Testing Loss: 0.0022988549899309874 | Testing Acc: 100.0\n",
            "Epoch: 2375 | Training Loss: 0.001261424389667809 | Training Acc: 100.0 | Testing Loss: 0.002298883395269513 | Testing Acc: 100.0\n",
            "Epoch: 2376 | Training Loss: 0.0012606863165274262 | Training Acc: 100.0 | Testing Loss: 0.002297168131917715 | Testing Acc: 100.0\n",
            "Epoch: 2377 | Training Loss: 0.001259935088455677 | Training Acc: 100.0 | Testing Loss: 0.002297116443514824 | Testing Acc: 100.0\n",
            "Epoch: 2378 | Training Loss: 0.001259214011952281 | Training Acc: 100.0 | Testing Loss: 0.002295366721227765 | Testing Acc: 100.0\n",
            "Epoch: 2379 | Training Loss: 0.0012584497453644872 | Training Acc: 100.0 | Testing Loss: 0.0022952924482524395 | Testing Acc: 100.0\n",
            "Epoch: 2380 | Training Loss: 0.0012577429879456758 | Training Acc: 100.0 | Testing Loss: 0.0022935259621590376 | Testing Acc: 100.0\n",
            "Epoch: 2381 | Training Loss: 0.0012569852406159043 | Training Acc: 100.0 | Testing Loss: 0.002291884506121278 | Testing Acc: 100.0\n",
            "Epoch: 2382 | Training Loss: 0.001256264396943152 | Training Acc: 100.0 | Testing Loss: 0.002291895216330886 | Testing Acc: 100.0\n",
            "Epoch: 2383 | Training Loss: 0.0012555154971778393 | Training Acc: 100.0 | Testing Loss: 0.002290202770382166 | Testing Acc: 100.0\n",
            "Epoch: 2384 | Training Loss: 0.0012547810329124331 | Training Acc: 100.0 | Testing Loss: 0.002290190663188696 | Testing Acc: 100.0\n",
            "Epoch: 2385 | Training Loss: 0.0012540516909211874 | Training Acc: 100.0 | Testing Loss: 0.0022884635254740715 | Testing Acc: 100.0\n",
            "Epoch: 2386 | Training Loss: 0.0012533013941720128 | Training Acc: 100.0 | Testing Loss: 0.0022884460631757975 | Testing Acc: 100.0\n",
            "Epoch: 2387 | Training Loss: 0.0012525992933660746 | Training Acc: 100.0 | Testing Loss: 0.002286718925461173 | Testing Acc: 100.0\n",
            "Epoch: 2388 | Training Loss: 0.0012518393341451883 | Training Acc: 100.0 | Testing Loss: 0.0022850832901895046 | Testing Acc: 100.0\n",
            "Epoch: 2389 | Training Loss: 0.0012511222157627344 | Training Acc: 100.0 | Testing Loss: 0.0022850995883345604 | Testing Acc: 100.0\n",
            "Epoch: 2390 | Training Loss: 0.0012503871694207191 | Training Acc: 100.0 | Testing Loss: 0.002283435547724366 | Testing Acc: 100.0\n",
            "Epoch: 2391 | Training Loss: 0.0012496524723246694 | Training Acc: 100.0 | Testing Loss: 0.0022834003902971745 | Testing Acc: 100.0\n",
            "Epoch: 2392 | Training Loss: 0.0012489281361922622 | Training Acc: 100.0 | Testing Loss: 0.002281701425090432 | Testing Acc: 100.0\n",
            "Epoch: 2393 | Training Loss: 0.0012481885496526957 | Training Acc: 100.0 | Testing Loss: 0.002281655790284276 | Testing Acc: 100.0\n",
            "Epoch: 2394 | Training Loss: 0.0012474758550524712 | Training Acc: 100.0 | Testing Loss: 0.002279917011037469 | Testing Acc: 100.0\n",
            "Epoch: 2395 | Training Loss: 0.0012467224150896072 | Training Acc: 100.0 | Testing Loss: 0.002278366591781378 | Testing Acc: 100.0\n",
            "Epoch: 2396 | Training Loss: 0.0012460255529731512 | Training Acc: 100.0 | Testing Loss: 0.002278377301990986 | Testing Acc: 100.0\n",
            "Epoch: 2397 | Training Loss: 0.0012452795635908842 | Training Acc: 100.0 | Testing Loss: 0.0022767181508243084 | Testing Acc: 100.0\n",
            "Epoch: 2398 | Training Loss: 0.0012445643078535795 | Training Acc: 100.0 | Testing Loss: 0.0022767065092921257 | Testing Acc: 100.0\n",
            "Epoch: 2399 | Training Loss: 0.001243840204551816 | Training Acc: 100.0 | Testing Loss: 0.0022749961353838444 | Testing Acc: 100.0\n",
            "Epoch: 2400 | Training Loss: 0.0012431115610525012 | Training Acc: 100.0 | Testing Loss: 0.0022749551571905613 | Testing Acc: 100.0\n",
            "Epoch: 2401 | Training Loss: 0.0012423977022990584 | Training Acc: 100.0 | Testing Loss: 0.002273233374580741 | Testing Acc: 100.0\n",
            "Epoch: 2402 | Training Loss: 0.0012416562531143427 | Training Acc: 100.0 | Testing Loss: 0.002271625678986311 | Testing Acc: 100.0\n",
            "Epoch: 2403 | Training Loss: 0.0012409609043970704 | Training Acc: 100.0 | Testing Loss: 0.0022716533858329058 | Testing Acc: 100.0\n",
            "Epoch: 2404 | Training Loss: 0.0012402182910591364 | Training Acc: 100.0 | Testing Loss: 0.0022699995897710323 | Testing Acc: 100.0\n",
            "Epoch: 2405 | Training Loss: 0.0012395085068419576 | Training Acc: 100.0 | Testing Loss: 0.0022700221743434668 | Testing Acc: 100.0\n",
            "Epoch: 2406 | Training Loss: 0.0012387890601530671 | Training Acc: 100.0 | Testing Loss: 0.0022683576680719852 | Testing Acc: 100.0\n",
            "Epoch: 2407 | Training Loss: 0.0012380580883473158 | Training Acc: 100.0 | Testing Loss: 0.002268293872475624 | Testing Acc: 100.0\n",
            "Epoch: 2408 | Training Loss: 0.001237349584698677 | Training Acc: 100.0 | Testing Loss: 0.0022666058503091335 | Testing Acc: 100.0\n",
            "Epoch: 2409 | Training Loss: 0.0012366141891106963 | Training Acc: 100.0 | Testing Loss: 0.002266531577333808 | Testing Acc: 100.0\n",
            "Epoch: 2410 | Training Loss: 0.0012359216343611479 | Training Acc: 100.0 | Testing Loss: 0.0022648037411272526 | Testing Acc: 100.0\n",
            "Epoch: 2411 | Training Loss: 0.0012351814657449722 | Training Acc: 100.0 | Testing Loss: 0.0022631955798715353 | Testing Acc: 100.0\n",
            "Epoch: 2412 | Training Loss: 0.001234485418535769 | Training Acc: 100.0 | Testing Loss: 0.002263206522911787 | Testing Acc: 100.0\n",
            "Epoch: 2413 | Training Loss: 0.0012337580556049943 | Training Acc: 100.0 | Testing Loss: 0.0022615469060838223 | Testing Acc: 100.0\n",
            "Epoch: 2414 | Training Loss: 0.00123304664157331 | Training Acc: 100.0 | Testing Loss: 0.0022615808993577957 | Testing Acc: 100.0\n",
            "Epoch: 2415 | Training Loss: 0.0012323438422754407 | Training Acc: 100.0 | Testing Loss: 0.002259887056425214 | Testing Acc: 100.0\n",
            "Epoch: 2416 | Training Loss: 0.001231609145179391 | Training Acc: 100.0 | Testing Loss: 0.0022598295472562313 | Testing Acc: 100.0\n",
            "Epoch: 2417 | Training Loss: 0.0012309281155467033 | Training Acc: 100.0 | Testing Loss: 0.00225815293379128 | Testing Acc: 100.0\n",
            "Epoch: 2418 | Training Loss: 0.001230187132023275 | Training Acc: 100.0 | Testing Loss: 0.0022565440740436316 | Testing Acc: 100.0\n",
            "Epoch: 2419 | Training Loss: 0.0012294930638745427 | Training Acc: 100.0 | Testing Loss: 0.0022565494291484356 | Testing Acc: 100.0\n",
            "Epoch: 2420 | Training Loss: 0.0012287760619074106 | Training Acc: 100.0 | Testing Loss: 0.0022549175191670656 | Testing Acc: 100.0\n",
            "Epoch: 2421 | Training Loss: 0.001228066743351519 | Training Acc: 100.0 | Testing Loss: 0.0022548942361027002 | Testing Acc: 100.0\n",
            "Epoch: 2422 | Training Loss: 0.0012273669708520174 | Training Acc: 100.0 | Testing Loss: 0.0022532460279762745 | Testing Acc: 100.0\n",
            "Epoch: 2423 | Training Loss: 0.0012266432167962193 | Training Acc: 100.0 | Testing Loss: 0.0022532001603394747 | Testing Acc: 100.0\n",
            "Epoch: 2424 | Training Loss: 0.0012259564828127623 | Training Acc: 100.0 | Testing Loss: 0.0022515119053423405 | Testing Acc: 100.0\n",
            "Epoch: 2425 | Training Loss: 0.0012252292362973094 | Training Acc: 100.0 | Testing Loss: 0.002249994780868292 | Testing Acc: 100.0\n",
            "Epoch: 2426 | Training Loss: 0.0012245376128703356 | Training Acc: 100.0 | Testing Loss: 0.0022499938495457172 | Testing Acc: 100.0\n",
            "Epoch: 2427 | Training Loss: 0.0012238204944878817 | Training Acc: 100.0 | Testing Loss: 0.0022483740467578173 | Testing Acc: 100.0\n",
            "Epoch: 2428 | Training Loss: 0.0012231236323714256 | Training Acc: 100.0 | Testing Loss: 0.002248362172394991 | Testing Acc: 100.0\n",
            "Epoch: 2429 | Training Loss: 0.0012224202509969473 | Training Acc: 100.0 | Testing Loss: 0.0022466962691396475 | Testing Acc: 100.0\n",
            "Epoch: 2430 | Training Loss: 0.0012217091862112284 | Training Acc: 100.0 | Testing Loss: 0.002246656222268939 | Testing Acc: 100.0\n",
            "Epoch: 2431 | Training Loss: 0.0012210265267640352 | Training Acc: 100.0 | Testing Loss: 0.0022449905518442392 | Testing Acc: 100.0\n",
            "Epoch: 2432 | Training Loss: 0.0012202970683574677 | Training Acc: 100.0 | Testing Loss: 0.0022434217389672995 | Testing Acc: 100.0\n",
            "Epoch: 2433 | Training Loss: 0.001219617435708642 | Training Acc: 100.0 | Testing Loss: 0.0022434955462813377 | Testing Acc: 100.0\n",
            "Epoch: 2434 | Training Loss: 0.0012189045082777739 | Training Acc: 100.0 | Testing Loss: 0.002241875510662794 | Testing Acc: 100.0\n",
            "Epoch: 2435 | Training Loss: 0.0012182134669274092 | Training Acc: 100.0 | Testing Loss: 0.002241851994767785 | Testing Acc: 100.0\n",
            "Epoch: 2436 | Training Loss: 0.001217513345181942 | Training Acc: 100.0 | Testing Loss: 0.00224022613838315 | Testing Acc: 100.0\n",
            "Epoch: 2437 | Training Loss: 0.0012168129906058311 | Training Acc: 100.0 | Testing Loss: 0.0022401567548513412 | Testing Acc: 100.0\n",
            "Epoch: 2438 | Training Loss: 0.001216126256622374 | Training Acc: 100.0 | Testing Loss: 0.00223848526366055 | Testing Acc: 100.0\n",
            "Epoch: 2439 | Training Loss: 0.0012154045980423689 | Training Acc: 100.0 | Testing Loss: 0.002236944856122136 | Testing Acc: 100.0\n",
            "Epoch: 2440 | Training Loss: 0.0012147374218329787 | Training Acc: 100.0 | Testing Loss: 0.0022369674406945705 | Testing Acc: 100.0\n",
            "Epoch: 2441 | Training Loss: 0.0012140233302488923 | Training Acc: 100.0 | Testing Loss: 0.0022353525273501873 | Testing Acc: 100.0\n",
            "Epoch: 2442 | Training Loss: 0.0012133435811847448 | Training Acc: 100.0 | Testing Loss: 0.0022353979293257 | Testing Acc: 100.0\n",
            "Epoch: 2443 | Training Loss: 0.0012126376386731863 | Training Acc: 100.0 | Testing Loss: 0.00223376601934433 | Testing Acc: 100.0\n",
            "Epoch: 2444 | Training Loss: 0.001211940892972052 | Training Acc: 100.0 | Testing Loss: 0.002233719453215599 | Testing Acc: 100.0\n",
            "Epoch: 2445 | Training Loss: 0.0012112603290006518 | Training Acc: 100.0 | Testing Loss: 0.0022320644930005074 | Testing Acc: 100.0\n",
            "Epoch: 2446 | Training Loss: 0.001210550544783473 | Training Acc: 100.0 | Testing Loss: 0.0022319902200251818 | Testing Acc: 100.0\n",
            "Epoch: 2447 | Training Loss: 0.001209885347634554 | Training Acc: 100.0 | Testing Loss: 0.00223033525981009 | Testing Acc: 100.0\n",
            "Epoch: 2448 | Training Loss: 0.001209168927744031 | Training Acc: 100.0 | Testing Loss: 0.002228772034868598 | Testing Acc: 100.0\n",
            "Epoch: 2449 | Training Loss: 0.0012084967456758022 | Training Acc: 100.0 | Testing Loss: 0.0022287711035460234 | Testing Acc: 100.0\n",
            "Epoch: 2450 | Training Loss: 0.0012077938299626112 | Training Acc: 100.0 | Testing Loss: 0.002227185061201453 | Testing Acc: 100.0\n",
            "Epoch: 2451 | Training Loss: 0.001207109191454947 | Training Acc: 100.0 | Testing Loss: 0.0022271503694355488 | Testing Acc: 100.0\n",
            "Epoch: 2452 | Training Loss: 0.0012064238544553518 | Training Acc: 100.0 | Testing Loss: 0.002225518459454179 | Testing Acc: 100.0\n",
            "Epoch: 2453 | Training Loss: 0.0012057296698912978 | Training Acc: 100.0 | Testing Loss: 0.002225517760962248 | Testing Acc: 100.0\n",
            "Epoch: 2454 | Training Loss: 0.0012050599325448275 | Training Acc: 100.0 | Testing Loss: 0.0022238630335778 | Testing Acc: 100.0\n",
            "Epoch: 2455 | Training Loss: 0.0012043560855090618 | Training Acc: 100.0 | Testing Loss: 0.0022223335690796375 | Testing Acc: 100.0\n",
            "Epoch: 2456 | Training Loss: 0.0012036887928843498 | Training Acc: 100.0 | Testing Loss: 0.0022223331034183502 | Testing Acc: 100.0\n",
            "Epoch: 2457 | Training Loss: 0.0012029889039695263 | Training Acc: 100.0 | Testing Loss: 0.002220746362581849 | Testing Acc: 100.0\n",
            "Epoch: 2458 | Training Loss: 0.0012023079907521605 | Training Acc: 100.0 | Testing Loss: 0.0022207344882190228 | Testing Acc: 100.0\n",
            "Epoch: 2459 | Training Loss: 0.0012016333639621735 | Training Acc: 100.0 | Testing Loss: 0.0022191195748746395 | Testing Acc: 100.0\n",
            "Epoch: 2460 | Training Loss: 0.0012009380152449012 | Training Acc: 100.0 | Testing Loss: 0.0022190678864717484 | Testing Acc: 100.0\n",
            "Epoch: 2461 | Training Loss: 0.0012002799194306135 | Training Acc: 100.0 | Testing Loss: 0.002217440865933895 | Testing Acc: 100.0\n",
            "Epoch: 2462 | Training Loss: 0.001199575257487595 | Training Acc: 100.0 | Testing Loss: 0.00221594562754035 | Testing Acc: 100.0\n",
            "Epoch: 2463 | Training Loss: 0.0011989092454314232 | Training Acc: 100.0 | Testing Loss: 0.0022159391082823277 | Testing Acc: 100.0\n",
            "Epoch: 2464 | Training Loss: 0.001198217854835093 | Training Acc: 100.0 | Testing Loss: 0.0022143807727843523 | Testing Acc: 100.0\n",
            "Epoch: 2465 | Training Loss: 0.001197547884657979 | Training Acc: 100.0 | Testing Loss: 0.0022143516689538956 | Testing Acc: 100.0\n",
            "Epoch: 2466 | Training Loss: 0.0011968621984124184 | Training Acc: 100.0 | Testing Loss: 0.002212782623246312 | Testing Acc: 100.0\n",
            "Epoch: 2467 | Training Loss: 0.0011961836135014892 | Training Acc: 100.0 | Testing Loss: 0.0022127190604805946 | Testing Acc: 100.0\n",
            "Epoch: 2468 | Training Loss: 0.0011955139925703406 | Training Acc: 100.0 | Testing Loss: 0.002211109735071659 | Testing Acc: 100.0\n",
            "Epoch: 2469 | Training Loss: 0.0011948184110224247 | Training Acc: 100.0 | Testing Loss: 0.0022095851600170135 | Testing Acc: 100.0\n",
            "Epoch: 2470 | Training Loss: 0.001194171840324998 | Training Acc: 100.0 | Testing Loss: 0.0022096417378634214 | Testing Acc: 100.0\n",
            "Epoch: 2471 | Training Loss: 0.0011934780050069094 | Training Acc: 100.0 | Testing Loss: 0.0022080722264945507 | Testing Acc: 100.0\n",
            "Epoch: 2472 | Training Loss: 0.0011928086169064045 | Training Acc: 100.0 | Testing Loss: 0.0022080426570028067 | Testing Acc: 100.0\n",
            "Epoch: 2473 | Training Loss: 0.0011921339901164174 | Training Acc: 100.0 | Testing Loss: 0.002206461736932397 | Testing Acc: 100.0\n",
            "Epoch: 2474 | Training Loss: 0.0011914543574675918 | Training Acc: 100.0 | Testing Loss: 0.0022064039949327707 | Testing Acc: 100.0\n",
            "Epoch: 2475 | Training Loss: 0.0011907934676855803 | Training Acc: 100.0 | Testing Loss: 0.0022048053797334433 | Testing Acc: 100.0\n",
            "Epoch: 2476 | Training Loss: 0.0011901017278432846 | Training Acc: 100.0 | Testing Loss: 0.0022047131787985563 | Testing Acc: 100.0\n",
            "Epoch: 2477 | Training Loss: 0.0011894551571458578 | Training Acc: 100.0 | Testing Loss: 0.0022031038533896208 | Testing Acc: 100.0\n",
            "Epoch: 2478 | Training Loss: 0.0011887685395777225 | Training Acc: 100.0 | Testing Loss: 0.0022015846334397793 | Testing Acc: 100.0\n",
            "Epoch: 2479 | Training Loss: 0.0011881086975336075 | Training Acc: 100.0 | Testing Loss: 0.002201590221375227 | Testing Acc: 100.0\n",
            "Epoch: 2480 | Training Loss: 0.0011874318588525057 | Training Acc: 100.0 | Testing Loss: 0.0022000372409820557 | Testing Acc: 100.0\n",
            "Epoch: 2481 | Training Loss: 0.0011867646826431155 | Training Acc: 100.0 | Testing Loss: 0.002200048416852951 | Testing Acc: 100.0\n",
            "Epoch: 2482 | Training Loss: 0.0011861009988933802 | Training Acc: 100.0 | Testing Loss: 0.00219844956882298 | Testing Acc: 100.0\n",
            "Epoch: 2483 | Training Loss: 0.0011854174081236124 | Training Acc: 100.0 | Testing Loss: 0.002198409289121628 | Testing Acc: 100.0\n",
            "Epoch: 2484 | Training Loss: 0.001184777356684208 | Training Acc: 100.0 | Testing Loss: 0.002196787390857935 | Testing Acc: 100.0\n",
            "Epoch: 2485 | Training Loss: 0.0011840893421322107 | Training Acc: 100.0 | Testing Loss: 0.002195274457335472 | Testing Acc: 100.0\n",
            "Epoch: 2486 | Training Loss: 0.0011834343895316124 | Training Acc: 100.0 | Testing Loss: 0.0021953026298433542 | Testing Acc: 100.0\n",
            "Epoch: 2487 | Training Loss: 0.0011827597627416253 | Training Acc: 100.0 | Testing Loss: 0.0021937552373856306 | Testing Acc: 100.0\n",
            "Epoch: 2488 | Training Loss: 0.0011820975923910737 | Training Acc: 100.0 | Testing Loss: 0.002193714492022991 | Testing Acc: 100.0\n",
            "Epoch: 2489 | Training Loss: 0.0011814453173428774 | Training Acc: 100.0 | Testing Loss: 0.002192167332395911 | Testing Acc: 100.0\n",
            "Epoch: 2490 | Training Loss: 0.001180762192234397 | Training Acc: 100.0 | Testing Loss: 0.002192155923694372 | Testing Acc: 100.0\n",
            "Epoch: 2491 | Training Loss: 0.0011801160871982574 | Training Acc: 100.0 | Testing Loss: 0.002190562430769205 | Testing Acc: 100.0\n",
            "Epoch: 2492 | Training Loss: 0.0011794379679486156 | Training Acc: 100.0 | Testing Loss: 0.0021890660282224417 | Testing Acc: 100.0\n",
            "Epoch: 2493 | Training Loss: 0.0011787926778197289 | Training Acc: 100.0 | Testing Loss: 0.002189077204093337 | Testing Acc: 100.0\n",
            "Epoch: 2494 | Training Loss: 0.001178118633106351 | Training Acc: 100.0 | Testing Loss: 0.0021875405218452215 | Testing Acc: 100.0\n",
            "Epoch: 2495 | Training Loss: 0.0011774646118283272 | Training Acc: 100.0 | Testing Loss: 0.0021875174716115 | Testing Acc: 100.0\n",
            "Epoch: 2496 | Training Loss: 0.0011768063995987177 | Training Acc: 100.0 | Testing Loss: 0.002185946563258767 | Testing Acc: 100.0\n",
            "Epoch: 2497 | Training Loss: 0.0011761341011151671 | Training Acc: 100.0 | Testing Loss: 0.00218591233715415 | Testing Acc: 100.0\n",
            "Epoch: 2498 | Training Loss: 0.001175491139292717 | Training Acc: 100.0 | Testing Loss: 0.0021843304857611656 | Testing Acc: 100.0\n",
            "Epoch: 2499 | Training Loss: 0.001174813136458397 | Training Acc: 100.0 | Testing Loss: 0.0021828515455126762 | Testing Acc: 100.0\n",
            "Epoch: 2500 | Training Loss: 0.0011741786729544401 | Training Acc: 100.0 | Testing Loss: 0.002182919532060623 | Testing Acc: 100.0\n",
            "Epoch: 2501 | Training Loss: 0.0011735145235434175 | Training Acc: 100.0 | Testing Loss: 0.0021813767962157726 | Testing Acc: 100.0\n",
            "Epoch: 2502 | Training Loss: 0.0011728566605597734 | Training Acc: 100.0 | Testing Loss: 0.002181359101086855 | Testing Acc: 100.0\n",
            "Epoch: 2503 | Training Loss: 0.001172208576463163 | Training Acc: 100.0 | Testing Loss: 0.0021798117086291313 | Testing Acc: 100.0\n",
            "Epoch: 2504 | Training Loss: 0.0011715461732819676 | Training Acc: 100.0 | Testing Loss: 0.0021797597873955965 | Testing Acc: 100.0\n",
            "Epoch: 2505 | Training Loss: 0.0011709022801369429 | Training Acc: 100.0 | Testing Loss: 0.002178189344704151 | Testing Acc: 100.0\n",
            "Epoch: 2506 | Training Loss: 0.0011702359188348055 | Training Acc: 100.0 | Testing Loss: 0.002176726935431361 | Testing Acc: 100.0\n",
            "Epoch: 2507 | Training Loss: 0.0011696035508066416 | Training Acc: 100.0 | Testing Loss: 0.002176732523366809 | Testing Acc: 100.0\n",
            "Epoch: 2508 | Training Loss: 0.0011689323000609875 | Training Acc: 100.0 | Testing Loss: 0.002175229834392667 | Testing Acc: 100.0\n",
            "Epoch: 2509 | Training Loss: 0.0011682957410812378 | Training Acc: 100.0 | Testing Loss: 0.002175235189497471 | Testing Acc: 100.0\n",
            "Epoch: 2510 | Training Loss: 0.0011676378780975938 | Training Acc: 100.0 | Testing Loss: 0.0021736931521445513 | Testing Acc: 100.0\n",
            "Epoch: 2511 | Training Loss: 0.0011669802479445934 | Training Acc: 100.0 | Testing Loss: 0.002173658460378647 | Testing Acc: 100.0\n",
            "Epoch: 2512 | Training Loss: 0.0011663443874567747 | Training Acc: 100.0 | Testing Loss: 0.0021720933727920055 | Testing Acc: 100.0\n",
            "Epoch: 2513 | Training Loss: 0.001165672205388546 | Training Acc: 100.0 | Testing Loss: 0.0021706135012209415 | Testing Acc: 100.0\n",
            "Epoch: 2514 | Training Loss: 0.0011650478700175881 | Training Acc: 100.0 | Testing Loss: 0.002170630032196641 | Testing Acc: 100.0\n",
            "Epoch: 2515 | Training Loss: 0.0011643831385299563 | Training Acc: 100.0 | Testing Loss: 0.002169109880924225 | Testing Acc: 100.0\n",
            "Epoch: 2516 | Training Loss: 0.0011637501884251833 | Training Acc: 100.0 | Testing Loss: 0.002169086830690503 | Testing Acc: 100.0\n",
            "Epoch: 2517 | Training Loss: 0.0011630976805463433 | Training Acc: 100.0 | Testing Loss: 0.0021675669122487307 | Testing Acc: 100.0\n",
            "Epoch: 2518 | Training Loss: 0.0011624464532360435 | Training Acc: 100.0 | Testing Loss: 0.002167549217119813 | Testing Acc: 100.0\n",
            "Epoch: 2519 | Training Loss: 0.0011618133867159486 | Training Acc: 100.0 | Testing Loss: 0.002166001359000802 | Testing Acc: 100.0\n",
            "Epoch: 2520 | Training Loss: 0.0011611499357968569 | Training Acc: 100.0 | Testing Loss: 0.0021645561791956425 | Testing Acc: 100.0\n",
            "Epoch: 2521 | Training Loss: 0.0011605365434661508 | Training Acc: 100.0 | Testing Loss: 0.00216453755274415 | Testing Acc: 100.0\n",
            "Epoch: 2522 | Training Loss: 0.001159863779321313 | Training Acc: 100.0 | Testing Loss: 0.002163052326068282 | Testing Acc: 100.0\n",
            "Epoch: 2523 | Training Loss: 0.0011592378141358495 | Training Acc: 100.0 | Testing Loss: 0.0021630232222378254 | Testing Acc: 100.0\n",
            "Epoch: 2524 | Training Loss: 0.0011585906613618135 | Training Acc: 100.0 | Testing Loss: 0.0021615151781588793 | Testing Acc: 100.0\n",
            "Epoch: 2525 | Training Loss: 0.0011579415295273066 | Training Acc: 100.0 | Testing Loss: 0.002161468844860792 | Testing Acc: 100.0\n",
            "Epoch: 2526 | Training Loss: 0.001157313003204763 | Training Acc: 100.0 | Testing Loss: 0.0021599321626126766 | Testing Acc: 100.0\n",
            "Epoch: 2527 | Training Loss: 0.0011566596804186702 | Training Acc: 100.0 | Testing Loss: 0.0021598574239760637 | Testing Acc: 100.0\n",
            "Epoch: 2528 | Training Loss: 0.0011560374405235052 | Training Acc: 100.0 | Testing Loss: 0.002158320276066661 | Testing Acc: 100.0\n",
            "Epoch: 2529 | Training Loss: 0.0011553894728422165 | Training Acc: 100.0 | Testing Loss: 0.002156886039301753 | Testing Acc: 100.0\n",
            "Epoch: 2530 | Training Loss: 0.0011547524482011795 | Training Acc: 100.0 | Testing Loss: 0.0021568741649389267 | Testing Acc: 100.0\n",
            "Epoch: 2531 | Training Loss: 0.0011541100684553385 | Training Acc: 100.0 | Testing Loss: 0.0021553770639002323 | Testing Acc: 100.0\n",
            "Epoch: 2532 | Training Loss: 0.0011534778168424964 | Training Acc: 100.0 | Testing Loss: 0.0021553481929004192 | Testing Acc: 100.0\n",
            "Epoch: 2533 | Training Loss: 0.0011528471950441599 | Training Acc: 100.0 | Testing Loss: 0.0021538224536925554 | Testing Acc: 100.0\n",
            "Epoch: 2534 | Training Loss: 0.0011521951528266072 | Training Acc: 100.0 | Testing Loss: 0.0021537879947572947 | Testing Acc: 100.0\n",
            "Epoch: 2535 | Training Loss: 0.0011515782680362463 | Training Acc: 100.0 | Testing Loss: 0.0021522389724850655 | Testing Acc: 100.0\n",
            "Epoch: 2536 | Training Loss: 0.0011509234318509698 | Training Acc: 100.0 | Testing Loss: 0.0021507875062525272 | Testing Acc: 100.0\n",
            "Epoch: 2537 | Training Loss: 0.0011503092246130109 | Training Acc: 100.0 | Testing Loss: 0.0021508443169295788 | Testing Acc: 100.0\n",
            "Epoch: 2538 | Training Loss: 0.0011496650986373425 | Training Acc: 100.0 | Testing Loss: 0.00214935839176178 | Testing Acc: 100.0\n",
            "Epoch: 2539 | Training Loss: 0.0011490366887301207 | Training Acc: 100.0 | Testing Loss: 0.002149323932826519 | Testing Acc: 100.0\n",
            "Epoch: 2540 | Training Loss: 0.00114841153845191 | Training Acc: 100.0 | Testing Loss: 0.002147826598957181 | Testing Acc: 100.0\n",
            "Epoch: 2541 | Training Loss: 0.0011477574007585645 | Training Acc: 100.0 | Testing Loss: 0.0021477865520864725 | Testing Acc: 100.0\n",
            "Epoch: 2542 | Training Loss: 0.001147134811617434 | Training Acc: 100.0 | Testing Loss: 0.0021462426520884037 | Testing Acc: 100.0\n",
            "Epoch: 2543 | Training Loss: 0.0011464912677183747 | Training Acc: 100.0 | Testing Loss: 0.002144796308130026 | Testing Acc: 100.0\n",
            "Epoch: 2544 | Training Loss: 0.001145876944065094 | Training Acc: 100.0 | Testing Loss: 0.002144807716831565 | Testing Acc: 100.0\n",
            "Epoch: 2545 | Training Loss: 0.001145234564319253 | Training Acc: 100.0 | Testing Loss: 0.002143356017768383 | Testing Acc: 100.0\n",
            "Epoch: 2546 | Training Loss: 0.001144616981036961 | Training Acc: 100.0 | Testing Loss: 0.002143372781574726 | Testing Acc: 100.0\n",
            "Epoch: 2547 | Training Loss: 0.001143982051871717 | Training Acc: 100.0 | Testing Loss: 0.0021418638061732054 | Testing Acc: 100.0\n",
            "Epoch: 2548 | Training Loss: 0.0011433522449806333 | Training Acc: 100.0 | Testing Loss: 0.002141817705705762 | Testing Acc: 100.0\n",
            "Epoch: 2549 | Training Loss: 0.00114274094812572 | Training Acc: 100.0 | Testing Loss: 0.002140291268005967 | Testing Acc: 100.0\n",
            "Epoch: 2550 | Training Loss: 0.0011420904193073511 | Training Acc: 100.0 | Testing Loss: 0.002138862619176507 | Testing Acc: 100.0\n",
            "Epoch: 2551 | Training Loss: 0.0011414848268032074 | Training Acc: 100.0 | Testing Loss: 0.0021388791501522064 | Testing Acc: 100.0\n",
            "Epoch: 2552 | Training Loss: 0.0011408489663153887 | Training Acc: 100.0 | Testing Loss: 0.0021374099887907505 | Testing Acc: 100.0\n",
            "Epoch: 2553 | Training Loss: 0.0011402246309444308 | Training Acc: 100.0 | Testing Loss: 0.002137386705726385 | Testing Acc: 100.0\n",
            "Epoch: 2554 | Training Loss: 0.001139600994065404 | Training Acc: 100.0 | Testing Loss: 0.0021359180100262165 | Testing Acc: 100.0\n",
            "Epoch: 2555 | Training Loss: 0.001138967229053378 | Training Acc: 100.0 | Testing Loss: 0.002135865855962038 | Testing Acc: 100.0\n",
            "Epoch: 2556 | Training Loss: 0.001138355117291212 | Training Acc: 100.0 | Testing Loss: 0.0021343622356653214 | Testing Acc: 100.0\n",
            "Epoch: 2557 | Training Loss: 0.001137720188125968 | Training Acc: 100.0 | Testing Loss: 0.00213297875598073 | Testing Acc: 100.0\n",
            "Epoch: 2558 | Training Loss: 0.0011371166910976171 | Training Acc: 100.0 | Testing Loss: 0.0021329724695533514 | Testing Acc: 100.0\n",
            "Epoch: 2559 | Training Loss: 0.0011364814126864076 | Training Acc: 100.0 | Testing Loss: 0.0021315203048288822 | Testing Acc: 100.0\n",
            "Epoch: 2560 | Training Loss: 0.0011358679039403796 | Training Acc: 100.0 | Testing Loss: 0.002131497021764517 | Testing Acc: 100.0\n",
            "Epoch: 2561 | Training Loss: 0.0011352489236742258 | Training Acc: 100.0 | Testing Loss: 0.0021300283260643482 | Testing Acc: 100.0\n",
            "Epoch: 2562 | Training Loss: 0.0011346199316903949 | Training Acc: 100.0 | Testing Loss: 0.002129987580701709 | Testing Acc: 100.0\n",
            "Epoch: 2563 | Training Loss: 0.0011340115452185273 | Training Acc: 100.0 | Testing Loss: 0.0021284837275743484 | Testing Acc: 100.0\n",
            "Epoch: 2564 | Training Loss: 0.0011333741713315248 | Training Acc: 100.0 | Testing Loss: 0.002128414809703827 | Testing Acc: 100.0\n",
            "Epoch: 2565 | Training Loss: 0.0011327809188514948 | Training Acc: 100.0 | Testing Loss: 0.00212691118940711 | Testing Acc: 100.0\n",
            "Epoch: 2566 | Training Loss: 0.0011321504134684801 | Training Acc: 100.0 | Testing Loss: 0.002125521656125784 | Testing Acc: 100.0\n",
            "Epoch: 2567 | Training Loss: 0.0011315379524603486 | Training Acc: 100.0 | Testing Loss: 0.0021255160681903362 | Testing Acc: 100.0\n",
            "Epoch: 2568 | Training Loss: 0.0011309186229482293 | Training Acc: 100.0 | Testing Loss: 0.0021240694914013147 | Testing Acc: 100.0\n",
            "Epoch: 2569 | Training Loss: 0.00113029801286757 | Training Acc: 100.0 | Testing Loss: 0.0021240231581032276 | Testing Acc: 100.0\n",
            "Epoch: 2570 | Training Loss: 0.0011296961456537247 | Training Acc: 100.0 | Testing Loss: 0.0021225479431450367 | Testing Acc: 100.0\n",
            "Epoch: 2571 | Training Loss: 0.0011290644761174917 | Training Acc: 100.0 | Testing Loss: 0.0021224962547421455 | Testing Acc: 100.0\n",
            "Epoch: 2572 | Training Loss: 0.0011284704087302089 | Training Acc: 100.0 | Testing Loss: 0.0021210096310824156 | Testing Acc: 100.0\n",
            "Epoch: 2573 | Training Loss: 0.001127836061641574 | Training Acc: 100.0 | Testing Loss: 0.0021195970475673676 | Testing Acc: 100.0\n",
            "Epoch: 2574 | Training Loss: 0.0011272422270849347 | Training Acc: 100.0 | Testing Loss: 0.0021196480374783278 | Testing Acc: 100.0\n",
            "Epoch: 2575 | Training Loss: 0.0011266255751252174 | Training Acc: 100.0 | Testing Loss: 0.0021182014606893063 | Testing Acc: 100.0\n",
            "Epoch: 2576 | Training Loss: 0.0011260086903348565 | Training Acc: 100.0 | Testing Loss: 0.002118166536092758 | Testing Acc: 100.0\n",
            "Epoch: 2577 | Training Loss: 0.001125412411056459 | Training Acc: 100.0 | Testing Loss: 0.002116708317771554 | Testing Acc: 100.0\n",
            "Epoch: 2578 | Training Loss: 0.0011247816728428006 | Training Acc: 100.0 | Testing Loss: 0.002116656396538019 | Testing Acc: 100.0\n",
            "Epoch: 2579 | Training Loss: 0.0011241912143304944 | Training Acc: 100.0 | Testing Loss: 0.0021151690743863583 | Testing Acc: 100.0\n",
            "Epoch: 2580 | Training Loss: 0.0011235633864998817 | Training Acc: 100.0 | Testing Loss: 0.002113779541105032 | Testing Acc: 100.0\n",
            "Epoch: 2581 | Training Loss: 0.0011229695519432425 | Training Acc: 100.0 | Testing Loss: 0.002113779541105032 | Testing Acc: 100.0\n",
            "Epoch: 2582 | Training Loss: 0.0011223454494029284 | Training Acc: 100.0 | Testing Loss: 0.0021123667247593403 | Testing Acc: 100.0\n",
            "Epoch: 2583 | Training Loss: 0.0011217454448342323 | Training Acc: 100.0 | Testing Loss: 0.002112372312694788 | Testing Acc: 100.0\n",
            "Epoch: 2584 | Training Loss: 0.0011211363598704338 | Training Acc: 100.0 | Testing Loss: 0.002110925503075123 | Testing Acc: 100.0\n",
            "Epoch: 2585 | Training Loss: 0.0011205209884792566 | Training Acc: 100.0 | Testing Loss: 0.0021108731161803007 | Testing Acc: 100.0\n",
            "Epoch: 2586 | Training Loss: 0.0011199356522411108 | Training Acc: 100.0 | Testing Loss: 0.002109391847625375 | Testing Acc: 100.0\n",
            "Epoch: 2587 | Training Loss: 0.0011193158570677042 | Training Acc: 100.0 | Testing Loss: 0.0021080137230455875 | Testing Acc: 100.0\n",
            "Epoch: 2588 | Training Loss: 0.0011187189957126975 | Training Acc: 100.0 | Testing Loss: 0.002108024898916483 | Testing Acc: 100.0\n",
            "Epoch: 2589 | Training Loss: 0.00111811060924083 | Training Acc: 100.0 | Testing Loss: 0.0021066062618047 | Testing Acc: 100.0\n",
            "Epoch: 2590 | Training Loss: 0.0011175067629665136 | Training Acc: 100.0 | Testing Loss: 0.0021065715700387955 | Testing Acc: 100.0\n",
            "Epoch: 2591 | Training Loss: 0.001116905827075243 | Training Acc: 100.0 | Testing Loss: 0.0021051473449915648 | Testing Acc: 100.0\n",
            "Epoch: 2592 | Training Loss: 0.0011163000017404556 | Training Acc: 100.0 | Testing Loss: 0.0021050781942903996 | Testing Acc: 100.0\n",
            "Epoch: 2593 | Training Loss: 0.0011157036060467362 | Training Acc: 100.0 | Testing Loss: 0.0021036313846707344 | Testing Acc: 100.0\n",
            "Epoch: 2594 | Training Loss: 0.001115087536163628 | Training Acc: 100.0 | Testing Loss: 0.0021022872533649206 | Testing Acc: 100.0\n",
            "Epoch: 2595 | Training Loss: 0.0011145072057843208 | Training Acc: 100.0 | Testing Loss: 0.0021022867877036333 | Testing Acc: 100.0\n",
            "Epoch: 2596 | Training Loss: 0.0011138845002278686 | Training Acc: 100.0 | Testing Loss: 0.0021008853800594807 | Testing Acc: 100.0\n",
            "Epoch: 2597 | Training Loss: 0.0011133021907880902 | Training Acc: 100.0 | Testing Loss: 0.0021008565090596676 | Testing Acc: 100.0\n",
            "Epoch: 2598 | Training Loss: 0.0011127028847113252 | Training Acc: 100.0 | Testing Loss: 0.002099420642480254 | Testing Acc: 100.0\n",
            "Epoch: 2599 | Training Loss: 0.0011120917042717338 | Training Acc: 100.0 | Testing Loss: 0.002099380362778902 | Testing Acc: 100.0\n",
            "Epoch: 2600 | Training Loss: 0.001111506251618266 | Training Acc: 100.0 | Testing Loss: 0.002097915392369032 | Testing Acc: 100.0\n",
            "Epoch: 2601 | Training Loss: 0.001110892160795629 | Training Acc: 100.0 | Testing Loss: 0.0020965486764907837 | Testing Acc: 100.0\n",
            "Epoch: 2602 | Training Loss: 0.0011103098513558507 | Training Acc: 100.0 | Testing Loss: 0.002096593612805009 | Testing Acc: 100.0\n",
            "Epoch: 2603 | Training Loss: 0.001109706237912178 | Training Acc: 100.0 | Testing Loss: 0.0020951922051608562 | Testing Acc: 100.0\n",
            "Epoch: 2604 | Training Loss: 0.001109104952774942 | Training Acc: 100.0 | Testing Loss: 0.002095145406201482 | Testing Acc: 100.0\n",
            "Epoch: 2605 | Training Loss: 0.001108512980863452 | Training Acc: 100.0 | Testing Loss: 0.002093720715492964 | Testing Acc: 100.0\n",
            "Epoch: 2606 | Training Loss: 0.0011079128598794341 | Training Acc: 100.0 | Testing Loss: 0.0020936746150255203 | Testing Acc: 100.0\n",
            "Epoch: 2607 | Training Loss: 0.001107331132516265 | Training Acc: 100.0 | Testing Loss: 0.002092215698212385 | Testing Acc: 100.0\n",
            "Epoch: 2608 | Training Loss: 0.0011067198356613517 | Training Acc: 100.0 | Testing Loss: 0.0020908652804791927 | Testing Acc: 100.0\n",
            "Epoch: 2609 | Training Loss: 0.0011061455588787794 | Training Acc: 100.0 | Testing Loss: 0.002090858994051814 | Testing Acc: 100.0\n",
            "Epoch: 2610 | Training Loss: 0.0011055364739149809 | Training Acc: 100.0 | Testing Loss: 0.002089463174343109 | Testing Acc: 100.0\n",
            "Epoch: 2611 | Training Loss: 0.001104958588257432 | Training Acc: 100.0 | Testing Loss: 0.002089485991746187 | Testing Acc: 100.0\n",
            "Epoch: 2612 | Training Loss: 0.001104359165765345 | Training Acc: 100.0 | Testing Loss: 0.0020880610682070255 | Testing Acc: 100.0\n",
            "Epoch: 2613 | Training Loss: 0.0011037535732612014 | Training Acc: 100.0 | Testing Loss: 0.0020880033262073994 | Testing Acc: 100.0\n",
            "Epoch: 2614 | Training Loss: 0.0011031818576157093 | Training Acc: 100.0 | Testing Loss: 0.0020865844562649727 | Testing Acc: 100.0\n",
            "Epoch: 2615 | Training Loss: 0.0011025759158656001 | Training Acc: 100.0 | Testing Loss: 0.0020852170418947935 | Testing Acc: 100.0\n",
            "Epoch: 2616 | Training Loss: 0.0011020038509741426 | Training Acc: 100.0 | Testing Loss: 0.0020852163434028625 | Testing Acc: 100.0\n",
            "Epoch: 2617 | Training Loss: 0.001101406291127205 | Training Acc: 100.0 | Testing Loss: 0.0020838375203311443 | Testing Acc: 100.0\n",
            "Epoch: 2618 | Training Loss: 0.0011008235160261393 | Training Acc: 100.0 | Testing Loss: 0.0020838200580328703 | Testing Acc: 100.0\n",
            "Epoch: 2619 | Training Loss: 0.0011002253741025925 | Training Acc: 100.0 | Testing Loss: 0.002082423772662878 | Testing Acc: 100.0\n",
            "Epoch: 2620 | Training Loss: 0.001099645858630538 | Training Acc: 100.0 | Testing Loss: 0.0020823662634938955 | Testing Acc: 100.0\n",
            "Epoch: 2621 | Training Loss: 0.0010990637820214033 | Training Acc: 100.0 | Testing Loss: 0.0020809415727853775 | Testing Acc: 100.0\n",
            "Epoch: 2622 | Training Loss: 0.0010984556283801794 | Training Acc: 100.0 | Testing Loss: 0.002080918289721012 | Testing Acc: 100.0\n",
            "Epoch: 2623 | Training Loss: 0.0010979010257869959 | Training Acc: 100.0 | Testing Loss: 0.002079470083117485 | Testing Acc: 100.0\n",
            "Epoch: 2624 | Training Loss: 0.0010972956661134958 | Training Acc: 100.0 | Testing Loss: 0.002078108489513397 | Testing Acc: 100.0\n",
            "Epoch: 2625 | Training Loss: 0.0010967147536575794 | Training Acc: 100.0 | Testing Loss: 0.00207810802385211 | Testing Acc: 100.0\n",
            "Epoch: 2626 | Training Loss: 0.0010961249936372042 | Training Acc: 100.0 | Testing Loss: 0.002076711505651474 | Testing Acc: 100.0\n",
            "Epoch: 2627 | Training Loss: 0.0010955415200442076 | Training Acc: 100.0 | Testing Loss: 0.0020766709931194782 | Testing Acc: 100.0\n",
            "Epoch: 2628 | Training Loss: 0.001094965380616486 | Training Acc: 100.0 | Testing Loss: 0.0020752805285155773 | Testing Acc: 100.0\n",
            "Epoch: 2629 | Training Loss: 0.0010943695669993758 | Training Acc: 100.0 | Testing Loss: 0.0020752171985805035 | Testing Acc: 100.0\n",
            "Epoch: 2630 | Training Loss: 0.0010938094928860664 | Training Acc: 100.0 | Testing Loss: 0.002073785988613963 | Testing Acc: 100.0\n",
            "Epoch: 2631 | Training Loss: 0.0010932112345471978 | Training Acc: 100.0 | Testing Loss: 0.002072492614388466 | Testing Acc: 100.0\n",
            "Epoch: 2632 | Training Loss: 0.0010926479008048773 | Training Acc: 100.0 | Testing Loss: 0.0020724807400256395 | Testing Acc: 100.0\n",
            "Epoch: 2633 | Training Loss: 0.0010920572094619274 | Training Acc: 100.0 | Testing Loss: 0.0020711016841232777 | Testing Acc: 100.0\n",
            "Epoch: 2634 | Training Loss: 0.0010914735030382872 | Training Acc: 100.0 | Testing Loss: 0.0020710784010589123 | Testing Acc: 100.0\n",
            "Epoch: 2635 | Training Loss: 0.0010909026023000479 | Training Acc: 100.0 | Testing Loss: 0.0020696816500276327 | Testing Acc: 100.0\n",
            "Epoch: 2636 | Training Loss: 0.0010903102811425924 | Training Acc: 100.0 | Testing Loss: 0.002069618320092559 | Testing Acc: 100.0\n",
            "Epoch: 2637 | Training Loss: 0.0010897514875978231 | Training Acc: 100.0 | Testing Loss: 0.002068215748295188 | Testing Acc: 100.0\n",
            "Epoch: 2638 | Training Loss: 0.0010891559068113565 | Training Acc: 100.0 | Testing Loss: 0.0020668823271989822 | Testing Acc: 100.0\n",
            "Epoch: 2639 | Training Loss: 0.0010885896626859903 | Training Acc: 100.0 | Testing Loss: 0.0020669104997068644 | Testing Acc: 100.0\n",
            "Epoch: 2640 | Training Loss: 0.0010880109621211886 | Training Acc: 100.0 | Testing Loss: 0.0020655537955462933 | Testing Acc: 100.0\n",
            "Epoch: 2641 | Training Loss: 0.001087429584003985 | Training Acc: 100.0 | Testing Loss: 0.00206550769507885 | Testing Acc: 100.0\n",
            "Epoch: 2642 | Training Loss: 0.0010868607787415385 | Training Acc: 100.0 | Testing Loss: 0.0020641281735152006 | Testing Acc: 100.0\n",
            "Epoch: 2643 | Training Loss: 0.001086275209672749 | Training Acc: 100.0 | Testing Loss: 0.0020640764851123095 | Testing Acc: 100.0\n",
            "Epoch: 2644 | Training Loss: 0.0010857193265110254 | Training Acc: 100.0 | Testing Loss: 0.002062668325379491 | Testing Acc: 100.0\n",
            "Epoch: 2645 | Training Loss: 0.001085125608369708 | Training Acc: 100.0 | Testing Loss: 0.002061345847323537 | Testing Acc: 100.0\n",
            "Epoch: 2646 | Training Loss: 0.0010845630895346403 | Training Acc: 100.0 | Testing Loss: 0.0020613339729607105 | Testing Acc: 100.0\n",
            "Epoch: 2647 | Training Loss: 0.0010839840397238731 | Training Acc: 100.0 | Testing Loss: 0.002059971448034048 | Testing Acc: 100.0\n",
            "Epoch: 2648 | Training Loss: 0.0010834134882315993 | Training Acc: 100.0 | Testing Loss: 0.002059999853372574 | Testing Acc: 100.0\n",
            "Epoch: 2649 | Training Loss: 0.0010828488739207387 | Training Acc: 100.0 | Testing Loss: 0.002058608690276742 | Testing Acc: 100.0\n",
            "Epoch: 2650 | Training Loss: 0.0010822608601301908 | Training Acc: 100.0 | Testing Loss: 0.002058562822639942 | Testing Acc: 100.0\n",
            "Epoch: 2651 | Training Loss: 0.0010817063739523292 | Training Acc: 100.0 | Testing Loss: 0.0020571653731167316 | Testing Acc: 100.0\n",
            "Epoch: 2652 | Training Loss: 0.0010811162646859884 | Training Acc: 100.0 | Testing Loss: 0.00205583730712533 | Testing Acc: 100.0\n",
            "Epoch: 2653 | Training Loss: 0.0010805692290887237 | Training Acc: 100.0 | Testing Loss: 0.0020558370742946863 | Testing Acc: 100.0\n",
            "Epoch: 2654 | Training Loss: 0.001079987152479589 | Training Acc: 100.0 | Testing Loss: 0.0020544910803437233 | Testing Acc: 100.0\n",
            "Epoch: 2655 | Training Loss: 0.0010794175323098898 | Training Acc: 100.0 | Testing Loss: 0.0020544619765132666 | Testing Acc: 100.0\n",
            "Epoch: 2656 | Training Loss: 0.0010788514045998454 | Training Acc: 100.0 | Testing Loss: 0.002053088042885065 | Testing Acc: 100.0\n",
            "Epoch: 2657 | Training Loss: 0.0010782750323414803 | Training Acc: 100.0 | Testing Loss: 0.0020530535839498043 | Testing Acc: 100.0\n",
            "Epoch: 2658 | Training Loss: 0.0010777199640870094 | Training Acc: 100.0 | Testing Loss: 0.0020516561344265938 | Testing Acc: 100.0\n",
            "Epoch: 2659 | Training Loss: 0.0010771378874778748 | Training Acc: 100.0 | Testing Loss: 0.0020503909327089787 | Testing Acc: 100.0\n",
            "Epoch: 2660 | Training Loss: 0.00107658957131207 | Training Acc: 100.0 | Testing Loss: 0.0020503965206444263 | Testing Acc: 100.0\n",
            "Epoch: 2661 | Training Loss: 0.001076012966223061 | Training Acc: 100.0 | Testing Loss: 0.002049050759524107 | Testing Acc: 100.0\n",
            "Epoch: 2662 | Training Loss: 0.0010754481190815568 | Training Acc: 100.0 | Testing Loss: 0.0020490160677582026 | Testing Acc: 100.0\n",
            "Epoch: 2663 | Training Loss: 0.0010748908389359713 | Training Acc: 100.0 | Testing Loss: 0.0020476647187024355 | Testing Acc: 100.0\n",
            "Epoch: 2664 | Training Loss: 0.001074307831004262 | Training Acc: 100.0 | Testing Loss: 0.0020476127974689007 | Testing Acc: 100.0\n",
            "Epoch: 2665 | Training Loss: 0.0010737621923908591 | Training Acc: 100.0 | Testing Loss: 0.0020462151151150465 | Testing Acc: 100.0\n",
            "Epoch: 2666 | Training Loss: 0.0010731903603300452 | Training Acc: 100.0 | Testing Loss: 0.002044932683929801 | Testing Acc: 100.0\n",
            "Epoch: 2667 | Training Loss: 0.001072632148861885 | Training Acc: 100.0 | Testing Loss: 0.0020449613220989704 | Testing Acc: 100.0\n",
            "Epoch: 2668 | Training Loss: 0.001072057755663991 | Training Acc: 100.0 | Testing Loss: 0.0020436211489140987 | Testing Acc: 100.0\n",
            "Epoch: 2669 | Training Loss: 0.001071498729288578 | Training Acc: 100.0 | Testing Loss: 0.0020435978658497334 | Testing Acc: 100.0\n",
            "Epoch: 2670 | Training Loss: 0.0010709378402680159 | Training Acc: 100.0 | Testing Loss: 0.0020422288216650486 | Testing Acc: 100.0\n",
            "Epoch: 2671 | Training Loss: 0.0010703703155741096 | Training Acc: 100.0 | Testing Loss: 0.002042182721197605 | Testing Acc: 100.0\n",
            "Epoch: 2672 | Training Loss: 0.0010698274709284306 | Training Acc: 100.0 | Testing Loss: 0.002040826017037034 | Testing Acc: 100.0\n",
            "Epoch: 2673 | Training Loss: 0.0010692448122426867 | Training Acc: 100.0 | Testing Loss: 0.002039519837126136 | Testing Acc: 100.0\n",
            "Epoch: 2674 | Training Loss: 0.0010687032481655478 | Training Acc: 100.0 | Testing Loss: 0.0020395079627633095 | Testing Acc: 100.0\n",
            "Epoch: 2675 | Training Loss: 0.0010681358398869634 | Training Acc: 100.0 | Testing Loss: 0.0020382024813443422 | Testing Acc: 100.0\n",
            "Epoch: 2676 | Training Loss: 0.0010675776284188032 | Training Acc: 100.0 | Testing Loss: 0.0020381961949169636 | Testing Acc: 100.0\n",
            "Epoch: 2677 | Training Loss: 0.0010670236079022288 | Training Acc: 100.0 | Testing Loss: 0.002036856021732092 | Testing Acc: 100.0\n",
            "Epoch: 2678 | Training Loss: 0.0010664614383131266 | Training Acc: 100.0 | Testing Loss: 0.0020367924589663744 | Testing Acc: 100.0\n",
            "Epoch: 2679 | Training Loss: 0.0010659111430868506 | Training Acc: 100.0 | Testing Loss: 0.002035429235547781 | Testing Acc: 100.0\n",
            "Epoch: 2680 | Training Loss: 0.0010653346544131637 | Training Acc: 100.0 | Testing Loss: 0.002034140517935157 | Testing Acc: 100.0\n",
            "Epoch: 2681 | Training Loss: 0.0010648058960214257 | Training Acc: 100.0 | Testing Loss: 0.0020341461058706045 | Testing Acc: 100.0\n",
            "Epoch: 2682 | Training Loss: 0.001064241281710565 | Training Acc: 100.0 | Testing Loss: 0.002032834803685546 | Testing Acc: 100.0\n",
            "Epoch: 2683 | Training Loss: 0.0010636821389198303 | Training Acc: 100.0 | Testing Loss: 0.0020328115206211805 | Testing Acc: 100.0\n",
            "Epoch: 2684 | Training Loss: 0.0010631273034960032 | Training Acc: 100.0 | Testing Loss: 0.002031465293839574 | Testing Acc: 100.0\n",
            "Epoch: 2685 | Training Loss: 0.0010625639697536826 | Training Acc: 100.0 | Testing Loss: 0.0020314480643719435 | Testing Acc: 100.0\n",
            "Epoch: 2686 | Training Loss: 0.0010620227549225092 | Training Acc: 100.0 | Testing Loss: 0.0020300785545259714 | Testing Acc: 100.0\n",
            "Epoch: 2687 | Training Loss: 0.0010614576749503613 | Training Acc: 100.0 | Testing Loss: 0.0020287896040827036 | Testing Acc: 100.0\n",
            "Epoch: 2688 | Training Loss: 0.0010609263554215431 | Training Acc: 100.0 | Testing Loss: 0.0020287896040827036 | Testing Acc: 100.0\n",
            "Epoch: 2689 | Training Loss: 0.0010603616246953607 | Training Acc: 100.0 | Testing Loss: 0.002027489012107253 | Testing Acc: 100.0\n",
            "Epoch: 2690 | Training Loss: 0.001059813774190843 | Training Acc: 100.0 | Testing Loss: 0.0020274429116398096 | Testing Acc: 100.0\n",
            "Epoch: 2691 | Training Loss: 0.001059256261214614 | Training Acc: 100.0 | Testing Loss: 0.0020261139143258333 | Testing Acc: 100.0\n",
            "Epoch: 2692 | Training Loss: 0.001058704685419798 | Training Acc: 100.0 | Testing Loss: 0.0020260850433260202 | Testing Acc: 100.0\n",
            "Epoch: 2693 | Training Loss: 0.0010581670794636011 | Training Acc: 100.0 | Testing Loss: 0.0020247329957783222 | Testing Acc: 100.0\n",
            "Epoch: 2694 | Training Loss: 0.0010575964115560055 | Training Acc: 100.0 | Testing Loss: 0.002023449633270502 | Testing Acc: 100.0\n",
            "Epoch: 2695 | Training Loss: 0.0010570696322247386 | Training Acc: 100.0 | Testing Loss: 0.0020234952680766582 | Testing Acc: 100.0\n",
            "Epoch: 2696 | Training Loss: 0.0010565075790509582 | Training Acc: 100.0 | Testing Loss: 0.0020221893209964037 | Testing Acc: 100.0\n",
            "Epoch: 2697 | Training Loss: 0.0010559677612036467 | Training Acc: 100.0 | Testing Loss: 0.002022166270762682 | Testing Acc: 100.0\n",
            "Epoch: 2698 | Training Loss: 0.001055415952578187 | Training Acc: 100.0 | Testing Loss: 0.0020208368077874184 | Testing Acc: 100.0\n",
            "Epoch: 2699 | Training Loss: 0.0010548608843237162 | Training Acc: 100.0 | Testing Loss: 0.002020779298618436 | Testing Acc: 100.0\n",
            "Epoch: 2700 | Training Loss: 0.0010543267708271742 | Training Acc: 100.0 | Testing Loss: 0.0020194328390061855 | Testing Acc: 100.0\n",
            "Epoch: 2701 | Training Loss: 0.001053761807270348 | Training Acc: 100.0 | Testing Loss: 0.002018178114667535 | Testing Acc: 100.0\n",
            "Epoch: 2702 | Training Loss: 0.0010532343294471502 | Training Acc: 100.0 | Testing Loss: 0.002018148545175791 | Testing Acc: 100.0\n",
            "Epoch: 2703 | Training Loss: 0.001052668085321784 | Training Acc: 100.0 | Testing Loss: 0.002016853541135788 | Testing Acc: 100.0\n",
            "Epoch: 2704 | Training Loss: 0.001052133971825242 | Training Acc: 100.0 | Testing Loss: 0.0020168586634099483 | Testing Acc: 100.0\n",
            "Epoch: 2705 | Training Loss: 0.0010515893809497356 | Training Acc: 100.0 | Testing Loss: 0.0020155354868620634 | Testing Acc: 100.0\n",
            "Epoch: 2706 | Training Loss: 0.0010510378051549196 | Training Acc: 100.0 | Testing Loss: 0.002015471924096346 | Testing Acc: 100.0\n",
            "Epoch: 2707 | Training Loss: 0.0010505078826099634 | Training Acc: 100.0 | Testing Loss: 0.0020141543354839087 | Testing Acc: 100.0\n",
            "Epoch: 2708 | Training Loss: 0.0010499529307708144 | Training Acc: 100.0 | Testing Loss: 0.0020128823816776276 | Testing Acc: 100.0\n",
            "Epoch: 2709 | Training Loss: 0.0010494288289919496 | Training Acc: 100.0 | Testing Loss: 0.0020128819160163403 | Testing Acc: 100.0\n",
            "Epoch: 2710 | Training Loss: 0.0010488698026165366 | Training Acc: 100.0 | Testing Loss: 0.00201159855350852 | Testing Acc: 100.0\n",
            "Epoch: 2711 | Training Loss: 0.0010483380174264312 | Training Acc: 100.0 | Testing Loss: 0.002011569682508707 | Testing Acc: 100.0\n",
            "Epoch: 2712 | Training Loss: 0.0010477936593815684 | Training Acc: 100.0 | Testing Loss: 0.002010268857702613 | Testing Acc: 100.0\n",
            "Epoch: 2713 | Training Loss: 0.0010472516296431422 | Training Acc: 100.0 | Testing Loss: 0.002010251395404339 | Testing Acc: 100.0\n",
            "Epoch: 2714 | Training Loss: 0.0010467220563441515 | Training Acc: 100.0 | Testing Loss: 0.00200889864936471 | Testing Acc: 100.0\n",
            "Epoch: 2715 | Training Loss: 0.0010461632627993822 | Training Acc: 100.0 | Testing Loss: 0.002007660921663046 | Testing Acc: 100.0\n",
            "Epoch: 2716 | Training Loss: 0.001045647426508367 | Training Acc: 100.0 | Testing Loss: 0.002007649280130863 | Testing Acc: 100.0\n",
            "Epoch: 2717 | Training Loss: 0.0010450928239151835 | Training Acc: 100.0 | Testing Loss: 0.002006371272727847 | Testing Acc: 100.0\n",
            "Epoch: 2718 | Training Loss: 0.0010445615043863654 | Training Acc: 100.0 | Testing Loss: 0.0020063540432602167 | Testing Acc: 100.0\n",
            "Epoch: 2719 | Training Loss: 0.0010440223850309849 | Training Acc: 100.0 | Testing Loss: 0.002005041344091296 | Testing Acc: 100.0\n",
            "Epoch: 2720 | Training Loss: 0.001043479423969984 | Training Acc: 100.0 | Testing Loss: 0.0020049780141562223 | Testing Acc: 100.0\n",
            "Epoch: 2721 | Training Loss: 0.0010429495014250278 | Training Acc: 100.0 | Testing Loss: 0.0020036655478179455 | Testing Acc: 100.0\n",
            "Epoch: 2722 | Training Loss: 0.0010423988569527864 | Training Acc: 100.0 | Testing Loss: 0.0020036310888826847 | Testing Acc: 100.0\n",
            "Epoch: 2723 | Training Loss: 0.0010418867459520698 | Training Acc: 100.0 | Testing Loss: 0.0020022899843752384 | Testing Acc: 100.0\n",
            "Epoch: 2724 | Training Loss: 0.0010413379641249776 | Training Acc: 100.0 | Testing Loss: 0.0020010462030768394 | Testing Acc: 100.0\n",
            "Epoch: 2725 | Training Loss: 0.0010408135131001472 | Training Acc: 100.0 | Testing Loss: 0.002001022920012474 | Testing Acc: 100.0\n",
            "Epoch: 2726 | Training Loss: 0.0010402689222246408 | Training Acc: 100.0 | Testing Loss: 0.001999733503907919 | Testing Acc: 100.0\n",
            "Epoch: 2727 | Training Loss: 0.0010397362057119608 | Training Acc: 100.0 | Testing Loss: 0.0019997041672468185 | Testing Acc: 100.0\n",
            "Epoch: 2728 | Training Loss: 0.001039210008457303 | Training Acc: 100.0 | Testing Loss: 0.0019983977545052767 | Testing Acc: 100.0\n",
            "Epoch: 2729 | Training Loss: 0.0010386599460616708 | Training Acc: 100.0 | Testing Loss: 0.001998328138142824 | Testing Acc: 100.0\n",
            "Epoch: 2730 | Training Loss: 0.0010381517931818962 | Training Acc: 100.0 | Testing Loss: 0.0019970214925706387 | Testing Acc: 100.0\n",
            "Epoch: 2731 | Training Loss: 0.0010376039426773787 | Training Acc: 100.0 | Testing Loss: 0.0019957663025707006 | Testing Acc: 100.0\n",
            "Epoch: 2732 | Training Loss: 0.0010370800737291574 | Training Acc: 100.0 | Testing Loss: 0.0019958005286753178 | Testing Acc: 100.0\n",
            "Epoch: 2733 | Training Loss: 0.001036541536450386 | Training Acc: 100.0 | Testing Loss: 0.0019945395179092884 | Testing Acc: 100.0\n",
            "Epoch: 2734 | Training Loss: 0.0010360053274780512 | Training Acc: 100.0 | Testing Loss: 0.0019944931846112013 | Testing Acc: 100.0\n",
            "Epoch: 2735 | Training Loss: 0.00103548273909837 | Training Acc: 100.0 | Testing Loss: 0.0019931974820792675 | Testing Acc: 100.0\n",
            "Epoch: 2736 | Training Loss: 0.001034942688420415 | Training Acc: 100.0 | Testing Loss: 0.001993157435208559 | Testing Acc: 100.0\n",
            "Epoch: 2737 | Training Loss: 0.0010344281326979399 | Training Acc: 100.0 | Testing Loss: 0.001991844270378351 | Testing Acc: 100.0\n",
            "Epoch: 2738 | Training Loss: 0.0010338881984353065 | Training Acc: 100.0 | Testing Loss: 0.001990612130612135 | Testing Acc: 100.0\n",
            "Epoch: 2739 | Training Loss: 0.0010333744576200843 | Training Acc: 100.0 | Testing Loss: 0.0019906233064830303 | Testing Acc: 100.0\n",
            "Epoch: 2740 | Training Loss: 0.0010328323114663363 | Training Acc: 100.0 | Testing Loss: 0.0019893329590559006 | Testing Acc: 100.0\n",
            "Epoch: 2741 | Training Loss: 0.001032303785905242 | Training Acc: 100.0 | Testing Loss: 0.001989332726225257 | Testing Acc: 100.0\n",
            "Epoch: 2742 | Training Loss: 0.0010317872511222959 | Training Acc: 100.0 | Testing Loss: 0.001988071948289871 | Testing Acc: 100.0\n",
            "Epoch: 2743 | Training Loss: 0.0010312467347830534 | Training Acc: 100.0 | Testing Loss: 0.0019880086183547974 | Testing Acc: 100.0\n",
            "Epoch: 2744 | Training Loss: 0.0010307385819032788 | Training Acc: 100.0 | Testing Loss: 0.001986713381484151 | Testing Acc: 100.0\n",
            "Epoch: 2745 | Training Loss: 0.0010301986476406455 | Training Acc: 100.0 | Testing Loss: 0.00198548031039536 | Testing Acc: 100.0\n",
            "Epoch: 2746 | Training Loss: 0.0010296826949343085 | Training Acc: 100.0 | Testing Loss: 0.0019854684360325336 | Testing Acc: 100.0\n",
            "Epoch: 2747 | Training Loss: 0.001029154285788536 | Training Acc: 100.0 | Testing Loss: 0.0019842188339680433 | Testing Acc: 100.0\n",
            "Epoch: 2748 | Training Loss: 0.0010286257602274418 | Training Acc: 100.0 | Testing Loss: 0.001984201604500413 | Testing Acc: 100.0\n",
            "Epoch: 2749 | Training Loss: 0.0010281053837388754 | Training Acc: 100.0 | Testing Loss: 0.001982911489903927 | Testing Acc: 100.0\n",
            "Epoch: 2750 | Training Loss: 0.0010275698732584715 | Training Acc: 100.0 | Testing Loss: 0.001982888439670205 | Testing Acc: 100.0\n",
            "Epoch: 2751 | Training Loss: 0.0010270706843584776 | Training Acc: 100.0 | Testing Loss: 0.0019815980922430754 | Testing Acc: 100.0\n",
            "Epoch: 2752 | Training Loss: 0.0010265301680192351 | Training Acc: 100.0 | Testing Loss: 0.001980371307581663 | Testing Acc: 100.0\n",
            "Epoch: 2753 | Training Loss: 0.0010260252747684717 | Training Acc: 100.0 | Testing Loss: 0.001980365253984928 | Testing Acc: 100.0\n",
            "Epoch: 2754 | Training Loss: 0.001025493605993688 | Training Acc: 100.0 | Testing Loss: 0.001979132415726781 | Testing Acc: 100.0\n",
            "Epoch: 2755 | Training Loss: 0.0010249691549688578 | Training Acc: 100.0 | Testing Loss: 0.0019790860824286938 | Testing Acc: 100.0\n",
            "Epoch: 2756 | Training Loss: 0.00102445506490767 | Training Acc: 100.0 | Testing Loss: 0.0019778129644691944 | Testing Acc: 100.0\n",
            "Epoch: 2757 | Training Loss: 0.001023928401991725 | Training Acc: 100.0 | Testing Loss: 0.0019777726847678423 | Testing Acc: 100.0\n",
            "Epoch: 2758 | Training Loss: 0.0010234282817691565 | Training Acc: 100.0 | Testing Loss: 0.001976494211703539 | Testing Acc: 100.0\n",
            "Epoch: 2759 | Training Loss: 0.0010228852042928338 | Training Acc: 100.0 | Testing Loss: 0.0019752955995500088 | Testing Acc: 100.0\n",
            "Epoch: 2760 | Training Loss: 0.0010223775170743465 | Training Acc: 100.0 | Testing Loss: 0.001975312363356352 | Testing Acc: 100.0\n",
            "Epoch: 2761 | Training Loss: 0.001021858653984964 | Training Acc: 100.0 | Testing Loss: 0.001974050886929035 | Testing Acc: 100.0\n",
            "Epoch: 2762 | Training Loss: 0.0010213317582383752 | Training Acc: 100.0 | Testing Loss: 0.0019740222487598658 | Testing Acc: 100.0\n",
            "Epoch: 2763 | Training Loss: 0.001020829426124692 | Training Acc: 100.0 | Testing Loss: 0.001972772181034088 | Testing Acc: 100.0\n",
            "Epoch: 2764 | Training Loss: 0.001020293217152357 | Training Acc: 100.0 | Testing Loss: 0.0019727142062038183 | Testing Acc: 100.0\n",
            "Epoch: 2765 | Training Loss: 0.0010197965893894434 | Training Acc: 100.0 | Testing Loss: 0.001971441088244319 | Testing Acc: 100.0\n",
            "Epoch: 2766 | Training Loss: 0.0010192707413807511 | Training Acc: 100.0 | Testing Loss: 0.001970225479453802 | Testing Acc: 100.0\n",
            "Epoch: 2767 | Training Loss: 0.0010187611915171146 | Training Acc: 100.0 | Testing Loss: 0.0019702480640262365 | Testing Acc: 100.0\n",
            "Epoch: 2768 | Training Loss: 0.0010182388359680772 | Training Acc: 100.0 | Testing Loss: 0.0019689917098730803 | Testing Acc: 100.0\n",
            "Epoch: 2769 | Training Loss: 0.0010177234653383493 | Training Acc: 100.0 | Testing Loss: 0.0019689626060426235 | Testing Acc: 100.0\n",
            "Epoch: 2770 | Training Loss: 0.0010172177571803331 | Training Acc: 100.0 | Testing Loss: 0.0019677127711474895 | Testing Acc: 100.0\n",
            "Epoch: 2771 | Training Loss: 0.00101669377181679 | Training Acc: 100.0 | Testing Loss: 0.0019676608499139547 | Testing Acc: 100.0\n",
            "Epoch: 2772 | Training Loss: 0.0010162029648199677 | Training Acc: 100.0 | Testing Loss: 0.0019663930870592594 | Testing Acc: 100.0\n",
            "Epoch: 2773 | Training Loss: 0.0010156711796298623 | Training Acc: 100.0 | Testing Loss: 0.0019651888869702816 | Testing Acc: 100.0\n",
            "Epoch: 2774 | Training Loss: 0.0010151674505323172 | Training Acc: 100.0 | Testing Loss: 0.001965171191841364 | Testing Acc: 100.0\n",
            "Epoch: 2775 | Training Loss: 0.0010146533604711294 | Training Acc: 100.0 | Testing Loss: 0.0019639553502202034 | Testing Acc: 100.0\n",
            "Epoch: 2776 | Training Loss: 0.0010141346137970686 | Training Acc: 100.0 | Testing Loss: 0.001963926712051034 | Testing Acc: 100.0\n",
            "Epoch: 2777 | Training Loss: 0.001013627857901156 | Training Acc: 100.0 | Testing Loss: 0.001962670125067234 | Testing Acc: 100.0\n",
            "Epoch: 2778 | Training Loss: 0.0010131107410416007 | Training Acc: 100.0 | Testing Loss: 0.0019626528955996037 | Testing Acc: 100.0\n",
            "Epoch: 2779 | Training Loss: 0.001012618886306882 | Training Acc: 100.0 | Testing Loss: 0.0019613795448094606 | Testing Acc: 100.0\n",
            "Epoch: 2780 | Training Loss: 0.0010120903607457876 | Training Acc: 100.0 | Testing Loss: 0.0019601923413574696 | Testing Acc: 100.0\n",
            "Epoch: 2781 | Training Loss: 0.0010115980403497815 | Training Acc: 100.0 | Testing Loss: 0.001960174646228552 | Testing Acc: 100.0\n",
            "Epoch: 2782 | Training Loss: 0.0010110780131071806 | Training Acc: 100.0 | Testing Loss: 0.0019589411094784737 | Testing Acc: 100.0\n",
            "Epoch: 2783 | Training Loss: 0.0010105641558766365 | Training Acc: 100.0 | Testing Loss: 0.0019589061848819256 | Testing Acc: 100.0\n",
            "Epoch: 2784 | Training Loss: 0.001010066014714539 | Training Acc: 100.0 | Testing Loss: 0.001957678934559226 | Testing Acc: 100.0\n",
            "Epoch: 2785 | Training Loss: 0.0010095371399074793 | Training Acc: 100.0 | Testing Loss: 0.0019576270133256912 | Testing Acc: 100.0\n",
            "Epoch: 2786 | Training Loss: 0.0010090477298945189 | Training Acc: 100.0 | Testing Loss: 0.0019563823007047176 | Testing Acc: 100.0\n",
            "Epoch: 2787 | Training Loss: 0.0010085318936035037 | Training Acc: 100.0 | Testing Loss: 0.001955223735421896 | Testing Acc: 100.0\n",
            "Epoch: 2788 | Training Loss: 0.0010080330539494753 | Training Acc: 100.0 | Testing Loss: 0.0019552060402929783 | Testing Acc: 100.0\n",
            "Epoch: 2789 | Training Loss: 0.001007517334073782 | Training Acc: 100.0 | Testing Loss: 0.001954007428139448 | Testing Acc: 100.0\n",
            "Epoch: 2790 | Training Loss: 0.0010070176795125008 | Training Acc: 100.0 | Testing Loss: 0.001953978557139635 | Testing Acc: 100.0\n",
            "Epoch: 2791 | Training Loss: 0.001006521051749587 | Training Acc: 100.0 | Testing Loss: 0.0019527394324541092 | Testing Acc: 100.0\n",
            "Epoch: 2792 | Training Loss: 0.0010059925261884928 | Training Acc: 100.0 | Testing Loss: 0.0019526875112205744 | Testing Acc: 100.0\n",
            "Epoch: 2793 | Training Loss: 0.0010055114980787039 | Training Acc: 100.0 | Testing Loss: 0.001951425103470683 | Testing Acc: 100.0\n",
            "Epoch: 2794 | Training Loss: 0.0010049814591184258 | Training Acc: 100.0 | Testing Loss: 0.0019502375507727265 | Testing Acc: 100.0\n",
            "Epoch: 2795 | Training Loss: 0.001004498451948166 | Training Acc: 100.0 | Testing Loss: 0.0019502602517604828 | Testing Acc: 100.0\n",
            "Epoch: 2796 | Training Loss: 0.0010039962362498045 | Training Acc: 100.0 | Testing Loss: 0.001949061406776309 | Testing Acc: 100.0\n",
            "Epoch: 2797 | Training Loss: 0.0010034922743216157 | Training Acc: 100.0 | Testing Loss: 0.0019490265985950828 | Testing Acc: 100.0\n",
            "Epoch: 2798 | Training Loss: 0.0010029940167441964 | Training Acc: 100.0 | Testing Loss: 0.0019478047033771873 | Testing Acc: 100.0\n",
            "Epoch: 2799 | Training Loss: 0.001002481090836227 | Training Acc: 100.0 | Testing Loss: 0.0019477527821436524 | Testing Acc: 100.0\n",
            "Epoch: 2800 | Training Loss: 0.0010019930778071284 | Training Acc: 100.0 | Testing Loss: 0.0019464964279904962 | Testing Acc: 100.0\n",
            "Epoch: 2801 | Training Loss: 0.00100148085039109 | Training Acc: 100.0 | Testing Loss: 0.0019453199347481132 | Testing Acc: 100.0\n",
            "Epoch: 2802 | Training Loss: 0.0010009942343458533 | Training Acc: 100.0 | Testing Loss: 0.0019453199347481132 | Testing Acc: 100.0\n",
            "Epoch: 2803 | Training Loss: 0.00100047851447016 | Training Acc: 100.0 | Testing Loss: 0.0019441094482317567 | Testing Acc: 100.0\n",
            "Epoch: 2804 | Training Loss: 0.0009999822359532118 | Training Acc: 100.0 | Testing Loss: 0.001944114686921239 | Testing Acc: 100.0\n",
            "Epoch: 2805 | Training Loss: 0.0009994942229241133 | Training Acc: 100.0 | Testing Loss: 0.0019428867381066084 | Testing Acc: 100.0\n",
            "Epoch: 2806 | Training Loss: 0.000998974428512156 | Training Acc: 100.0 | Testing Loss: 0.001942840637639165 | Testing Acc: 100.0\n",
            "Epoch: 2807 | Training Loss: 0.0009984936332330108 | Training Acc: 100.0 | Testing Loss: 0.001941612921655178 | Testing Acc: 100.0\n",
            "Epoch: 2808 | Training Loss: 0.0009979837341234088 | Training Acc: 100.0 | Testing Loss: 0.0019404368940740824 | Testing Acc: 100.0\n",
            "Epoch: 2809 | Training Loss: 0.0009974961867555976 | Training Acc: 100.0 | Testing Loss: 0.0019404131453484297 | Testing Acc: 100.0\n",
            "Epoch: 2810 | Training Loss: 0.0009969950187951326 | Training Acc: 100.0 | Testing Loss: 0.0019392197718843818 | Testing Acc: 100.0\n",
            "Epoch: 2811 | Training Loss: 0.0009965011849999428 | Training Acc: 100.0 | Testing Loss: 0.0019391847308725119 | Testing Acc: 100.0\n",
            "Epoch: 2812 | Training Loss: 0.0009960087481886148 | Training Acc: 100.0 | Testing Loss: 0.0019379567820578814 | Testing Acc: 100.0\n",
            "Epoch: 2813 | Training Loss: 0.000995500828139484 | Training Acc: 100.0 | Testing Loss: 0.001937962369993329 | Testing Acc: 100.0\n",
            "Epoch: 2814 | Training Loss: 0.0009950302774086595 | Training Acc: 100.0 | Testing Loss: 0.0019367228960618377 | Testing Acc: 100.0\n",
            "Epoch: 2815 | Training Loss: 0.0009945149067789316 | Training Acc: 100.0 | Testing Loss: 0.0019355462864041328 | Testing Acc: 100.0\n",
            "Epoch: 2816 | Training Loss: 0.0009940301533788443 | Training Acc: 100.0 | Testing Loss: 0.0019355460535734892 | Testing Acc: 100.0\n",
            "Epoch: 2817 | Training Loss: 0.0009935314301401377 | Training Acc: 100.0 | Testing Loss: 0.0019343525636941195 | Testing Acc: 100.0\n",
            "Epoch: 2818 | Training Loss: 0.0009930410888046026 | Training Acc: 100.0 | Testing Loss: 0.0019343061139807105 | Testing Acc: 100.0\n",
            "Epoch: 2819 | Training Loss: 0.0009925499325618148 | Training Acc: 100.0 | Testing Loss: 0.0019331127405166626 | Testing Acc: 100.0\n",
            "Epoch: 2820 | Training Loss: 0.000992045970633626 | Training Acc: 100.0 | Testing Loss: 0.0019330608192831278 | Testing Acc: 100.0\n",
            "Epoch: 2821 | Training Loss: 0.000991570996120572 | Training Acc: 100.0 | Testing Loss: 0.0019318207632750273 | Testing Acc: 100.0\n",
            "Epoch: 2822 | Training Loss: 0.0009910690132528543 | Training Acc: 100.0 | Testing Loss: 0.0019306730246171355 | Testing Acc: 100.0\n",
            "Epoch: 2823 | Training Loss: 0.0009905851911753416 | Training Acc: 100.0 | Testing Loss: 0.0019306900212541223 | Testing Acc: 100.0\n",
            "Epoch: 2824 | Training Loss: 0.0009900842560455203 | Training Acc: 100.0 | Testing Loss: 0.0019295020028948784 | Testing Acc: 100.0\n",
            "Epoch: 2825 | Training Loss: 0.0009895878611132503 | Training Acc: 100.0 | Testing Loss: 0.0019294848898425698 | Testing Acc: 100.0\n",
            "Epoch: 2826 | Training Loss: 0.0009891081135720015 | Training Acc: 100.0 | Testing Loss: 0.0019282680004835129 | Testing Acc: 100.0\n",
            "Epoch: 2827 | Training Loss: 0.0009886104380711913 | Training Acc: 100.0 | Testing Loss: 0.001928216079249978 | Testing Acc: 100.0\n",
            "Epoch: 2828 | Training Loss: 0.0009881339501589537 | Training Acc: 100.0 | Testing Loss: 0.0019270051270723343 | Testing Acc: 100.0\n",
            "Epoch: 2829 | Training Loss: 0.0009876313852146268 | Training Acc: 100.0 | Testing Loss: 0.0019258575048297644 | Testing Acc: 100.0\n",
            "Epoch: 2830 | Training Loss: 0.00098715559579432 | Training Acc: 100.0 | Testing Loss: 0.0019258398097008467 | Testing Acc: 100.0\n",
            "Epoch: 2831 | Training Loss: 0.000986658618785441 | Training Acc: 100.0 | Testing Loss: 0.0019246746087446809 | Testing Acc: 100.0\n",
            "Epoch: 2832 | Training Loss: 0.0009861660655587912 | Training Acc: 100.0 | Testing Loss: 0.0019246626179665327 | Testing Acc: 100.0\n",
            "Epoch: 2833 | Training Loss: 0.0009856871329247952 | Training Acc: 100.0 | Testing Loss: 0.0019234633073210716 | Testing Acc: 100.0\n",
            "Epoch: 2834 | Training Loss: 0.000985190854407847 | Training Acc: 100.0 | Testing Loss: 0.00192341732326895 | Testing Acc: 100.0\n",
            "Epoch: 2835 | Training Loss: 0.0009847256587818265 | Training Acc: 100.0 | Testing Loss: 0.0019222060218453407 | Testing Acc: 100.0\n",
            "Epoch: 2836 | Training Loss: 0.0009842220460996032 | Training Acc: 100.0 | Testing Loss: 0.0019210580503568053 | Testing Acc: 100.0\n",
            "Epoch: 2837 | Training Loss: 0.0009837386896833777 | Training Acc: 100.0 | Testing Loss: 0.0019210638711228967 | Testing Acc: 100.0\n",
            "Epoch: 2838 | Training Loss: 0.0009832516079768538 | Training Acc: 100.0 | Testing Loss: 0.001919869682751596 | Testing Acc: 100.0\n",
            "Epoch: 2839 | Training Loss: 0.0009827654575929046 | Training Acc: 100.0 | Testing Loss: 0.001919834641739726 | Testing Acc: 100.0\n",
            "Epoch: 2840 | Training Loss: 0.0009822860592976213 | Training Acc: 100.0 | Testing Loss: 0.0019186462741345167 | Testing Acc: 100.0\n",
            "Epoch: 2841 | Training Loss: 0.0009817893151193857 | Training Acc: 100.0 | Testing Loss: 0.0019186290446668863 | Testing Acc: 100.0\n",
            "Epoch: 2842 | Training Loss: 0.0009813231881707907 | Training Acc: 100.0 | Testing Loss: 0.0019174352055415511 | Testing Acc: 100.0\n",
            "Epoch: 2843 | Training Loss: 0.0009808270260691643 | Training Acc: 100.0 | Testing Loss: 0.0019162807147949934 | Testing Acc: 100.0\n",
            "Epoch: 2844 | Training Loss: 0.0009803520515561104 | Training Acc: 100.0 | Testing Loss: 0.0019162691896781325 | Testing Acc: 100.0\n",
            "Epoch: 2845 | Training Loss: 0.0009798684623092413 | Training Acc: 100.0 | Testing Loss: 0.001915092347189784 | Testing Acc: 100.0\n",
            "Epoch: 2846 | Training Loss: 0.0009793806821107864 | Training Acc: 100.0 | Testing Loss: 0.0019150633597746491 | Testing Acc: 100.0\n",
            "Epoch: 2847 | Training Loss: 0.0009789066389203072 | Training Acc: 100.0 | Testing Loss: 0.0019138865172863007 | Testing Acc: 100.0\n",
            "Epoch: 2848 | Training Loss: 0.0009784132707864046 | Training Acc: 100.0 | Testing Loss: 0.0019138290081173182 | Testing Acc: 100.0\n",
            "Epoch: 2849 | Training Loss: 0.0009779543615877628 | Training Acc: 100.0 | Testing Loss: 0.0019126345869153738 | Testing Acc: 100.0\n",
            "Epoch: 2850 | Training Loss: 0.0009774561040103436 | Training Acc: 100.0 | Testing Loss: 0.0019115207251161337 | Testing Acc: 100.0\n",
            "Epoch: 2851 | Training Loss: 0.0009769892785698175 | Training Acc: 100.0 | Testing Loss: 0.0019115094328299165 | Testing Acc: 100.0\n",
            "Epoch: 2852 | Training Loss: 0.0009764996357262135 | Training Acc: 100.0 | Testing Loss: 0.001910343416966498 | Testing Acc: 100.0\n",
            "Epoch: 2853 | Training Loss: 0.0009760262328200042 | Training Acc: 100.0 | Testing Loss: 0.0019103087252005935 | Testing Acc: 100.0\n",
            "Epoch: 2854 | Training Loss: 0.0009755496867001057 | Training Acc: 100.0 | Testing Loss: 0.001909131882712245 | Testing Acc: 100.0\n",
            "Epoch: 2855 | Training Loss: 0.0009750562021508813 | Training Acc: 100.0 | Testing Loss: 0.0019090743735432625 | Testing Acc: 100.0\n",
            "Epoch: 2856 | Training Loss: 0.0009746000869199634 | Training Acc: 100.0 | Testing Loss: 0.0019078856566920877 | Testing Acc: 100.0\n",
            "Epoch: 2857 | Training Loss: 0.0009741121903061867 | Training Acc: 100.0 | Testing Loss: 0.0019067430403083563 | Testing Acc: 100.0\n",
            "Epoch: 2858 | Training Loss: 0.0009736405918374658 | Training Acc: 100.0 | Testing Loss: 0.0019067658577114344 | Testing Acc: 100.0\n",
            "Epoch: 2859 | Training Loss: 0.0009731644531711936 | Training Acc: 100.0 | Testing Loss: 0.0019056175369769335 | Testing Acc: 100.0\n",
            "Epoch: 2860 | Training Loss: 0.0009726837160997093 | Training Acc: 100.0 | Testing Loss: 0.0019055710872635245 | Testing Acc: 100.0\n",
            "Epoch: 2861 | Training Loss: 0.0009722145041450858 | Training Acc: 100.0 | Testing Loss: 0.0019043877255171537 | Testing Acc: 100.0\n",
            "Epoch: 2862 | Training Loss: 0.000971729401499033 | Training Acc: 100.0 | Testing Loss: 0.0019043416250497103 | Testing Acc: 100.0\n",
            "Epoch: 2863 | Training Loss: 0.0009712657774798572 | Training Acc: 100.0 | Testing Loss: 0.001903153257444501 | Testing Acc: 100.0\n",
            "Epoch: 2864 | Training Loss: 0.0009707752615213394 | Training Acc: 100.0 | Testing Loss: 0.0019020277541130781 | Testing Acc: 100.0\n",
            "Epoch: 2865 | Training Loss: 0.0009703155374154449 | Training Acc: 100.0 | Testing Loss: 0.0019020217005163431 | Testing Acc: 100.0\n",
            "Epoch: 2866 | Training Loss: 0.0009698340436443686 | Training Acc: 100.0 | Testing Loss: 0.001900861389003694 | Testing Acc: 100.0\n",
            "Epoch: 2867 | Training Loss: 0.0009693567408248782 | Training Acc: 100.0 | Testing Loss: 0.0019008268136531115 | Testing Acc: 100.0\n",
            "Epoch: 2868 | Training Loss: 0.0009688948048278689 | Training Acc: 100.0 | Testing Loss: 0.001899666734971106 | Testing Acc: 100.0\n",
            "Epoch: 2869 | Training Loss: 0.0009684112155809999 | Training Acc: 100.0 | Testing Loss: 0.0018996552098542452 | Testing Acc: 100.0\n",
            "Epoch: 2870 | Training Loss: 0.0009679508511908352 | Training Acc: 100.0 | Testing Loss: 0.0018984719645231962 | Testing Acc: 100.0\n",
            "Epoch: 2871 | Training Loss: 0.0009674687753431499 | Training Acc: 100.0 | Testing Loss: 0.0018973464611917734 | Testing Acc: 100.0\n",
            "Epoch: 2872 | Training Loss: 0.0009670074214227498 | Training Acc: 100.0 | Testing Loss: 0.001897323178127408 | Testing Acc: 100.0\n",
            "Epoch: 2873 | Training Loss: 0.0009665308753028512 | Training Acc: 100.0 | Testing Loss: 0.0018961804453283548 | Testing Acc: 100.0\n",
            "Epoch: 2874 | Training Loss: 0.0009660577634349465 | Training Acc: 100.0 | Testing Loss: 0.0018961632158607244 | Testing Acc: 100.0\n",
            "Epoch: 2875 | Training Loss: 0.0009655954199843109 | Training Acc: 100.0 | Testing Loss: 0.0018949797376990318 | Testing Acc: 100.0\n",
            "Epoch: 2876 | Training Loss: 0.0009651107829995453 | Training Acc: 100.0 | Testing Loss: 0.0018949394579976797 | Testing Acc: 100.0\n",
            "Epoch: 2877 | Training Loss: 0.0009646633407101035 | Training Acc: 100.0 | Testing Loss: 0.001893768087029457 | Testing Acc: 100.0\n",
            "Epoch: 2878 | Training Loss: 0.0009641767246648669 | Training Acc: 100.0 | Testing Loss: 0.001892682397738099 | Testing Acc: 100.0\n",
            "Epoch: 2879 | Training Loss: 0.0009637156617827713 | Training Acc: 100.0 | Testing Loss: 0.0018926591146737337 | Testing Acc: 100.0\n",
            "Epoch: 2880 | Training Loss: 0.0009632417932152748 | Training Acc: 100.0 | Testing Loss: 0.001891533494926989 | Testing Acc: 100.0\n",
            "Epoch: 2881 | Training Loss: 0.0009627713588997722 | Training Acc: 100.0 | Testing Loss: 0.0018914870452135801 | Testing Acc: 100.0\n",
            "Epoch: 2882 | Training Loss: 0.0009623078512959182 | Training Acc: 100.0 | Testing Loss: 0.001890326850116253 | Testing Acc: 100.0\n",
            "Epoch: 2883 | Training Loss: 0.0009618291514925659 | Training Acc: 100.0 | Testing Loss: 0.001889223582111299 | Testing Acc: 100.0\n",
            "Epoch: 2884 | Training Loss: 0.000961380370426923 | Training Acc: 100.0 | Testing Loss: 0.0018892294028773904 | Testing Acc: 100.0\n",
            "Epoch: 2885 | Training Loss: 0.0009608965483494103 | Training Acc: 100.0 | Testing Loss: 0.001888097613118589 | Testing Acc: 100.0\n",
            "Epoch: 2886 | Training Loss: 0.0009604425285942852 | Training Acc: 100.0 | Testing Loss: 0.0018881149590015411 | Testing Acc: 100.0\n",
            "Epoch: 2887 | Training Loss: 0.000959974539000541 | Training Acc: 100.0 | Testing Loss: 0.0018869662890210748 | Testing Acc: 100.0\n",
            "Epoch: 2888 | Training Loss: 0.0009594999137334526 | Training Acc: 100.0 | Testing Loss: 0.0018869198393076658 | Testing Acc: 100.0\n",
            "Epoch: 2889 | Training Loss: 0.0009590481640771031 | Training Acc: 100.0 | Testing Loss: 0.00188576546497643 | Testing Acc: 100.0\n",
            "Epoch: 2890 | Training Loss: 0.0009585718507878482 | Training Acc: 100.0 | Testing Loss: 0.0018846625462174416 | Testing Acc: 100.0\n",
            "Epoch: 2891 | Training Loss: 0.0009581235935911536 | Training Acc: 100.0 | Testing Loss: 0.0018846450839191675 | Testing Acc: 100.0\n",
            "Epoch: 2892 | Training Loss: 0.0009576530428603292 | Training Acc: 100.0 | Testing Loss: 0.001883524702861905 | Testing Acc: 100.0\n",
            "Epoch: 2893 | Training Loss: 0.0009571925620548427 | Training Acc: 100.0 | Testing Loss: 0.0018835017690435052 | Testing Acc: 100.0\n",
            "Epoch: 2894 | Training Loss: 0.0009567277738824487 | Training Acc: 100.0 | Testing Loss: 0.0018823586869984865 | Testing Acc: 100.0\n",
            "Epoch: 2895 | Training Loss: 0.0009562635095790029 | Training Acc: 100.0 | Testing Loss: 0.0018823525169864297 | Testing Acc: 100.0\n",
            "Epoch: 2896 | Training Loss: 0.0009558085585013032 | Training Acc: 100.0 | Testing Loss: 0.0018811920890584588 | Testing Acc: 100.0\n",
            "Epoch: 2897 | Training Loss: 0.0009553249692544341 | Training Acc: 100.0 | Testing Loss: 0.001880089403130114 | Testing Acc: 100.0\n",
            "Epoch: 2898 | Training Loss: 0.0009548797970637679 | Training Acc: 100.0 | Testing Loss: 0.001880077295936644 | Testing Acc: 100.0\n",
            "Epoch: 2899 | Training Loss: 0.0009544116328470409 | Training Acc: 100.0 | Testing Loss: 0.0018789572641253471 | Testing Acc: 100.0\n",
            "Epoch: 2900 | Training Loss: 0.000953956157900393 | Training Acc: 100.0 | Testing Loss: 0.001878928393125534 | Testing Acc: 100.0\n",
            "Epoch: 2901 | Training Loss: 0.0009534984128549695 | Training Acc: 100.0 | Testing Loss: 0.0018777906661853194 | Testing Acc: 100.0\n",
            "Epoch: 2902 | Training Loss: 0.0009530337410978973 | Training Acc: 100.0 | Testing Loss: 0.0018777325749397278 | Testing Acc: 100.0\n",
            "Epoch: 2903 | Training Loss: 0.0009525789064355195 | Training Acc: 100.0 | Testing Loss: 0.00187657808419317 | Testing Acc: 100.0\n",
            "Epoch: 2904 | Training Loss: 0.000952103000599891 | Training Acc: 100.0 | Testing Loss: 0.001875532791018486 | Testing Acc: 100.0\n",
            "Epoch: 2905 | Training Loss: 0.0009516632999293506 | Training Acc: 100.0 | Testing Loss: 0.0018755095079541206 | Testing Acc: 100.0\n",
            "Epoch: 2906 | Training Loss: 0.0009511910611763597 | Training Acc: 100.0 | Testing Loss: 0.0018743833061307669 | Testing Acc: 100.0\n",
            "Epoch: 2907 | Training Loss: 0.0009507384966127574 | Training Acc: 100.0 | Testing Loss: 0.0018743660766631365 | Testing Acc: 100.0\n",
            "Epoch: 2908 | Training Loss: 0.000950283370912075 | Training Acc: 100.0 | Testing Loss: 0.001873216824606061 | Testing Acc: 100.0\n",
            "Epoch: 2909 | Training Loss: 0.0009498160216026008 | Training Acc: 100.0 | Testing Loss: 0.0018731588497757912 | Testing Acc: 100.0\n",
            "Epoch: 2910 | Training Loss: 0.0009493714896962047 | Training Acc: 100.0 | Testing Loss: 0.001872027525678277 | Testing Acc: 100.0\n",
            "Epoch: 2911 | Training Loss: 0.0009489039075560868 | Training Acc: 100.0 | Testing Loss: 0.0018709416035562754 | Testing Acc: 100.0\n",
            "Epoch: 2912 | Training Loss: 0.0009484570473432541 | Training Acc: 100.0 | Testing Loss: 0.0018709407886490226 | Testing Acc: 100.0\n",
            "Epoch: 2913 | Training Loss: 0.0009479938307777047 | Training Acc: 100.0 | Testing Loss: 0.001869849511422217 | Testing Acc: 100.0\n",
            "Epoch: 2914 | Training Loss: 0.0009475451661273837 | Training Acc: 100.0 | Testing Loss: 0.0018697971245273948 | Testing Acc: 100.0\n",
            "Epoch: 2915 | Training Loss: 0.0009470859658904374 | Training Acc: 100.0 | Testing Loss: 0.0018686593975871801 | Testing Acc: 100.0\n",
            "Epoch: 2916 | Training Loss: 0.0009466246701776981 | Training Acc: 100.0 | Testing Loss: 0.0018686248222365975 | Testing Acc: 100.0\n",
            "Epoch: 2917 | Training Loss: 0.0009461884619668126 | Training Acc: 100.0 | Testing Loss: 0.0018674756865948439 | Testing Acc: 100.0\n",
            "Epoch: 2918 | Training Loss: 0.000945715350098908 | Training Acc: 100.0 | Testing Loss: 0.0018663955852389336 | Testing Acc: 100.0\n",
            "Epoch: 2919 | Training Loss: 0.0009452787926420569 | Training Acc: 100.0 | Testing Loss: 0.0018663837108761072 | Testing Acc: 100.0\n",
            "Epoch: 2920 | Training Loss: 0.0009448133641853929 | Training Acc: 100.0 | Testing Loss: 0.001865274622105062 | Testing Acc: 100.0\n",
            "Epoch: 2921 | Training Loss: 0.0009443621383979917 | Training Acc: 100.0 | Testing Loss: 0.0018652686849236488 | Testing Acc: 100.0\n",
            "Epoch: 2922 | Training Loss: 0.0009439090499654412 | Training Acc: 100.0 | Testing Loss: 0.001864153891801834 | Testing Acc: 100.0\n",
            "Epoch: 2923 | Training Loss: 0.0009434530511498451 | Training Acc: 100.0 | Testing Loss: 0.0018641019705682993 | Testing Acc: 100.0\n",
            "Epoch: 2924 | Training Loss: 0.0009430099162273109 | Training Acc: 100.0 | Testing Loss: 0.001862958655692637 | Testing Acc: 100.0\n",
            "Epoch: 2925 | Training Loss: 0.0009425501339137554 | Training Acc: 100.0 | Testing Loss: 0.0018618839094415307 | Testing Acc: 100.0\n",
            "Epoch: 2926 | Training Loss: 0.0009421086870133877 | Training Acc: 100.0 | Testing Loss: 0.001861866214312613 | Testing Acc: 100.0\n",
            "Epoch: 2927 | Training Loss: 0.0009416479733772576 | Training Acc: 100.0 | Testing Loss: 0.0018607627134770155 | Testing Acc: 100.0\n",
            "Epoch: 2928 | Training Loss: 0.0009412012877874076 | Training Acc: 100.0 | Testing Loss: 0.0018607223173603415 | Testing Acc: 100.0\n",
            "Epoch: 2929 | Training Loss: 0.000940762460231781 | Training Acc: 100.0 | Testing Loss: 0.0018596075242385268 | Testing Acc: 100.0\n",
            "Epoch: 2930 | Training Loss: 0.0009403006988577545 | Training Acc: 100.0 | Testing Loss: 0.0018595553701743484 | Testing Acc: 100.0\n",
            "Epoch: 2931 | Training Loss: 0.0009398603579029441 | Training Acc: 100.0 | Testing Loss: 0.001858446397818625 | Testing Acc: 100.0\n",
            "Epoch: 2932 | Training Loss: 0.0009394001099281013 | Training Acc: 100.0 | Testing Loss: 0.0018573947018012404 | Testing Acc: 100.0\n",
            "Epoch: 2933 | Training Loss: 0.0009389608167111874 | Training Acc: 100.0 | Testing Loss: 0.0018573652487248182 | Testing Acc: 100.0\n",
            "Epoch: 2934 | Training Loss: 0.000938509008847177 | Training Acc: 100.0 | Testing Loss: 0.0018562618643045425 | Testing Acc: 100.0\n",
            "Epoch: 2935 | Training Loss: 0.0009380606934428215 | Training Acc: 100.0 | Testing Loss: 0.0018562389304861426 | Testing Acc: 100.0\n",
            "Epoch: 2936 | Training Loss: 0.000937617092859 | Training Acc: 100.0 | Testing Loss: 0.001855135546065867 | Testing Acc: 100.0\n",
            "Epoch: 2937 | Training Loss: 0.0009371601045131683 | Training Acc: 100.0 | Testing Loss: 0.0018550719833001494 | Testing Acc: 100.0\n",
            "Epoch: 2938 | Training Loss: 0.0009367285529151559 | Training Acc: 100.0 | Testing Loss: 0.0018539626616984606 | Testing Acc: 100.0\n",
            "Epoch: 2939 | Training Loss: 0.0009362682467326522 | Training Acc: 100.0 | Testing Loss: 0.0018528880318626761 | Testing Acc: 100.0\n",
            "Epoch: 2940 | Training Loss: 0.0009358272072859108 | Training Acc: 100.0 | Testing Loss: 0.0018528992077335715 | Testing Acc: 100.0\n",
            "Epoch: 2941 | Training Loss: 0.0009353799978271127 | Training Acc: 100.0 | Testing Loss: 0.0018518244614824653 | Testing Acc: 100.0\n",
            "Epoch: 2942 | Training Loss: 0.0009349401807412505 | Training Acc: 100.0 | Testing Loss: 0.0018517898861318827 | Testing Acc: 100.0\n",
            "Epoch: 2943 | Training Loss: 0.0009344954160042107 | Training Acc: 100.0 | Testing Loss: 0.0018506745109334588 | Testing Acc: 100.0\n",
            "Epoch: 2944 | Training Loss: 0.0009340443648397923 | Training Acc: 100.0 | Testing Loss: 0.0018506164196878672 | Testing Acc: 100.0\n",
            "Epoch: 2945 | Training Loss: 0.0009336167713627219 | Training Acc: 100.0 | Testing Loss: 0.0018494954565539956 | Testing Acc: 100.0\n",
            "Epoch: 2946 | Training Loss: 0.0009331569308415055 | Training Acc: 100.0 | Testing Loss: 0.0018484322354197502 | Testing Acc: 100.0\n",
            "Epoch: 2947 | Training Loss: 0.0009327275911346078 | Training Acc: 100.0 | Testing Loss: 0.0018484320025891066 | Testing Acc: 100.0\n",
            "Epoch: 2948 | Training Loss: 0.0009322765981778502 | Training Acc: 100.0 | Testing Loss: 0.0018473280360922217 | Testing Acc: 100.0\n",
            "Epoch: 2949 | Training Loss: 0.000931840215343982 | Training Acc: 100.0 | Testing Loss: 0.0018473161617293954 | Testing Acc: 100.0\n",
            "Epoch: 2950 | Training Loss: 0.0009313911432400346 | Training Acc: 100.0 | Testing Loss: 0.0018462244188413024 | Testing Acc: 100.0\n",
            "Epoch: 2951 | Training Loss: 0.0009309441666118801 | Training Acc: 100.0 | Testing Loss: 0.0018461665604263544 | Testing Acc: 100.0\n",
            "Epoch: 2952 | Training Loss: 0.0009305168641731143 | Training Acc: 100.0 | Testing Loss: 0.0018450455972924829 | Testing Acc: 100.0\n",
            "Epoch: 2953 | Training Loss: 0.0009300628444179893 | Training Acc: 100.0 | Testing Loss: 0.0018439991399645805 | Testing Acc: 100.0\n",
            "Epoch: 2954 | Training Loss: 0.0009296347270719707 | Training Acc: 100.0 | Testing Loss: 0.0018439877312630415 | Testing Acc: 100.0\n",
            "Epoch: 2955 | Training Loss: 0.000929188565351069 | Training Acc: 100.0 | Testing Loss: 0.0018428951734676957 | Testing Acc: 100.0\n",
            "Epoch: 2956 | Training Loss: 0.0009287431603297591 | Training Acc: 100.0 | Testing Loss: 0.0018428722396492958 | Testing Acc: 100.0\n",
            "Epoch: 2957 | Training Loss: 0.0009283110266551375 | Training Acc: 100.0 | Testing Loss: 0.0018417680403217673 | Testing Acc: 100.0\n",
            "Epoch: 2958 | Training Loss: 0.0009278616053052247 | Training Acc: 100.0 | Testing Loss: 0.0018417449900880456 | Testing Acc: 100.0\n",
            "Epoch: 2959 | Training Loss: 0.0009274404728785157 | Training Acc: 100.0 | Testing Loss: 0.0018406470771878958 | Testing Acc: 100.0\n",
            "Epoch: 2960 | Training Loss: 0.0009269829606637359 | Training Acc: 100.0 | Testing Loss: 0.0018395775696262717 | Testing Acc: 100.0\n",
            "Epoch: 2961 | Training Loss: 0.0009265522239729762 | Training Acc: 100.0 | Testing Loss: 0.0018395602237433195 | Testing Acc: 100.0\n",
            "Epoch: 2962 | Training Loss: 0.0009261133382096887 | Training Acc: 100.0 | Testing Loss: 0.0018384965369477868 | Testing Acc: 100.0\n",
            "Epoch: 2963 | Training Loss: 0.000925676547922194 | Training Acc: 100.0 | Testing Loss: 0.0018384561408311129 | Testing Acc: 100.0\n",
            "Epoch: 2964 | Training Loss: 0.0009252418531104922 | Training Acc: 100.0 | Testing Loss: 0.001837357645854354 | Testing Acc: 100.0\n",
            "Epoch: 2965 | Training Loss: 0.0009247936541214585 | Training Acc: 100.0 | Testing Loss: 0.0018373116618022323 | Testing Acc: 100.0\n",
            "Epoch: 2966 | Training Loss: 0.0009243732201866806 | Training Acc: 100.0 | Testing Loss: 0.0018362135160714388 | Testing Acc: 100.0\n",
            "Epoch: 2967 | Training Loss: 0.0009239250794053078 | Training Acc: 100.0 | Testing Loss: 0.0018351782346144319 | Testing Acc: 100.0\n",
            "Epoch: 2968 | Training Loss: 0.0009234976023435593 | Training Acc: 100.0 | Testing Loss: 0.0018351611215621233 | Testing Acc: 100.0\n",
            "Epoch: 2969 | Training Loss: 0.0009230586001649499 | Training Acc: 100.0 | Testing Loss: 0.0018341090762987733 | Testing Acc: 100.0\n",
            "Epoch: 2970 | Training Loss: 0.0009226157562807202 | Training Acc: 100.0 | Testing Loss: 0.0018340509850531816 | Testing Acc: 100.0\n",
            "Epoch: 2971 | Training Loss: 0.0009221885120496154 | Training Acc: 100.0 | Testing Loss: 0.0018329700687900186 | Testing Acc: 100.0\n",
            "Epoch: 2972 | Training Loss: 0.000921746133826673 | Training Acc: 100.0 | Testing Loss: 0.0018319465452805161 | Testing Acc: 100.0\n",
            "Epoch: 2973 | Training Loss: 0.0009213266894221306 | Training Acc: 100.0 | Testing Loss: 0.001831940608099103 | Testing Acc: 100.0\n",
            "Epoch: 2974 | Training Loss: 0.0009208790143020451 | Training Acc: 100.0 | Testing Loss: 0.0018308826256543398 | Testing Acc: 100.0\n",
            "Epoch: 2975 | Training Loss: 0.0009204571251757443 | Training Acc: 100.0 | Testing Loss: 0.0018308997387066483 | Testing Acc: 100.0\n",
            "Epoch: 2976 | Training Loss: 0.0009200209751725197 | Training Acc: 100.0 | Testing Loss: 0.0018298185896128416 | Testing Acc: 100.0\n",
            "Epoch: 2977 | Training Loss: 0.0009195858729071915 | Training Acc: 100.0 | Testing Loss: 0.0018297666683793068 | Testing Acc: 100.0\n",
            "Epoch: 2978 | Training Loss: 0.0009191584540531039 | Training Acc: 100.0 | Testing Loss: 0.0018286792328581214 | Testing Acc: 100.0\n",
            "Epoch: 2979 | Training Loss: 0.0009187143296003342 | Training Acc: 100.0 | Testing Loss: 0.0018276501214131713 | Testing Acc: 100.0\n",
            "Epoch: 2980 | Training Loss: 0.0009182997164316475 | Training Acc: 100.0 | Testing Loss: 0.0018276444170624018 | Testing Acc: 100.0\n",
            "Epoch: 2981 | Training Loss: 0.0009178548352792859 | Training Acc: 100.0 | Testing Loss: 0.0018265802646055818 | Testing Acc: 100.0\n",
            "Epoch: 2982 | Training Loss: 0.0009174252045340836 | Training Acc: 100.0 | Testing Loss: 0.0018265455728396773 | Testing Acc: 100.0\n",
            "Epoch: 2983 | Training Loss: 0.0009169980185106397 | Training Acc: 100.0 | Testing Loss: 0.0018254701280966401 | Testing Acc: 100.0\n",
            "Epoch: 2984 | Training Loss: 0.00091656728181988 | Training Acc: 100.0 | Testing Loss: 0.0018254583701491356 | Testing Acc: 100.0\n",
            "Epoch: 2985 | Training Loss: 0.0009161427733488381 | Training Acc: 100.0 | Testing Loss: 0.0018243774538859725 | Testing Acc: 100.0\n",
            "Epoch: 2986 | Training Loss: 0.0009156974265351892 | Training Acc: 100.0 | Testing Loss: 0.0018233424052596092 | Testing Acc: 100.0\n",
            "Epoch: 2987 | Training Loss: 0.000915288575924933 | Training Acc: 100.0 | Testing Loss: 0.0018233305308967829 | Testing Acc: 100.0\n",
            "Epoch: 2988 | Training Loss: 0.0009148478275164962 | Training Acc: 100.0 | Testing Loss: 0.0018222840735688806 | Testing Acc: 100.0\n",
            "Epoch: 2989 | Training Loss: 0.0009144229115918279 | Training Acc: 100.0 | Testing Loss: 0.0018222375074401498 | Testing Acc: 100.0\n",
            "Epoch: 2990 | Training Loss: 0.0009139926405623555 | Training Acc: 100.0 | Testing Loss: 0.001821179292164743 | Testing Acc: 100.0\n",
            "Epoch: 2991 | Training Loss: 0.0009135600994341075 | Training Acc: 100.0 | Testing Loss: 0.00182112748734653 | Testing Acc: 100.0\n",
            "Epoch: 2992 | Training Loss: 0.0009131415863521397 | Training Acc: 100.0 | Testing Loss: 0.0018200462218374014 | Testing Acc: 100.0\n",
            "Epoch: 2993 | Training Loss: 0.0009127050871029496 | Training Acc: 100.0 | Testing Loss: 0.0018190511036664248 | Testing Acc: 100.0\n",
            "Epoch: 2994 | Training Loss: 0.0009122932096943259 | Training Acc: 100.0 | Testing Loss: 0.0018190338741987944 | Testing Acc: 100.0\n",
            "Epoch: 2995 | Training Loss: 0.0009118586895056069 | Training Acc: 100.0 | Testing Loss: 0.0018179757753387094 | Testing Acc: 100.0\n",
            "Epoch: 2996 | Training Loss: 0.0009114322019740939 | Training Acc: 100.0 | Testing Loss: 0.0018179466715082526 | Testing Acc: 100.0\n",
            "Epoch: 2997 | Training Loss: 0.0009110121754929423 | Training Acc: 100.0 | Testing Loss: 0.0018168824026361108 | Testing Acc: 100.0\n",
            "Epoch: 2998 | Training Loss: 0.0009105782955884933 | Training Acc: 100.0 | Testing Loss: 0.0018168187234550714 | Testing Acc: 100.0\n",
            "Epoch: 2999 | Training Loss: 0.0009101625764742494 | Training Acc: 100.0 | Testing Loss: 0.0018157551530748606 | Testing Acc: 100.0\n",
            "Epoch: 3000 | Training Loss: 0.000909731606952846 | Training Acc: 100.0 | Testing Loss: 0.001814731047488749 | Testing Acc: 100.0\n",
            "Epoch: 3001 | Training Loss: 0.0009093169355764985 | Training Acc: 100.0 | Testing Loss: 0.0018147077644243836 | Testing Acc: 100.0\n",
            "Epoch: 3002 | Training Loss: 0.0009088821825571358 | Training Acc: 100.0 | Testing Loss: 0.0018136665457859635 | Testing Acc: 100.0\n",
            "Epoch: 3003 | Training Loss: 0.0009084646590054035 | Training Acc: 100.0 | Testing Loss: 0.0018136663129553199 | Testing Acc: 100.0\n",
            "Epoch: 3004 | Training Loss: 0.0009080400923267007 | Training Acc: 100.0 | Testing Loss: 0.0018125962233170867 | Testing Acc: 100.0\n",
            "Epoch: 3005 | Training Loss: 0.0009076108108274639 | Training Acc: 100.0 | Testing Loss: 0.0018125504720956087 | Testing Acc: 100.0\n",
            "Epoch: 3006 | Training Loss: 0.0009071993408724666 | Training Acc: 100.0 | Testing Loss: 0.0018114920239895582 | Testing Acc: 100.0\n",
            "Epoch: 3007 | Training Loss: 0.0009067581268027425 | Training Acc: 100.0 | Testing Loss: 0.0018104624468833208 | Testing Acc: 100.0\n",
            "Epoch: 3008 | Training Loss: 0.0009063418256118894 | Training Acc: 100.0 | Testing Loss: 0.001810444868169725 | Testing Acc: 100.0\n",
            "Epoch: 3009 | Training Loss: 0.0009059218573383987 | Training Acc: 100.0 | Testing Loss: 0.0018094092374667525 | Testing Acc: 100.0\n",
            "Epoch: 3010 | Training Loss: 0.000905502587556839 | Training Acc: 100.0 | Testing Loss: 0.001809380599297583 | Testing Acc: 100.0\n",
            "Epoch: 3011 | Training Loss: 0.0009050780208781362 | Training Acc: 100.0 | Testing Loss: 0.0018083220347762108 | Testing Acc: 100.0\n",
            "Epoch: 3012 | Training Loss: 0.0009046517079696059 | Training Acc: 100.0 | Testing Loss: 0.00180831050965935 | Testing Acc: 100.0\n",
            "Epoch: 3013 | Training Loss: 0.0009042476303875446 | Training Acc: 100.0 | Testing Loss: 0.0018072286620736122 | Testing Acc: 100.0\n",
            "Epoch: 3014 | Training Loss: 0.000903816893696785 | Training Acc: 100.0 | Testing Loss: 0.0018062163144350052 | Testing Acc: 100.0\n",
            "Epoch: 3015 | Training Loss: 0.0009034036775119603 | Training Acc: 100.0 | Testing Loss: 0.001806210377253592 | Testing Acc: 100.0\n",
            "Epoch: 3016 | Training Loss: 0.0009029788780026138 | Training Acc: 100.0 | Testing Loss: 0.0018051571678370237 | Testing Acc: 100.0\n",
            "Epoch: 3017 | Training Loss: 0.0009025612962432206 | Training Acc: 100.0 | Testing Loss: 0.0018051048973575234 | Testing Acc: 100.0\n",
            "Epoch: 3018 | Training Loss: 0.0009021455189213157 | Training Acc: 100.0 | Testing Loss: 0.0018040636787191033 | Testing Acc: 100.0\n",
            "Epoch: 3019 | Training Loss: 0.0009017189149744809 | Training Acc: 100.0 | Testing Loss: 0.0018040118739008904 | Testing Acc: 100.0\n",
            "Epoch: 3020 | Training Loss: 0.0009013119270093739 | Training Acc: 100.0 | Testing Loss: 0.0018029473721981049 | Testing Acc: 100.0\n",
            "Epoch: 3021 | Training Loss: 0.0009008871275000274 | Training Acc: 100.0 | Testing Loss: 0.0018019812414422631 | Testing Acc: 100.0\n",
            "Epoch: 3022 | Training Loss: 0.0009004781022667885 | Training Acc: 100.0 | Testing Loss: 0.00180194026324898 | Testing Acc: 100.0\n",
            "Epoch: 3023 | Training Loss: 0.0009000551654025912 | Training Acc: 100.0 | Testing Loss: 0.0018009046325460076 | Testing Acc: 100.0\n",
            "Epoch: 3024 | Training Loss: 0.0008996360702440143 | Training Acc: 100.0 | Testing Loss: 0.0018008701736107469 | Testing Acc: 100.0\n",
            "Epoch: 3025 | Training Loss: 0.0008992321672849357 | Training Acc: 100.0 | Testing Loss: 0.0017998283728957176 | Testing Acc: 100.0\n",
            "Epoch: 3026 | Training Loss: 0.0008988024783320725 | Training Acc: 100.0 | Testing Loss: 0.0017997706308960915 | Testing Acc: 100.0\n",
            "Epoch: 3027 | Training Loss: 0.0008983939187601209 | Training Acc: 100.0 | Testing Loss: 0.0017987176543101668 | Testing Acc: 100.0\n",
            "Epoch: 3028 | Training Loss: 0.0008979656849987805 | Training Acc: 100.0 | Testing Loss: 0.0017976990202441812 | Testing Acc: 100.0\n",
            "Epoch: 3029 | Training Loss: 0.0008975627133622766 | Training Acc: 100.0 | Testing Loss: 0.001797710545361042 | Testing Acc: 100.0\n",
            "Epoch: 3030 | Training Loss: 0.0008971368661150336 | Training Acc: 100.0 | Testing Loss: 0.0017966864397749305 | Testing Acc: 100.0\n",
            "Epoch: 3031 | Training Loss: 0.0008967238245531917 | Training Acc: 100.0 | Testing Loss: 0.0017966633895412087 | Testing Acc: 100.0\n",
            "Epoch: 3032 | Training Loss: 0.0008963164873421192 | Training Acc: 100.0 | Testing Loss: 0.0017956157680600882 | Testing Acc: 100.0\n",
            "Epoch: 3033 | Training Loss: 0.0008958902326412499 | Training Acc: 100.0 | Testing Loss: 0.001795558026060462 | Testing Acc: 100.0\n",
            "Epoch: 3034 | Training Loss: 0.0008954948862083256 | Training Acc: 100.0 | Testing Loss: 0.001794528216123581 | Testing Acc: 100.0\n",
            "Epoch: 3035 | Training Loss: 0.0008950711344368756 | Training Acc: 100.0 | Testing Loss: 0.0017935155192390084 | Testing Acc: 100.0\n",
            "Epoch: 3036 | Training Loss: 0.000894665252417326 | Training Acc: 100.0 | Testing Loss: 0.0017934979405254126 | Testing Acc: 100.0\n",
            "Epoch: 3037 | Training Loss: 0.0008942509302869439 | Training Acc: 100.0 | Testing Loss: 0.0017924790736287832 | Testing Acc: 100.0\n",
            "Epoch: 3038 | Training Loss: 0.0008938357932493091 | Training Acc: 100.0 | Testing Loss: 0.0017924674320966005 | Testing Acc: 100.0\n",
            "Epoch: 3039 | Training Loss: 0.0008934320649132133 | Training Acc: 100.0 | Testing Loss: 0.0017914375057443976 | Testing Acc: 100.0\n",
            "Epoch: 3040 | Training Loss: 0.0008930086041800678 | Training Acc: 100.0 | Testing Loss: 0.0017904650885611773 | Testing Acc: 100.0\n",
            "Epoch: 3041 | Training Loss: 0.0008926126174628735 | Training Acc: 100.0 | Testing Loss: 0.0017904419219121337 | Testing Acc: 100.0\n",
            "Epoch: 3042 | Training Loss: 0.0008921896805986762 | Training Acc: 100.0 | Testing Loss: 0.0017894173506647348 | Testing Acc: 100.0\n",
            "Epoch: 3043 | Training Loss: 0.0008917836239561439 | Training Acc: 100.0 | Testing Loss: 0.0017894055927172303 | Testing Acc: 100.0\n",
            "Epoch: 3044 | Training Loss: 0.0008913719793781638 | Training Acc: 100.0 | Testing Loss: 0.0017883812543004751 | Testing Acc: 100.0\n",
            "Epoch: 3045 | Training Loss: 0.0008909632451832294 | Training Acc: 100.0 | Testing Loss: 0.0017883351538330317 | Testing Acc: 100.0\n",
            "Epoch: 3046 | Training Loss: 0.0008905590511858463 | Training Acc: 100.0 | Testing Loss: 0.0017873108154162765 | Testing Acc: 100.0\n",
            "Epoch: 3047 | Training Loss: 0.00089013681281358 | Training Acc: 100.0 | Testing Loss: 0.0017863381654024124 | Testing Acc: 100.0\n",
            "Epoch: 3048 | Training Loss: 0.0008897412335500121 | Training Acc: 100.0 | Testing Loss: 0.0017863266402855515 | Testing Acc: 100.0\n",
            "Epoch: 3049 | Training Loss: 0.0008893240010365844 | Training Acc: 100.0 | Testing Loss: 0.0017853075405582786 | Testing Acc: 100.0\n",
            "Epoch: 3050 | Training Loss: 0.000888920680154115 | Training Acc: 100.0 | Testing Loss: 0.001785272965207696 | Testing Acc: 100.0\n",
            "Epoch: 3051 | Training Loss: 0.0008885062416084111 | Training Acc: 100.0 | Testing Loss: 0.001784254447557032 | Testing Acc: 100.0\n",
            "Epoch: 3052 | Training Loss: 0.0008880976820364594 | Training Acc: 100.0 | Testing Loss: 0.0017842024099081755 | Testing Acc: 100.0\n",
            "Epoch: 3053 | Training Loss: 0.0008877037325873971 | Training Acc: 100.0 | Testing Loss: 0.001783166080713272 | Testing Acc: 100.0\n",
            "Epoch: 3054 | Training Loss: 0.000887281377799809 | Training Acc: 100.0 | Testing Loss: 0.001782182021997869 | Testing Acc: 100.0\n",
            "Epoch: 3055 | Training Loss: 0.0008868871373124421 | Training Acc: 100.0 | Testing Loss: 0.001782193430699408 | Testing Acc: 100.0\n",
            "Epoch: 3056 | Training Loss: 0.0008864730480127037 | Training Acc: 100.0 | Testing Loss: 0.0017811860889196396 | Testing Acc: 100.0\n",
            "Epoch: 3057 | Training Loss: 0.0008860711823217571 | Training Acc: 100.0 | Testing Loss: 0.0017811572179198265 | Testing Acc: 100.0\n",
            "Epoch: 3058 | Training Loss: 0.0008856671047396958 | Training Acc: 100.0 | Testing Loss: 0.001780121005140245 | Testing Acc: 100.0\n",
            "Epoch: 3059 | Training Loss: 0.0008852537721395493 | Training Acc: 100.0 | Testing Loss: 0.0017800803761929274 | Testing Acc: 100.0\n",
            "Epoch: 3060 | Training Loss: 0.0008848641300573945 | Training Acc: 100.0 | Testing Loss: 0.0017790619749575853 | Testing Acc: 100.0\n",
            "Epoch: 3061 | Training Loss: 0.0008844405529089272 | Training Acc: 100.0 | Testing Loss: 0.0017780776834115386 | Testing Acc: 100.0\n",
            "Epoch: 3062 | Training Loss: 0.000884041772224009 | Training Acc: 100.0 | Testing Loss: 0.0017780658090487123 | Testing Acc: 100.0\n",
            "Epoch: 3063 | Training Loss: 0.0008836330962367356 | Training Acc: 100.0 | Testing Loss: 0.0017770582344383001 | Testing Acc: 100.0\n",
            "Epoch: 3064 | Training Loss: 0.0008832300081849098 | Training Acc: 100.0 | Testing Loss: 0.001777040772140026 | Testing Acc: 100.0\n",
            "Epoch: 3065 | Training Loss: 0.0008828319841995835 | Training Acc: 100.0 | Testing Loss: 0.0017760396003723145 | Testing Acc: 100.0\n",
            "Epoch: 3066 | Training Loss: 0.0008824242977425456 | Training Acc: 100.0 | Testing Loss: 0.0017759816255420446 | Testing Acc: 100.0\n",
            "Epoch: 3067 | Training Loss: 0.0008820303482934833 | Training Acc: 100.0 | Testing Loss: 0.001774945529177785 | Testing Acc: 100.0\n",
            "Epoch: 3068 | Training Loss: 0.0008816156769171357 | Training Acc: 100.0 | Testing Loss: 0.0017739783506840467 | Testing Acc: 100.0\n",
            "Epoch: 3069 | Training Loss: 0.0008812284795567393 | Training Acc: 100.0 | Testing Loss: 0.0017739497125148773 | Testing Acc: 100.0\n",
            "Epoch: 3070 | Training Loss: 0.0008808170678094029 | Training Acc: 100.0 | Testing Loss: 0.0017729420214891434 | Testing Acc: 100.0\n",
            "Epoch: 3071 | Training Loss: 0.0008804155513644218 | Training Acc: 100.0 | Testing Loss: 0.0017729013925418258 | Testing Acc: 100.0\n",
            "Epoch: 3072 | Training Loss: 0.0008800156647339463 | Training Acc: 100.0 | Testing Loss: 0.0017718998715281487 | Testing Acc: 100.0\n",
            "Epoch: 3073 | Training Loss: 0.0008796094916760921 | Training Acc: 100.0 | Testing Loss: 0.00177188275847584 | Testing Acc: 100.0\n",
            "Epoch: 3074 | Training Loss: 0.0008792201988399029 | Training Acc: 100.0 | Testing Loss: 0.0017708342056721449 | Testing Acc: 100.0\n",
            "Epoch: 3075 | Training Loss: 0.0008788098348304629 | Training Acc: 100.0 | Testing Loss: 0.0017698671435937285 | Testing Acc: 100.0\n",
            "Epoch: 3076 | Training Loss: 0.0008784212404862046 | Training Acc: 100.0 | Testing Loss: 0.0017698497977107763 | Testing Acc: 100.0\n",
            "Epoch: 3077 | Training Loss: 0.0008780171046964824 | Training Acc: 100.0 | Testing Loss: 0.0017688481602817774 | Testing Acc: 100.0\n",
            "Epoch: 3078 | Training Loss: 0.0008776184404268861 | Training Acc: 100.0 | Testing Loss: 0.0017688365187495947 | Testing Acc: 100.0\n",
            "Epoch: 3079 | Training Loss: 0.0008772182045504451 | Training Acc: 100.0 | Testing Loss: 0.0017678115982562304 | Testing Acc: 100.0\n",
            "Epoch: 3080 | Training Loss: 0.000876807956956327 | Training Acc: 100.0 | Testing Loss: 0.0017677654977887869 | Testing Acc: 100.0\n",
            "Epoch: 3081 | Training Loss: 0.0008764256490394473 | Training Acc: 100.0 | Testing Loss: 0.0017667580395936966 | Testing Acc: 100.0\n",
            "Epoch: 3082 | Training Loss: 0.0008760180207900703 | Training Acc: 100.0 | Testing Loss: 0.0017658196156844497 | Testing Acc: 100.0\n",
            "Epoch: 3083 | Training Loss: 0.000875624013133347 | Training Acc: 100.0 | Testing Loss: 0.0017657907446846366 | Testing Acc: 100.0\n",
            "Epoch: 3084 | Training Loss: 0.0008752301218919456 | Training Acc: 100.0 | Testing Loss: 0.0017648119246587157 | Testing Acc: 100.0\n",
            "Epoch: 3085 | Training Loss: 0.0008748326217755675 | Training Acc: 100.0 | Testing Loss: 0.0017647712957113981 | Testing Acc: 100.0\n",
            "Epoch: 3086 | Training Loss: 0.0008744386723265052 | Training Acc: 100.0 | Testing Loss: 0.0017637638375163078 | Testing Acc: 100.0\n",
            "Epoch: 3087 | Training Loss: 0.0008740313351154327 | Training Acc: 100.0 | Testing Loss: 0.0017628135392442346 | Testing Acc: 100.0\n",
            "Epoch: 3088 | Training Loss: 0.0008736528689041734 | Training Acc: 100.0 | Testing Loss: 0.0017628021305426955 | Testing Acc: 100.0\n",
            "Epoch: 3089 | Training Loss: 0.0008732442511245608 | Training Acc: 100.0 | Testing Loss: 0.001761829131282866 | Testing Acc: 100.0\n",
            "Epoch: 3090 | Training Loss: 0.0008728530374355614 | Training Acc: 100.0 | Testing Loss: 0.0017618171405047178 | Testing Acc: 100.0\n",
            "Epoch: 3091 | Training Loss: 0.0008724562940187752 | Training Acc: 100.0 | Testing Loss: 0.001760826795361936 | Testing Acc: 100.0\n",
            "Epoch: 3092 | Training Loss: 0.0008720634505152702 | Training Acc: 100.0 | Testing Loss: 0.001760780462063849 | Testing Acc: 100.0\n",
            "Epoch: 3093 | Training Loss: 0.0008716693264432251 | Training Acc: 100.0 | Testing Loss: 0.0017597728874534369 | Testing Acc: 100.0\n",
            "Epoch: 3094 | Training Loss: 0.0008712677517905831 | Training Acc: 100.0 | Testing Loss: 0.001758822938427329 | Testing Acc: 100.0\n",
            "Epoch: 3095 | Training Loss: 0.000870886433403939 | Training Acc: 100.0 | Testing Loss: 0.001758805476129055 | Testing Acc: 100.0\n",
            "Epoch: 3096 | Training Loss: 0.0008704822394065559 | Training Acc: 100.0 | Testing Loss: 0.0017578147817403078 | Testing Acc: 100.0\n",
            "Epoch: 3097 | Training Loss: 0.0008700952748768032 | Training Acc: 100.0 | Testing Loss: 0.0017577800899744034 | Testing Acc: 100.0\n",
            "Epoch: 3098 | Training Loss: 0.0008697024313732982 | Training Acc: 100.0 | Testing Loss: 0.0017567954491823912 | Testing Acc: 100.0\n",
            "Epoch: 3099 | Training Loss: 0.0008693023701198399 | Training Acc: 100.0 | Testing Loss: 0.001756749115884304 | Testing Acc: 100.0\n",
            "Epoch: 3100 | Training Loss: 0.0008689140668138862 | Training Acc: 100.0 | Testing Loss: 0.0017557644750922918 | Testing Acc: 100.0\n",
            "Epoch: 3101 | Training Loss: 0.0008685108041390777 | Training Acc: 100.0 | Testing Loss: 0.0017548197647556663 | Testing Acc: 100.0\n",
            "Epoch: 3102 | Training Loss: 0.0008681329782120883 | Training Acc: 100.0 | Testing Loss: 0.0017548026517033577 | Testing Acc: 100.0\n",
            "Epoch: 3103 | Training Loss: 0.0008677345467731357 | Training Acc: 100.0 | Testing Loss: 0.0017538350075483322 | Testing Acc: 100.0\n",
            "Epoch: 3104 | Training Loss: 0.0008673443226143718 | Training Acc: 100.0 | Testing Loss: 0.0017537946114316583 | Testing Acc: 100.0\n",
            "Epoch: 3105 | Training Loss: 0.0008669562521390617 | Training Acc: 100.0 | Testing Loss: 0.0017528038006275892 | Testing Acc: 100.0\n",
            "Epoch: 3106 | Training Loss: 0.0008665621280670166 | Training Acc: 100.0 | Testing Loss: 0.0017527634045109153 | Testing Acc: 100.0\n",
            "Epoch: 3107 | Training Loss: 0.0008661808678880334 | Training Acc: 100.0 | Testing Loss: 0.001751761301420629 | Testing Acc: 100.0\n",
            "Epoch: 3108 | Training Loss: 0.0008657749858684838 | Training Acc: 100.0 | Testing Loss: 0.001750822877511382 | Testing Acc: 100.0\n",
            "Epoch: 3109 | Training Loss: 0.0008654012344777584 | Training Acc: 100.0 | Testing Loss: 0.001750816823914647 | Testing Acc: 100.0\n",
            "Epoch: 3110 | Training Loss: 0.0008650074014440179 | Training Acc: 100.0 | Testing Loss: 0.0017498608212918043 | Testing Acc: 100.0\n",
            "Epoch: 3111 | Training Loss: 0.000864615838509053 | Training Acc: 100.0 | Testing Loss: 0.0017498203087598085 | Testing Acc: 100.0\n",
            "Epoch: 3112 | Training Loss: 0.0008642275934107602 | Training Acc: 100.0 | Testing Loss: 0.0017488294979557395 | Testing Acc: 100.0\n",
            "Epoch: 3113 | Training Loss: 0.0008638334693387151 | Training Acc: 100.0 | Testing Loss: 0.001748777343891561 | Testing Acc: 100.0\n",
            "Epoch: 3114 | Training Loss: 0.0008634552359580994 | Training Acc: 100.0 | Testing Loss: 0.001747775124385953 | Testing Acc: 100.0\n",
            "Epoch: 3115 | Training Loss: 0.0008630565134808421 | Training Acc: 100.0 | Testing Loss: 0.0017468251753598452 | Testing Acc: 100.0\n",
            "Epoch: 3116 | Training Loss: 0.0008626783383078873 | Training Acc: 100.0 | Testing Loss: 0.0017468134174123406 | Testing Acc: 100.0\n",
            "Epoch: 3117 | Training Loss: 0.0008622839814051986 | Training Acc: 100.0 | Testing Loss: 0.0017458575312048197 | Testing Acc: 100.0\n",
            "Epoch: 3118 | Training Loss: 0.0008619028958491981 | Training Acc: 100.0 | Testing Loss: 0.0017458340153098106 | Testing Acc: 100.0\n",
            "Epoch: 3119 | Training Loss: 0.0008615163969807327 | Training Acc: 100.0 | Testing Loss: 0.001744860433973372 | Testing Acc: 100.0\n",
            "Epoch: 3120 | Training Loss: 0.0008611248922534287 | Training Acc: 100.0 | Testing Loss: 0.0017448026919737458 | Testing Acc: 100.0\n",
            "Epoch: 3121 | Training Loss: 0.0008607468334957957 | Training Acc: 100.0 | Testing Loss: 0.0017438121140003204 | Testing Acc: 100.0\n",
            "Epoch: 3122 | Training Loss: 0.000860349740833044 | Training Acc: 100.0 | Testing Loss: 0.0017428850987926126 | Testing Acc: 100.0\n",
            "Epoch: 3123 | Training Loss: 0.0008599756401963532 | Training Acc: 100.0 | Testing Loss: 0.0017428441205993295 | Testing Acc: 100.0\n",
            "Epoch: 3124 | Training Loss: 0.0008595860563218594 | Training Acc: 100.0 | Testing Loss: 0.0017418883508071303 | Testing Acc: 100.0\n",
            "Epoch: 3125 | Training Loss: 0.0008591976948082447 | Training Acc: 100.0 | Testing Loss: 0.0017418593633919954 | Testing Acc: 100.0\n",
            "Epoch: 3126 | Training Loss: 0.0008588178898207843 | Training Acc: 100.0 | Testing Loss: 0.0017408740241080523 | Testing Acc: 100.0\n",
            "Epoch: 3127 | Training Loss: 0.0008584236493334174 | Training Acc: 100.0 | Testing Loss: 0.0017408508574590087 | Testing Acc: 100.0\n",
            "Epoch: 3128 | Training Loss: 0.0008580561843700707 | Training Acc: 100.0 | Testing Loss: 0.001739865867421031 | Testing Acc: 100.0\n",
            "Epoch: 3129 | Training Loss: 0.0008576646214351058 | Training Acc: 100.0 | Testing Loss: 0.0017389210406690836 | Testing Acc: 100.0\n",
            "Epoch: 3130 | Training Loss: 0.0008572848746553063 | Training Acc: 100.0 | Testing Loss: 0.00173889787402004 | Testing Acc: 100.0\n",
            "Epoch: 3131 | Training Loss: 0.0008569011697545648 | Training Acc: 100.0 | Testing Loss: 0.0017379358178004622 | Testing Acc: 100.0\n",
            "Epoch: 3132 | Training Loss: 0.0008565097232349217 | Training Acc: 100.0 | Testing Loss: 0.0017379069468006492 | Testing Acc: 100.0\n",
            "Epoch: 3133 | Training Loss: 0.0008561358554288745 | Training Acc: 100.0 | Testing Loss: 0.0017369275446981192 | Testing Acc: 100.0\n",
            "Epoch: 3134 | Training Loss: 0.000855738646350801 | Training Acc: 100.0 | Testing Loss: 0.0017359999474138021 | Testing Acc: 100.0\n",
            "Epoch: 3135 | Training Loss: 0.0008553738007321954 | Training Acc: 100.0 | Testing Loss: 0.0017360169440507889 | Testing Acc: 100.0\n",
            "Epoch: 3136 | Training Loss: 0.000854982528835535 | Training Acc: 100.0 | Testing Loss: 0.0017350784037262201 | Testing Acc: 100.0\n",
            "Epoch: 3137 | Training Loss: 0.0008546058088541031 | Training Acc: 100.0 | Testing Loss: 0.0017350375419482589 | Testing Acc: 100.0\n",
            "Epoch: 3138 | Training Loss: 0.0008542219293303788 | Training Acc: 100.0 | Testing Loss: 0.0017340872436761856 | Testing Acc: 100.0\n",
            "Epoch: 3139 | Training Loss: 0.0008538406109437346 | Training Acc: 100.0 | Testing Loss: 0.001734029152430594 | Testing Acc: 100.0\n",
            "Epoch: 3140 | Training Loss: 0.0008534640073776245 | Training Acc: 100.0 | Testing Loss: 0.001733049750328064 | Testing Acc: 100.0\n",
            "Epoch: 3141 | Training Loss: 0.0008530741324648261 | Training Acc: 100.0 | Testing Loss: 0.0017321277409791946 | Testing Acc: 100.0\n",
            "Epoch: 3142 | Training Loss: 0.0008527043391950428 | Training Acc: 100.0 | Testing Loss: 0.0017321102786809206 | Testing Acc: 100.0\n",
            "Epoch: 3143 | Training Loss: 0.0008523117867298424 | Training Acc: 100.0 | Testing Loss: 0.0017311598639935255 | Testing Acc: 100.0\n",
            "Epoch: 3144 | Training Loss: 0.0008519396069459617 | Training Acc: 100.0 | Testing Loss: 0.0017311482224613428 | Testing Acc: 100.0\n",
            "Epoch: 3145 | Training Loss: 0.0008515646914020181 | Training Acc: 100.0 | Testing Loss: 0.0017302033957093954 | Testing Acc: 100.0\n",
            "Epoch: 3146 | Training Loss: 0.0008511774358339608 | Training Acc: 100.0 | Testing Loss: 0.0017301456537097692 | Testing Acc: 100.0\n",
            "Epoch: 3147 | Training Loss: 0.0008508082246407866 | Training Acc: 100.0 | Testing Loss: 0.0017291719559580088 | Testing Acc: 100.0\n",
            "Epoch: 3148 | Training Loss: 0.0008504167199134827 | Training Acc: 100.0 | Testing Loss: 0.0017282558837905526 | Testing Acc: 100.0\n",
            "Epoch: 3149 | Training Loss: 0.000850048556458205 | Training Acc: 100.0 | Testing Loss: 0.001728215254843235 | Testing Acc: 100.0\n",
            "Epoch: 3150 | Training Loss: 0.0008496662485413253 | Training Acc: 100.0 | Testing Loss: 0.0017272820696234703 | Testing Acc: 100.0\n",
            "Epoch: 3151 | Training Loss: 0.0008492880733683705 | Training Acc: 100.0 | Testing Loss: 0.0017272410914301872 | Testing Acc: 100.0\n",
            "Epoch: 3152 | Training Loss: 0.0008489126339554787 | Training Acc: 100.0 | Testing Loss: 0.001726290793158114 | Testing Acc: 100.0\n",
            "Epoch: 3153 | Training Loss: 0.0008485271828249097 | Training Acc: 100.0 | Testing Loss: 0.001726255752146244 | Testing Acc: 100.0\n",
            "Epoch: 3154 | Training Loss: 0.0008481639670208097 | Training Acc: 100.0 | Testing Loss: 0.00172529392875731 | Testing Acc: 100.0\n",
            "Epoch: 3155 | Training Loss: 0.0008477751980535686 | Training Acc: 100.0 | Testing Loss: 0.001724360277876258 | Testing Acc: 100.0\n",
            "Epoch: 3156 | Training Loss: 0.0008474058704450727 | Training Acc: 100.0 | Testing Loss: 0.0017243309412151575 | Testing Acc: 100.0\n",
            "Epoch: 3157 | Training Loss: 0.0008470279281027615 | Training Acc: 100.0 | Testing Loss: 0.0017233975231647491 | Testing Acc: 100.0\n",
            "Epoch: 3158 | Training Loss: 0.0008466538856737316 | Training Acc: 100.0 | Testing Loss: 0.001723362714983523 | Testing Acc: 100.0\n",
            "Epoch: 3159 | Training Loss: 0.0008462830446660519 | Training Acc: 100.0 | Testing Loss: 0.0017224124167114496 | Testing Acc: 100.0\n",
            "Epoch: 3160 | Training Loss: 0.0008459021337330341 | Training Acc: 100.0 | Testing Loss: 0.0017223544418811798 | Testing Acc: 100.0\n",
            "Epoch: 3161 | Training Loss: 0.0008455415372736752 | Training Acc: 100.0 | Testing Loss: 0.0017214094987139106 | Testing Acc: 100.0\n",
            "Epoch: 3162 | Training Loss: 0.0008451513713225722 | Training Acc: 100.0 | Testing Loss: 0.0017205046024173498 | Testing Acc: 100.0\n",
            "Epoch: 3163 | Training Loss: 0.0008447851287201047 | Training Acc: 100.0 | Testing Loss: 0.0017204817850142717 | Testing Acc: 100.0\n",
            "Epoch: 3164 | Training Loss: 0.0008444099803455174 | Training Acc: 100.0 | Testing Loss: 0.0017195537220686674 | Testing Acc: 100.0\n",
            "Epoch: 3165 | Training Loss: 0.0008440329693257809 | Training Acc: 100.0 | Testing Loss: 0.001719507621601224 | Testing Acc: 100.0\n",
            "Epoch: 3166 | Training Loss: 0.0008436636999249458 | Training Acc: 100.0 | Testing Loss: 0.0017185567412525415 | Testing Acc: 100.0\n",
            "Epoch: 3167 | Training Loss: 0.0008432751637883484 | Training Acc: 100.0 | Testing Loss: 0.0017185102915391326 | Testing Acc: 100.0\n",
            "Epoch: 3168 | Training Loss: 0.0008429232984781265 | Training Acc: 100.0 | Testing Loss: 0.001717548118904233 | Testing Acc: 100.0\n",
            "Epoch: 3169 | Training Loss: 0.0008425408159382641 | Training Acc: 100.0 | Testing Loss: 0.0017166318139061332 | Testing Acc: 100.0\n",
            "Epoch: 3170 | Training Loss: 0.0008421757374890149 | Training Acc: 100.0 | Testing Loss: 0.0017166429897770286 | Testing Acc: 100.0\n",
            "Epoch: 3171 | Training Loss: 0.0008418019860982895 | Training Acc: 100.0 | Testing Loss: 0.0017156976973637938 | Testing Acc: 100.0\n",
            "Epoch: 3172 | Training Loss: 0.0008414252661168575 | Training Acc: 100.0 | Testing Loss: 0.0017156628891825676 | Testing Acc: 100.0\n",
            "Epoch: 3173 | Training Loss: 0.0008410632726736367 | Training Acc: 100.0 | Testing Loss: 0.001714723533950746 | Testing Acc: 100.0\n",
            "Epoch: 3174 | Training Loss: 0.0008406848646700382 | Training Acc: 100.0 | Testing Loss: 0.0017138129333034158 | Testing Acc: 100.0\n",
            "Epoch: 3175 | Training Loss: 0.0008403286337852478 | Training Acc: 100.0 | Testing Loss: 0.001713801408186555 | Testing Acc: 100.0\n",
            "Epoch: 3176 | Training Loss: 0.0008399477228522301 | Training Acc: 100.0 | Testing Loss: 0.001712867640890181 | Testing Acc: 100.0\n",
            "Epoch: 3177 | Training Loss: 0.0008395797340199351 | Training Acc: 100.0 | Testing Loss: 0.0017128502950072289 | Testing Acc: 100.0\n",
            "Epoch: 3178 | Training Loss: 0.0008392072049900889 | Training Acc: 100.0 | Testing Loss: 0.0017119108233600855 | Testing Acc: 100.0\n",
            "Epoch: 3179 | Training Loss: 0.0008388319984078407 | Training Acc: 100.0 | Testing Loss: 0.0017118934774771333 | Testing Acc: 100.0\n",
            "Epoch: 3180 | Training Loss: 0.0008384760585613549 | Training Acc: 100.0 | Testing Loss: 0.0017109422478824854 | Testing Acc: 100.0\n",
            "Epoch: 3181 | Training Loss: 0.0008380976505577564 | Training Acc: 100.0 | Testing Loss: 0.0017100314144045115 | Testing Acc: 100.0\n",
            "Epoch: 3182 | Training Loss: 0.0008377384510822594 | Training Acc: 100.0 | Testing Loss: 0.0017100084805861115 | Testing Acc: 100.0\n",
            "Epoch: 3183 | Training Loss: 0.0008373633027076721 | Training Acc: 100.0 | Testing Loss: 0.0017090976471081376 | Testing Acc: 100.0\n",
            "Epoch: 3184 | Training Loss: 0.0008369935676455498 | Training Acc: 100.0 | Testing Loss: 0.0017090629553422332 | Testing Acc: 100.0\n",
            "Epoch: 3185 | Training Loss: 0.0008366316324099898 | Training Acc: 100.0 | Testing Loss: 0.0017081234836950898 | Testing Acc: 100.0\n",
            "Epoch: 3186 | Training Loss: 0.000836254854220897 | Training Acc: 100.0 | Testing Loss: 0.0017080710968002677 | Testing Acc: 100.0\n",
            "Epoch: 3187 | Training Loss: 0.0008359000203199685 | Training Acc: 100.0 | Testing Loss: 0.0017071375623345375 | Testing Acc: 100.0\n",
            "Epoch: 3188 | Training Loss: 0.0008355247555300593 | Training Acc: 100.0 | Testing Loss: 0.0017062615370377898 | Testing Acc: 100.0\n",
            "Epoch: 3189 | Training Loss: 0.0008351643336936831 | Training Acc: 100.0 | Testing Loss: 0.0017062382539734244 | Testing Acc: 100.0\n",
            "Epoch: 3190 | Training Loss: 0.0008347994880750775 | Training Acc: 100.0 | Testing Loss: 0.0017053099581971765 | Testing Acc: 100.0\n",
            "Epoch: 3191 | Training Loss: 0.0008344252710230649 | Training Acc: 100.0 | Testing Loss: 0.0017052695620805025 | Testing Acc: 100.0\n",
            "Epoch: 3192 | Training Loss: 0.000834063277579844 | Training Acc: 100.0 | Testing Loss: 0.0017043413827195764 | Testing Acc: 100.0\n",
            "Epoch: 3193 | Training Loss: 0.0008336937753483653 | Training Acc: 100.0 | Testing Loss: 0.0017043009866029024 | Testing Acc: 100.0\n",
            "Epoch: 3194 | Training Loss: 0.0008333373698405921 | Training Acc: 100.0 | Testing Loss: 0.001703355461359024 | Testing Acc: 100.0\n",
            "Epoch: 3195 | Training Loss: 0.0008329682168550789 | Training Acc: 100.0 | Testing Loss: 0.0017024446278810501 | Testing Acc: 100.0\n",
            "Epoch: 3196 | Training Loss: 0.0008326089009642601 | Training Acc: 100.0 | Testing Loss: 0.0017024617409333587 | Testing Acc: 100.0\n",
            "Epoch: 3197 | Training Loss: 0.0008322394569404423 | Training Acc: 100.0 | Testing Loss: 0.0017015452031046152 | Testing Acc: 100.0\n",
            "Epoch: 3198 | Training Loss: 0.0008318729815073311 | Training Acc: 100.0 | Testing Loss: 0.00170149898622185 | Testing Acc: 100.0\n",
            "Epoch: 3199 | Training Loss: 0.0008315139566548169 | Training Acc: 100.0 | Testing Loss: 0.0017005825648084283 | Testing Acc: 100.0\n",
            "Epoch: 3200 | Training Loss: 0.0008311413112096488 | Training Acc: 100.0 | Testing Loss: 0.0017005305271595716 | Testing Acc: 100.0\n",
            "Epoch: 3201 | Training Loss: 0.0008307925309054554 | Training Acc: 100.0 | Testing Loss: 0.0016995792975649238 | Testing Acc: 100.0\n",
            "Epoch: 3202 | Training Loss: 0.0008304187213070691 | Training Acc: 100.0 | Testing Loss: 0.0016986913979053497 | Testing Acc: 100.0\n",
            "Epoch: 3203 | Training Loss: 0.0008300620829686522 | Training Acc: 100.0 | Testing Loss: 0.001698644831776619 | Testing Acc: 100.0\n",
            "Epoch: 3204 | Training Loss: 0.0008296957239508629 | Training Acc: 100.0 | Testing Loss: 0.0016977399354800582 | Testing Acc: 100.0\n",
            "Epoch: 3205 | Training Loss: 0.000829335127491504 | Training Acc: 100.0 | Testing Loss: 0.0016977224731817842 | Testing Acc: 100.0\n",
            "Epoch: 3206 | Training Loss: 0.00082898058462888 | Training Acc: 100.0 | Testing Loss: 0.001696811756119132 | Testing Acc: 100.0\n",
            "Epoch: 3207 | Training Loss: 0.0008286050288006663 | Training Acc: 100.0 | Testing Loss: 0.0016959179192781448 | Testing Acc: 100.0\n",
            "Epoch: 3208 | Training Loss: 0.0008282531052827835 | Training Acc: 100.0 | Testing Loss: 0.0016959120985120535 | Testing Acc: 100.0\n",
            "Epoch: 3209 | Training Loss: 0.0008278883178718388 | Training Acc: 100.0 | Testing Loss: 0.0016950007993727922 | Testing Acc: 100.0\n",
            "Epoch: 3210 | Training Loss: 0.0008275288855656981 | Training Acc: 100.0 | Testing Loss: 0.0016949776327237487 | Testing Acc: 100.0\n",
            "Epoch: 3211 | Training Loss: 0.0008271668339148164 | Training Acc: 100.0 | Testing Loss: 0.0016940550412982702 | Testing Acc: 100.0\n",
            "Epoch: 3212 | Training Loss: 0.0008268033270724118 | Training Acc: 100.0 | Testing Loss: 0.0016940204659476876 | Testing Acc: 100.0\n",
            "Epoch: 3213 | Training Loss: 0.000826446688733995 | Training Acc: 100.0 | Testing Loss: 0.0016930922865867615 | Testing Acc: 100.0\n",
            "Epoch: 3214 | Training Loss: 0.000826077361125499 | Training Acc: 100.0 | Testing Loss: 0.0016922332579270005 | Testing Acc: 100.0\n",
            "Epoch: 3215 | Training Loss: 0.0008257286390289664 | Training Acc: 100.0 | Testing Loss: 0.0016922156792134047 | Testing Acc: 100.0\n",
            "Epoch: 3216 | Training Loss: 0.000825366354547441 | Training Acc: 100.0 | Testing Loss: 0.0016913044964894652 | Testing Acc: 100.0\n",
            "Epoch: 3217 | Training Loss: 0.0008250115206465125 | Training Acc: 100.0 | Testing Loss: 0.0016912641003727913 | Testing Acc: 100.0\n",
            "Epoch: 3218 | Training Loss: 0.0008246509241871536 | Training Acc: 100.0 | Testing Loss: 0.0016903530340641737 | Testing Acc: 100.0\n",
            "Epoch: 3219 | Training Loss: 0.0008242871845141053 | Training Acc: 100.0 | Testing Loss: 0.0016903128707781434 | Testing Acc: 100.0\n",
            "Epoch: 3220 | Training Loss: 0.0008239321177825332 | Training Acc: 100.0 | Testing Loss: 0.001689390279352665 | Testing Acc: 100.0\n",
            "Epoch: 3221 | Training Loss: 0.0008235643617808819 | Training Acc: 100.0 | Testing Loss: 0.0016885079676285386 | Testing Acc: 100.0\n",
            "Epoch: 3222 | Training Loss: 0.000823219888843596 | Training Acc: 100.0 | Testing Loss: 0.0016885192599147558 | Testing Acc: 100.0\n",
            "Epoch: 3223 | Training Loss: 0.000822853296995163 | Training Acc: 100.0 | Testing Loss: 0.0016876140143722296 | Testing Acc: 100.0\n",
            "Epoch: 3224 | Training Loss: 0.0008224955527111888 | Training Acc: 100.0 | Testing Loss: 0.0016875736182555556 | Testing Acc: 100.0\n",
            "Epoch: 3225 | Training Loss: 0.0008221467724069953 | Training Acc: 100.0 | Testing Loss: 0.0016866683727130294 | Testing Acc: 100.0\n",
            "Epoch: 3226 | Training Loss: 0.000821784429717809 | Training Acc: 100.0 | Testing Loss: 0.001686616218648851 | Testing Acc: 100.0\n",
            "Epoch: 3227 | Training Loss: 0.0008214369299821556 | Training Acc: 100.0 | Testing Loss: 0.0016856699949130416 | Testing Acc: 100.0\n",
            "Epoch: 3228 | Training Loss: 0.0008210645173676312 | Training Acc: 100.0 | Testing Loss: 0.001684805378317833 | Testing Acc: 100.0\n",
            "Epoch: 3229 | Training Loss: 0.0008207213249988854 | Training Acc: 100.0 | Testing Loss: 0.0016847820952534676 | Testing Acc: 100.0\n",
            "Epoch: 3230 | Training Loss: 0.0008203621837310493 | Training Acc: 100.0 | Testing Loss: 0.001683882437646389 | Testing Acc: 100.0\n",
            "Epoch: 3231 | Training Loss: 0.0008200073498301208 | Training Acc: 100.0 | Testing Loss: 0.001683864975348115 | Testing Acc: 100.0\n",
            "Epoch: 3232 | Training Loss: 0.0008196585695259273 | Training Acc: 100.0 | Testing Loss: 0.0016829597298055887 | Testing Acc: 100.0\n",
            "Epoch: 3233 | Training Loss: 0.000819288834463805 | Training Acc: 100.0 | Testing Loss: 0.0016829073429107666 | Testing Acc: 100.0\n",
            "Epoch: 3234 | Training Loss: 0.0008189472137019038 | Training Acc: 100.0 | Testing Loss: 0.0016819846350699663 | Testing Acc: 100.0\n",
            "Epoch: 3235 | Training Loss: 0.0008185881306417286 | Training Acc: 100.0 | Testing Loss: 0.0016811138484627008 | Testing Acc: 100.0\n",
            "Epoch: 3236 | Training Loss: 0.0008182404562830925 | Training Acc: 100.0 | Testing Loss: 0.0016810849774628878 | Testing Acc: 100.0\n",
            "Epoch: 3237 | Training Loss: 0.0008178843418136239 | Training Acc: 100.0 | Testing Loss: 0.0016801853198558092 | Testing Acc: 100.0\n",
            "Epoch: 3238 | Training Loss: 0.0008175295079126954 | Training Acc: 100.0 | Testing Loss: 0.0016801503952592611 | Testing Acc: 100.0\n",
            "Epoch: 3239 | Training Loss: 0.0008171787485480309 | Training Acc: 100.0 | Testing Loss: 0.0016792448004707694 | Testing Acc: 100.0\n",
            "Epoch: 3240 | Training Loss: 0.0008168151834979653 | Training Acc: 100.0 | Testing Loss: 0.001678402884863317 | Testing Acc: 100.0\n",
            "Epoch: 3241 | Training Loss: 0.0008164782193489373 | Training Acc: 100.0 | Testing Loss: 0.0016783911269158125 | Testing Acc: 100.0\n",
            "Epoch: 3242 | Training Loss: 0.0008161187288351357 | Training Acc: 100.0 | Testing Loss: 0.0016774970572441816 | Testing Acc: 100.0\n",
            "Epoch: 3243 | Training Loss: 0.000815766747109592 | Training Acc: 100.0 | Testing Loss: 0.0016774684190750122 | Testing Acc: 100.0\n",
            "Epoch: 3244 | Training Loss: 0.000815413543023169 | Training Acc: 100.0 | Testing Loss: 0.0016765861073508859 | Testing Acc: 100.0\n",
            "Epoch: 3245 | Training Loss: 0.0008150599896907806 | Training Acc: 100.0 | Testing Loss: 0.0016765398904681206 | Testing Acc: 100.0\n",
            "Epoch: 3246 | Training Loss: 0.0008147124317474663 | Training Acc: 100.0 | Testing Loss: 0.0016756284749135375 | Testing Acc: 100.0\n",
            "Epoch: 3247 | Training Loss: 0.0008143532904796302 | Training Acc: 100.0 | Testing Loss: 0.001674775267019868 | Testing Acc: 100.0\n",
            "Epoch: 3248 | Training Loss: 0.000814013066701591 | Training Acc: 100.0 | Testing Loss: 0.0016747748013585806 | Testing Acc: 100.0\n",
            "Epoch: 3249 | Training Loss: 0.0008136508986353874 | Training Acc: 100.0 | Testing Loss: 0.001673886552453041 | Testing Acc: 100.0\n",
            "Epoch: 3250 | Training Loss: 0.0008133049122989178 | Training Acc: 100.0 | Testing Loss: 0.0016738518606871367 | Testing Acc: 100.0\n",
            "Epoch: 3251 | Training Loss: 0.0008129544439725578 | Training Acc: 100.0 | Testing Loss: 0.0016729638446122408 | Testing Acc: 100.0\n",
            "Epoch: 3252 | Training Loss: 0.0008125994354486465 | Training Acc: 100.0 | Testing Loss: 0.0016729116905480623 | Testing Acc: 100.0\n",
            "Epoch: 3253 | Training Loss: 0.0008122592116706073 | Training Acc: 100.0 | Testing Loss: 0.0016720059793442488 | Testing Acc: 100.0\n",
            "Epoch: 3254 | Training Loss: 0.000811901583801955 | Training Acc: 100.0 | Testing Loss: 0.001671140780672431 | Testing Acc: 100.0\n",
            "Epoch: 3255 | Training Loss: 0.0008115656673908234 | Training Acc: 100.0 | Testing Loss: 0.0016711235512048006 | Testing Acc: 100.0\n",
            "Epoch: 3256 | Training Loss: 0.0008112108334898949 | Training Acc: 100.0 | Testing Loss: 0.0016702410066500306 | Testing Acc: 100.0\n",
            "Epoch: 3257 | Training Loss: 0.0008108632755465806 | Training Acc: 100.0 | Testing Loss: 0.0016702174907550216 | Testing Acc: 100.0\n",
            "Epoch: 3258 | Training Loss: 0.0008105189772322774 | Training Acc: 100.0 | Testing Loss: 0.0016693233046680689 | Testing Acc: 100.0\n",
            "Epoch: 3259 | Training Loss: 0.0008101609419099987 | Training Acc: 100.0 | Testing Loss: 0.001669271499849856 | Testing Acc: 100.0\n",
            "Epoch: 3260 | Training Loss: 0.0008098192629404366 | Training Acc: 100.0 | Testing Loss: 0.0016683714929968119 | Testing Acc: 100.0\n",
            "Epoch: 3261 | Training Loss: 0.0008094658842310309 | Training Acc: 100.0 | Testing Loss: 0.0016675235237926245 | Testing Acc: 100.0\n",
            "Epoch: 3262 | Training Loss: 0.0008091210620477796 | Training Acc: 100.0 | Testing Loss: 0.0016674946527928114 | Testing Acc: 100.0\n",
            "Epoch: 3263 | Training Loss: 0.0008087751339189708 | Training Acc: 100.0 | Testing Loss: 0.0016666179290041327 | Testing Acc: 100.0\n",
            "Epoch: 3264 | Training Loss: 0.000808423210401088 | Training Acc: 100.0 | Testing Loss: 0.0016665890580043197 | Testing Acc: 100.0\n",
            "Epoch: 3265 | Training Loss: 0.0008080812986008823 | Training Acc: 100.0 | Testing Loss: 0.0016656828811392188 | Testing Acc: 100.0\n",
            "Epoch: 3266 | Training Loss: 0.0008077280363067985 | Training Acc: 100.0 | Testing Loss: 0.0016656657680869102 | Testing Acc: 100.0\n",
            "Epoch: 3267 | Training Loss: 0.0008073953213170171 | Training Acc: 100.0 | Testing Loss: 0.0016647775191813707 | Testing Acc: 100.0\n",
            "Epoch: 3268 | Training Loss: 0.0008070402545854449 | Training Acc: 100.0 | Testing Loss: 0.0016639179084450006 | Testing Acc: 100.0\n",
            "Epoch: 3269 | Training Loss: 0.0008067000890150666 | Training Acc: 100.0 | Testing Loss: 0.001663888804614544 | Testing Acc: 100.0\n",
            "Epoch: 3270 | Training Loss: 0.0008063541608862579 | Training Acc: 100.0 | Testing Loss: 0.0016630120808258653 | Testing Acc: 100.0\n",
            "Epoch: 3271 | Training Loss: 0.0008060067193582654 | Training Acc: 100.0 | Testing Loss: 0.0016629770398139954 | Testing Acc: 100.0\n",
            "Epoch: 3272 | Training Loss: 0.0008056676015257835 | Training Acc: 100.0 | Testing Loss: 0.001662094728089869 | Testing Acc: 100.0\n",
            "Epoch: 3273 | Training Loss: 0.0008053128840401769 | Training Acc: 100.0 | Testing Loss: 0.0016612407052889466 | Testing Acc: 100.0\n",
            "Epoch: 3274 | Training Loss: 0.0008049815078265965 | Training Acc: 100.0 | Testing Loss: 0.0016612580511718988 | Testing Acc: 100.0\n",
            "Epoch: 3275 | Training Loss: 0.0008046264993026853 | Training Acc: 100.0 | Testing Loss: 0.0016603926196694374 | Testing Acc: 100.0\n",
            "Epoch: 3276 | Training Loss: 0.0008042893605306745 | Training Acc: 100.0 | Testing Loss: 0.0016603464027866721 | Testing Acc: 100.0\n",
            "Epoch: 3277 | Training Loss: 0.0008039461681619287 | Training Acc: 100.0 | Testing Loss: 0.0016594750341027975 | Testing Acc: 100.0\n",
            "Epoch: 3278 | Training Loss: 0.0008035969804041088 | Training Acc: 100.0 | Testing Loss: 0.0016594346379861236 | Testing Acc: 100.0\n",
            "Epoch: 3279 | Training Loss: 0.0008032640325836837 | Training Acc: 100.0 | Testing Loss: 0.0016585461562499404 | Testing Acc: 100.0\n",
            "Epoch: 3280 | Training Loss: 0.0008029063465073705 | Training Acc: 100.0 | Testing Loss: 0.0016576979542151093 | Testing Acc: 100.0\n",
            "Epoch: 3281 | Training Loss: 0.0008025806164368987 | Training Acc: 100.0 | Testing Loss: 0.0016576685011386871 | Testing Acc: 100.0\n",
            "Epoch: 3282 | Training Loss: 0.0008022285765036941 | Training Acc: 100.0 | Testing Loss: 0.0016567973652854562 | Testing Acc: 100.0\n",
            "Epoch: 3283 | Training Loss: 0.0008018855005502701 | Training Acc: 100.0 | Testing Loss: 0.001656791428104043 | Testing Acc: 100.0\n",
            "Epoch: 3284 | Training Loss: 0.0008015484781935811 | Training Acc: 100.0 | Testing Loss: 0.0016559085343033075 | Testing Acc: 100.0\n",
            "Epoch: 3285 | Training Loss: 0.0008012022590264678 | Training Acc: 100.0 | Testing Loss: 0.0016558507923036814 | Testing Acc: 100.0\n",
            "Epoch: 3286 | Training Loss: 0.0008008678560145199 | Training Acc: 100.0 | Testing Loss: 0.001654973952099681 | Testing Acc: 100.0\n",
            "Epoch: 3287 | Training Loss: 0.0008005203562788665 | Training Acc: 100.0 | Testing Loss: 0.001654119580052793 | Testing Acc: 100.0\n",
            "Epoch: 3288 | Training Loss: 0.0008001887472346425 | Training Acc: 100.0 | Testing Loss: 0.0016541022341698408 | Testing Acc: 100.0\n",
            "Epoch: 3289 | Training Loss: 0.0007998456130735576 | Training Acc: 100.0 | Testing Loss: 0.0016532192239537835 | Testing Acc: 100.0\n",
            "Epoch: 3290 | Training Loss: 0.0007994981715455651 | Training Acc: 100.0 | Testing Loss: 0.0016531844157725573 | Testing Acc: 100.0\n",
            "Epoch: 3291 | Training Loss: 0.0007991649326868355 | Training Acc: 100.0 | Testing Loss: 0.0016523189842700958 | Testing Acc: 100.0\n",
            "Epoch: 3292 | Training Loss: 0.000798811495769769 | Training Acc: 100.0 | Testing Loss: 0.001652295934036374 | Testing Acc: 100.0\n",
            "Epoch: 3293 | Training Loss: 0.0007984860567376018 | Training Acc: 100.0 | Testing Loss: 0.0016514072194695473 | Testing Acc: 100.0\n",
            "Epoch: 3294 | Training Loss: 0.0007981398957781494 | Training Acc: 100.0 | Testing Loss: 0.0016505649546161294 | Testing Acc: 100.0\n",
            "Epoch: 3295 | Training Loss: 0.0007978070643730462 | Training Acc: 100.0 | Testing Loss: 0.001650541671551764 | Testing Acc: 100.0\n",
            "Epoch: 3296 | Training Loss: 0.0007974639302119613 | Training Acc: 100.0 | Testing Loss: 0.0016496762400493026 | Testing Acc: 100.0\n",
            "Epoch: 3297 | Training Loss: 0.0007971236482262611 | Training Acc: 100.0 | Testing Loss: 0.0016496472526341677 | Testing Acc: 100.0\n",
            "Epoch: 3298 | Training Loss: 0.0007967891287989914 | Training Acc: 100.0 | Testing Loss: 0.0016487815883010626 | Testing Acc: 100.0\n",
            "Epoch: 3299 | Training Loss: 0.0007964400574564934 | Training Acc: 100.0 | Testing Loss: 0.001647933037020266 | Testing Acc: 100.0\n",
            "Epoch: 3300 | Training Loss: 0.0007961130468174815 | Training Acc: 100.0 | Testing Loss: 0.0016479503829032183 | Testing Acc: 100.0\n",
            "Epoch: 3301 | Training Loss: 0.0007957742782309651 | Training Acc: 100.0 | Testing Loss: 0.0016470963601022959 | Testing Acc: 100.0\n",
            "Epoch: 3302 | Training Loss: 0.0007954399916343391 | Training Acc: 100.0 | Testing Loss: 0.0016470674891024828 | Testing Acc: 100.0\n",
            "Epoch: 3303 | Training Loss: 0.0007951071602292359 | Training Acc: 100.0 | Testing Loss: 0.0016462014755234122 | Testing Acc: 100.0\n",
            "Epoch: 3304 | Training Loss: 0.0007947622798383236 | Training Acc: 100.0 | Testing Loss: 0.0016461610794067383 | Testing Acc: 100.0\n",
            "Epoch: 3305 | Training Loss: 0.0007944322424009442 | Training Acc: 100.0 | Testing Loss: 0.0016452837735414505 | Testing Acc: 100.0\n",
            "Epoch: 3306 | Training Loss: 0.0007940847426652908 | Training Acc: 100.0 | Testing Loss: 0.0016444295179098845 | Testing Acc: 100.0\n",
            "Epoch: 3307 | Training Loss: 0.0007937606424093246 | Training Acc: 100.0 | Testing Loss: 0.0016444295179098845 | Testing Acc: 100.0\n",
            "Epoch: 3308 | Training Loss: 0.0007934217574074864 | Training Acc: 100.0 | Testing Loss: 0.001643575495108962 | Testing Acc: 100.0\n",
            "Epoch: 3309 | Training Loss: 0.0007930845022201538 | Training Acc: 100.0 | Testing Loss: 0.0016435638535767794 | Testing Acc: 100.0\n",
            "Epoch: 3310 | Training Loss: 0.0007927516708150506 | Training Acc: 100.0 | Testing Loss: 0.0016427157679572701 | Testing Acc: 100.0\n",
            "Epoch: 3311 | Training Loss: 0.0007924128440208733 | Training Acc: 100.0 | Testing Loss: 0.0016426576767116785 | Testing Acc: 100.0\n",
            "Epoch: 3312 | Training Loss: 0.0007920871721580625 | Training Acc: 100.0 | Testing Loss: 0.001641792245209217 | Testing Acc: 100.0\n",
            "Epoch: 3313 | Training Loss: 0.0007917425828054547 | Training Acc: 100.0 | Testing Loss: 0.001640966860577464 | Testing Acc: 100.0\n",
            "Epoch: 3314 | Training Loss: 0.0007914212765172124 | Training Acc: 100.0 | Testing Loss: 0.0016409496311098337 | Testing Acc: 100.0\n",
            "Epoch: 3315 | Training Loss: 0.0007910795393399894 | Training Acc: 100.0 | Testing Loss: 0.0016400835011154413 | Testing Acc: 100.0\n",
            "Epoch: 3316 | Training Loss: 0.000790748163126409 | Training Acc: 100.0 | Testing Loss: 0.001640048809349537 | Testing Acc: 100.0\n",
            "Epoch: 3317 | Training Loss: 0.0007904181256890297 | Training Acc: 100.0 | Testing Loss: 0.0016391886165365577 | Testing Acc: 100.0\n",
            "Epoch: 3318 | Training Loss: 0.000790076213888824 | Training Acc: 100.0 | Testing Loss: 0.001639165566302836 | Testing Acc: 100.0\n",
            "Epoch: 3319 | Training Loss: 0.0007897581090219319 | Training Acc: 100.0 | Testing Loss: 0.0016382941976189613 | Testing Acc: 100.0\n",
            "Epoch: 3320 | Training Loss: 0.0007894135196693242 | Training Acc: 100.0 | Testing Loss: 0.0016374748665839434 | Testing Acc: 100.0\n",
            "Epoch: 3321 | Training Loss: 0.0007890878478065133 | Training Acc: 100.0 | Testing Loss: 0.0016374457627534866 | Testing Acc: 100.0\n",
            "Epoch: 3322 | Training Loss: 0.0007887489045970142 | Training Acc: 100.0 | Testing Loss: 0.0016366031486541033 | Testing Acc: 100.0\n",
            "Epoch: 3323 | Training Loss: 0.0007884057122282684 | Training Acc: 100.0 | Testing Loss: 0.0016365505289286375 | Testing Acc: 100.0\n",
            "Epoch: 3324 | Training Loss: 0.0007880827179178596 | Training Acc: 100.0 | Testing Loss: 0.0016357023268938065 | Testing Acc: 100.0\n",
            "Epoch: 3325 | Training Loss: 0.0007877426105551422 | Training Acc: 100.0 | Testing Loss: 0.0016348769422620535 | Testing Acc: 100.0\n",
            "Epoch: 3326 | Training Loss: 0.0007874245056882501 | Training Acc: 100.0 | Testing Loss: 0.0016348885837942362 | Testing Acc: 100.0\n",
            "Epoch: 3327 | Training Loss: 0.0007870917324908078 | Training Acc: 100.0 | Testing Loss: 0.0016340514412149787 | Testing Acc: 100.0\n",
            "Epoch: 3328 | Training Loss: 0.0007867645472288132 | Training Acc: 100.0 | Testing Loss: 0.0016340225702151656 | Testing Acc: 100.0\n",
            "Epoch: 3329 | Training Loss: 0.0007864314829930663 | Training Acc: 100.0 | Testing Loss: 0.0016331564402207732 | Testing Acc: 100.0\n",
            "Epoch: 3330 | Training Loss: 0.0007860941695980728 | Training Acc: 100.0 | Testing Loss: 0.0016331275692209601 | Testing Acc: 100.0\n",
            "Epoch: 3331 | Training Loss: 0.0007857728051021695 | Training Acc: 100.0 | Testing Loss: 0.0016322616720572114 | Testing Acc: 100.0\n",
            "Epoch: 3332 | Training Loss: 0.0007854310679249465 | Training Acc: 100.0 | Testing Loss: 0.0016314301174134016 | Testing Acc: 100.0\n",
            "Epoch: 3333 | Training Loss: 0.0007851129630580544 | Training Acc: 100.0 | Testing Loss: 0.001631418475881219 | Testing Acc: 100.0\n",
            "Epoch: 3334 | Training Loss: 0.0007847797241993248 | Training Acc: 100.0 | Testing Loss: 0.0016305875033140182 | Testing Acc: 100.0\n",
            "Epoch: 3335 | Training Loss: 0.0007844498613849282 | Training Acc: 100.0 | Testing Loss: 0.0016305582830682397 | Testing Acc: 100.0\n",
            "Epoch: 3336 | Training Loss: 0.0007841228507459164 | Training Acc: 100.0 | Testing Loss: 0.0016297269612550735 | Testing Acc: 100.0\n",
            "Epoch: 3337 | Training Loss: 0.0007837897283025086 | Training Acc: 100.0 | Testing Loss: 0.0016296689864248037 | Testing Acc: 100.0\n",
            "Epoch: 3338 | Training Loss: 0.0007834698772057891 | Training Acc: 100.0 | Testing Loss: 0.0016288207843899727 | Testing Acc: 100.0\n",
            "Epoch: 3339 | Training Loss: 0.0007831386174075305 | Training Acc: 100.0 | Testing Loss: 0.0016280009876936674 | Testing Acc: 100.0\n",
            "Epoch: 3340 | Training Loss: 0.0007828160305507481 | Training Acc: 100.0 | Testing Loss: 0.001627977704629302 | Testing Acc: 100.0\n",
            "Epoch: 3341 | Training Loss: 0.0007824827916920185 | Training Acc: 100.0 | Testing Loss: 0.0016271404456347227 | Testing Acc: 100.0\n",
            "Epoch: 3342 | Training Loss: 0.0007821557810530066 | Training Acc: 100.0 | Testing Loss: 0.0016270941123366356 | Testing Acc: 100.0\n",
            "Epoch: 3343 | Training Loss: 0.0007818300509825349 | Training Acc: 100.0 | Testing Loss: 0.0016262572025880218 | Testing Acc: 100.0\n",
            "Epoch: 3344 | Training Loss: 0.0007814955897629261 | Training Acc: 100.0 | Testing Loss: 0.0016262283315882087 | Testing Acc: 100.0\n",
            "Epoch: 3345 | Training Loss: 0.0007811802206560969 | Training Acc: 100.0 | Testing Loss: 0.0016253854846581817 | Testing Acc: 100.0\n",
            "Epoch: 3346 | Training Loss: 0.0007808444788679481 | Training Acc: 100.0 | Testing Loss: 0.001624577445909381 | Testing Acc: 100.0\n",
            "Epoch: 3347 | Training Loss: 0.0007805305649526417 | Training Acc: 100.0 | Testing Loss: 0.0016245364677160978 | Testing Acc: 100.0\n",
            "Epoch: 3348 | Training Loss: 0.0007802004693076015 | Training Acc: 100.0 | Testing Loss: 0.001623693504370749 | Testing Acc: 100.0\n",
            "Epoch: 3349 | Training Loss: 0.0007798675214871764 | Training Acc: 100.0 | Testing Loss: 0.0016236469382420182 | Testing Acc: 100.0\n",
            "Epoch: 3350 | Training Loss: 0.0007795491837896407 | Training Acc: 100.0 | Testing Loss: 0.0016228215536102653 | Testing Acc: 100.0\n",
            "Epoch: 3351 | Training Loss: 0.0007792132091708481 | Training Acc: 100.0 | Testing Loss: 0.0016220074612647295 | Testing Acc: 100.0\n",
            "Epoch: 3352 | Training Loss: 0.0007789023220539093 | Training Acc: 100.0 | Testing Loss: 0.0016220072284340858 | Testing Acc: 100.0\n",
            "Epoch: 3353 | Training Loss: 0.0007785724592395127 | Training Acc: 100.0 | Testing Loss: 0.0016211814945563674 | Testing Acc: 100.0\n",
            "Epoch: 3354 | Training Loss: 0.0007782435859553516 | Training Acc: 100.0 | Testing Loss: 0.0016211526235565543 | Testing Acc: 100.0\n",
            "Epoch: 3355 | Training Loss: 0.0007779194274917245 | Training Acc: 100.0 | Testing Loss: 0.0016203094273805618 | Testing Acc: 100.0\n",
            "Epoch: 3356 | Training Loss: 0.0007775939302518964 | Training Acc: 100.0 | Testing Loss: 0.0016202690312638879 | Testing Acc: 100.0\n",
            "Epoch: 3357 | Training Loss: 0.0007772826938889921 | Training Acc: 100.0 | Testing Loss: 0.0016194141935557127 | Testing Acc: 100.0\n",
            "Epoch: 3358 | Training Loss: 0.0007769468938931823 | Training Acc: 100.0 | Testing Loss: 0.0016186119755730033 | Testing Acc: 100.0\n",
            "Epoch: 3359 | Training Loss: 0.0007766301860101521 | Training Acc: 100.0 | Testing Loss: 0.001618588692508638 | Testing Acc: 100.0\n",
            "Epoch: 3360 | Training Loss: 0.0007763043977320194 | Training Acc: 100.0 | Testing Loss: 0.0016177803045138717 | Testing Acc: 100.0\n",
            "Epoch: 3361 | Training Loss: 0.0007759846630506217 | Training Acc: 100.0 | Testing Loss: 0.0016177746001631021 | Testing Acc: 100.0\n",
            "Epoch: 3362 | Training Loss: 0.0007756605045869946 | Training Acc: 100.0 | Testing Loss: 0.0016169256996363401 | Testing Acc: 100.0\n",
            "Epoch: 3363 | Training Loss: 0.0007753305835649371 | Training Acc: 100.0 | Testing Loss: 0.0016168911242857575 | Testing Acc: 100.0\n",
            "Epoch: 3364 | Training Loss: 0.0007750237127766013 | Training Acc: 100.0 | Testing Loss: 0.0016160362865775824 | Testing Acc: 100.0\n",
            "Epoch: 3365 | Training Loss: 0.0007746878545731306 | Training Acc: 100.0 | Testing Loss: 0.0016152278985828161 | Testing Acc: 100.0\n",
            "Epoch: 3366 | Training Loss: 0.0007743711466901004 | Training Acc: 100.0 | Testing Loss: 0.001615204499103129 | Testing Acc: 100.0\n",
            "Epoch: 3367 | Training Loss: 0.0007740496657788754 | Training Acc: 100.0 | Testing Loss: 0.0016143847024068236 | Testing Acc: 100.0\n",
            "Epoch: 3368 | Training Loss: 0.0007737210835330188 | Training Acc: 100.0 | Testing Loss: 0.001614344073459506 | Testing Acc: 100.0\n",
            "Epoch: 3369 | Training Loss: 0.0007734011742286384 | Training Acc: 100.0 | Testing Loss: 0.001613512635231018 | Testing Acc: 100.0\n",
            "Epoch: 3370 | Training Loss: 0.000773077190387994 | Training Acc: 100.0 | Testing Loss: 0.0016134893521666527 | Testing Acc: 100.0\n",
            "Epoch: 3371 | Training Loss: 0.0007727644988335669 | Training Acc: 100.0 | Testing Loss: 0.0016126580303534865 | Testing Acc: 100.0\n",
            "Epoch: 3372 | Training Loss: 0.0007724345196038485 | Training Acc: 100.0 | Testing Loss: 0.0016118437051773071 | Testing Acc: 100.0\n",
            "Epoch: 3373 | Training Loss: 0.0007721207803115249 | Training Acc: 100.0 | Testing Loss: 0.0016118321800604463 | Testing Acc: 100.0\n",
            "Epoch: 3374 | Training Loss: 0.0007717992411926389 | Training Acc: 100.0 | Testing Loss: 0.0016110122669488192 | Testing Acc: 100.0\n",
            "Epoch: 3375 | Training Loss: 0.0007714796811342239 | Training Acc: 100.0 | Testing Loss: 0.0016109716380015016 | Testing Acc: 100.0\n",
            "Epoch: 3376 | Training Loss: 0.0007711612270213664 | Training Acc: 100.0 | Testing Loss: 0.0016101456712931395 | Testing Acc: 100.0\n",
            "Epoch: 3377 | Training Loss: 0.0007708384655416012 | Training Acc: 100.0 | Testing Loss: 0.001609348924830556 | Testing Acc: 100.0\n",
            "Epoch: 3378 | Training Loss: 0.0007705260068178177 | Training Acc: 100.0 | Testing Loss: 0.0016093486919999123 | Testing Acc: 100.0\n",
            "Epoch: 3379 | Training Loss: 0.0007702003931626678 | Training Acc: 100.0 | Testing Loss: 0.0016085400711745024 | Testing Acc: 100.0\n",
            "Epoch: 3380 | Training Loss: 0.0007698881672695279 | Training Acc: 100.0 | Testing Loss: 0.0016084995586425066 | Testing Acc: 100.0\n",
            "Epoch: 3381 | Training Loss: 0.0007695680251345038 | Training Acc: 100.0 | Testing Loss: 0.0016076795291155577 | Testing Acc: 100.0\n",
            "Epoch: 3382 | Training Loss: 0.0007692454964853823 | Training Acc: 100.0 | Testing Loss: 0.0016076334286481142 | Testing Acc: 100.0\n",
            "Epoch: 3383 | Training Loss: 0.0007689386839047074 | Training Acc: 100.0 | Testing Loss: 0.0016068073455244303 | Testing Acc: 100.0\n",
            "Epoch: 3384 | Training Loss: 0.0007686145254410803 | Training Acc: 100.0 | Testing Loss: 0.001606004312634468 | Testing Acc: 100.0\n",
            "Epoch: 3385 | Training Loss: 0.0007683021249249578 | Training Acc: 100.0 | Testing Loss: 0.0016059810295701027 | Testing Acc: 100.0\n",
            "Epoch: 3386 | Training Loss: 0.0007679807604290545 | Training Acc: 100.0 | Testing Loss: 0.0016051784623414278 | Testing Acc: 100.0\n",
            "Epoch: 3387 | Training Loss: 0.000767662248108536 | Training Acc: 100.0 | Testing Loss: 0.0016051668208092451 | Testing Acc: 100.0\n",
            "Epoch: 3388 | Training Loss: 0.000767345423810184 | Training Acc: 100.0 | Testing Loss: 0.0016043467912822962 | Testing Acc: 100.0\n",
            "Epoch: 3389 | Training Loss: 0.0007670199265703559 | Training Acc: 100.0 | Testing Loss: 0.0016042946372181177 | Testing Acc: 100.0\n",
            "Epoch: 3390 | Training Loss: 0.0007667131721973419 | Training Acc: 100.0 | Testing Loss: 0.0016034686705097556 | Testing Acc: 100.0\n",
            "Epoch: 3391 | Training Loss: 0.0007663875003345311 | Training Acc: 100.0 | Testing Loss: 0.0016026657540351152 | Testing Acc: 100.0\n",
            "Epoch: 3392 | Training Loss: 0.0007660809787921607 | Training Acc: 100.0 | Testing Loss: 0.001602648408152163 | Testing Acc: 100.0\n",
            "Epoch: 3393 | Training Loss: 0.0007657640380784869 | Training Acc: 100.0 | Testing Loss: 0.0016018336173146963 | Testing Acc: 100.0\n",
            "Epoch: 3394 | Training Loss: 0.0007654485525563359 | Training Acc: 100.0 | Testing Loss: 0.0016017990419641137 | Testing Acc: 100.0\n",
            "Epoch: 3395 | Training Loss: 0.0007651374908164144 | Training Acc: 100.0 | Testing Loss: 0.001600978896021843 | Testing Acc: 100.0\n",
            "Epoch: 3396 | Training Loss: 0.0007648133905604482 | Training Acc: 100.0 | Testing Loss: 0.0016009558457881212 | Testing Acc: 100.0\n",
            "Epoch: 3397 | Training Loss: 0.0007645124569535255 | Training Acc: 100.0 | Testing Loss: 0.001600123941898346 | Testing Acc: 100.0\n",
            "Epoch: 3398 | Training Loss: 0.0007641854463145137 | Training Acc: 100.0 | Testing Loss: 0.0015993152046576142 | Testing Acc: 100.0\n",
            "Epoch: 3399 | Training Loss: 0.0007638715906068683 | Training Acc: 100.0 | Testing Loss: 0.0015992861008271575 | Testing Acc: 100.0\n",
            "Epoch: 3400 | Training Loss: 0.0007635619258508086 | Training Acc: 100.0 | Testing Loss: 0.0015985008794814348 | Testing Acc: 100.0\n",
            "Epoch: 3401 | Training Loss: 0.0007632449269294739 | Training Acc: 100.0 | Testing Loss: 0.001598460366949439 | Testing Acc: 100.0\n",
            "Epoch: 3402 | Training Loss: 0.0007629382307641208 | Training Acc: 100.0 | Testing Loss: 0.0015976341674104333 | Testing Acc: 100.0\n",
            "Epoch: 3403 | Training Loss: 0.0007626127335242927 | Training Acc: 100.0 | Testing Loss: 0.0015968482475727797 | Testing Acc: 100.0\n",
            "Epoch: 3404 | Training Loss: 0.0007623119163326919 | Training Acc: 100.0 | Testing Loss: 0.001596854068338871 | Testing Acc: 100.0\n",
            "Epoch: 3405 | Training Loss: 0.0007619891548529267 | Training Acc: 100.0 | Testing Loss: 0.0015960568562150002 | Testing Acc: 100.0\n",
            "Epoch: 3406 | Training Loss: 0.0007616768125444651 | Training Acc: 100.0 | Testing Loss: 0.0015960162272676826 | Testing Acc: 100.0\n",
            "Epoch: 3407 | Training Loss: 0.0007613641791976988 | Training Acc: 100.0 | Testing Loss: 0.0015952251851558685 | Testing Acc: 100.0\n",
            "Epoch: 3408 | Training Loss: 0.0007610502652823925 | Training Acc: 100.0 | Testing Loss: 0.00159517303109169 | Testing Acc: 100.0\n",
            "Epoch: 3409 | Training Loss: 0.0007607419393025339 | Training Acc: 100.0 | Testing Loss: 0.0015943702310323715 | Testing Acc: 100.0\n",
            "Epoch: 3410 | Training Loss: 0.0007604282582178712 | Training Acc: 100.0 | Testing Loss: 0.0015935845440253615 | Testing Acc: 100.0\n",
            "Epoch: 3411 | Training Loss: 0.000760123017244041 | Training Acc: 100.0 | Testing Loss: 0.0015935497358441353 | Testing Acc: 100.0\n",
            "Epoch: 3412 | Training Loss: 0.0007598074153065681 | Training Acc: 100.0 | Testing Loss: 0.0015927525237202644 | Testing Acc: 100.0\n",
            "Epoch: 3413 | Training Loss: 0.0007594935595989227 | Training Acc: 100.0 | Testing Loss: 0.0015927525237202644 | Testing Acc: 100.0\n",
            "Epoch: 3414 | Training Loss: 0.0007591868052259088 | Training Acc: 100.0 | Testing Loss: 0.0015919437864795327 | Testing Acc: 100.0\n",
            "Epoch: 3415 | Training Loss: 0.0007588670705445111 | Training Acc: 100.0 | Testing Loss: 0.0015918916324153543 | Testing Acc: 100.0\n",
            "Epoch: 3416 | Training Loss: 0.0007585676503367722 | Training Acc: 100.0 | Testing Loss: 0.0015910830115899444 | Testing Acc: 100.0\n",
            "Epoch: 3417 | Training Loss: 0.0007582477992400527 | Training Acc: 100.0 | Testing Loss: 0.0015902913874015212 | Testing Acc: 100.0\n",
            "Epoch: 3418 | Training Loss: 0.0007579413359053433 | Training Acc: 100.0 | Testing Loss: 0.0015902681043371558 | Testing Acc: 100.0\n",
            "Epoch: 3419 | Training Loss: 0.0007576301577500999 | Training Acc: 100.0 | Testing Loss: 0.0015894767129793763 | Testing Acc: 100.0\n",
            "Epoch: 3420 | Training Loss: 0.0007573162438347936 | Training Acc: 100.0 | Testing Loss: 0.00158944190479815 | Testing Acc: 100.0\n",
            "Epoch: 3421 | Training Loss: 0.0007570107700303197 | Training Acc: 100.0 | Testing Loss: 0.0015886388719081879 | Testing Acc: 100.0\n",
            "Epoch: 3422 | Training Loss: 0.0007566941203549504 | Training Acc: 100.0 | Testing Loss: 0.0015886158216744661 | Testing Acc: 100.0\n",
            "Epoch: 3423 | Training Loss: 0.0007564006373286247 | Training Acc: 100.0 | Testing Loss: 0.0015878069680184126 | Testing Acc: 100.0\n",
            "Epoch: 3424 | Training Loss: 0.0007560807280242443 | Training Acc: 100.0 | Testing Loss: 0.0015870208153501153 | Testing Acc: 100.0\n",
            "Epoch: 3425 | Training Loss: 0.0007557786884717643 | Training Acc: 100.0 | Testing Loss: 0.001586986007168889 | Testing Acc: 100.0\n",
            "Epoch: 3426 | Training Loss: 0.0007554659969173372 | Training Acc: 100.0 | Testing Loss: 0.0015861829742789268 | Testing Acc: 100.0\n",
            "Epoch: 3427 | Training Loss: 0.000755156681407243 | Training Acc: 100.0 | Testing Loss: 0.0015861481660977006 | Testing Acc: 100.0\n",
            "Epoch: 3428 | Training Loss: 0.0007548540597781539 | Training Acc: 100.0 | Testing Loss: 0.001585356891155243 | Testing Acc: 100.0\n",
            "Epoch: 3429 | Training Loss: 0.0007545388070866466 | Training Acc: 100.0 | Testing Loss: 0.0015845764428377151 | Testing Acc: 100.0\n",
            "Epoch: 3430 | Training Loss: 0.0007542454404756427 | Training Acc: 100.0 | Testing Loss: 0.001584587967954576 | Testing Acc: 100.0\n",
            "Epoch: 3431 | Training Loss: 0.0007539342623203993 | Training Acc: 100.0 | Testing Loss: 0.0015837906394153833 | Testing Acc: 100.0\n",
            "Epoch: 3432 | Training Loss: 0.0007536278571933508 | Training Acc: 100.0 | Testing Loss: 0.0015837557148188353 | Testing Acc: 100.0\n",
            "Epoch: 3433 | Training Loss: 0.000753313594032079 | Training Acc: 100.0 | Testing Loss: 0.0015829699113965034 | Testing Acc: 100.0\n",
            "Epoch: 3434 | Training Loss: 0.0007530010188929737 | Training Acc: 100.0 | Testing Loss: 0.0015829235780984163 | Testing Acc: 100.0\n",
            "Epoch: 3435 | Training Loss: 0.0007527029956690967 | Training Acc: 100.0 | Testing Loss: 0.0015821264823898673 | Testing Acc: 100.0\n",
            "Epoch: 3436 | Training Loss: 0.0007523862877860665 | Training Acc: 100.0 | Testing Loss: 0.001581346383318305 | Testing Acc: 100.0\n",
            "Epoch: 3437 | Training Loss: 0.0007520855870097876 | Training Acc: 100.0 | Testing Loss: 0.001581346383318305 | Testing Acc: 100.0\n",
            "Epoch: 3438 | Training Loss: 0.0007517801132053137 | Training Acc: 100.0 | Testing Loss: 0.0015805369475856423 | Testing Acc: 100.0\n",
            "Epoch: 3439 | Training Loss: 0.0007514706812798977 | Training Acc: 100.0 | Testing Loss: 0.0015805196017026901 | Testing Acc: 100.0\n",
            "Epoch: 3440 | Training Loss: 0.0007511713192798197 | Training Acc: 100.0 | Testing Loss: 0.001579727977514267 | Testing Acc: 100.0\n",
            "Epoch: 3441 | Training Loss: 0.0007508602575398982 | Training Acc: 100.0 | Testing Loss: 0.0015796817606315017 | Testing Acc: 100.0\n",
            "Epoch: 3442 | Training Loss: 0.0007505592075176537 | Training Acc: 100.0 | Testing Loss: 0.0015788845485076308 | Testing Acc: 100.0\n",
            "Epoch: 3443 | Training Loss: 0.0007502484368160367 | Training Acc: 100.0 | Testing Loss: 0.0015781217953190207 | Testing Acc: 100.0\n",
            "Epoch: 3444 | Training Loss: 0.0007499475032091141 | Training Acc: 100.0 | Testing Loss: 0.001578098745085299 | Testing Acc: 100.0\n",
            "Epoch: 3445 | Training Loss: 0.0007496422622352839 | Training Acc: 100.0 | Testing Loss: 0.0015773068880662322 | Testing Acc: 100.0\n",
            "Epoch: 3446 | Training Loss: 0.0007493342855013907 | Training Acc: 100.0 | Testing Loss: 0.0015772603219375014 | Testing Acc: 100.0\n",
            "Epoch: 3447 | Training Loss: 0.0007490393472835422 | Training Acc: 100.0 | Testing Loss: 0.0015764685813337564 | Testing Acc: 100.0\n",
            "Epoch: 3448 | Training Loss: 0.0007487281691282988 | Training Acc: 100.0 | Testing Loss: 0.0015757291112095118 | Testing Acc: 100.0\n",
            "Epoch: 3449 | Training Loss: 0.0007484350353479385 | Training Acc: 100.0 | Testing Loss: 0.0015757115324959159 | Testing Acc: 100.0\n",
            "Epoch: 3450 | Training Loss: 0.0007481238571926951 | Training Acc: 100.0 | Testing Loss: 0.001574954716488719 | Testing Acc: 100.0\n",
            "Epoch: 3451 | Training Loss: 0.0007478214101865888 | Training Acc: 100.0 | Testing Loss: 0.0015749083831906319 | Testing Acc: 100.0\n",
            "Epoch: 3452 | Training Loss: 0.0007475116290152073 | Training Acc: 100.0 | Testing Loss: 0.0015741105889901519 | Testing Acc: 100.0\n",
            "Epoch: 3453 | Training Loss: 0.0007472066790796816 | Training Acc: 100.0 | Testing Loss: 0.0015740642556920648 | Testing Acc: 100.0\n",
            "Epoch: 3454 | Training Loss: 0.0007469159900210798 | Training Acc: 100.0 | Testing Loss: 0.0015732668107375503 | Testing Acc: 100.0\n",
            "Epoch: 3455 | Training Loss: 0.000746606383472681 | Training Acc: 100.0 | Testing Loss: 0.001572515582665801 | Testing Acc: 100.0\n",
            "Epoch: 3456 | Training Loss: 0.0007463101064786315 | Training Acc: 100.0 | Testing Loss: 0.0015725037083029747 | Testing Acc: 100.0\n",
            "Epoch: 3457 | Training Loss: 0.000745998986531049 | Training Acc: 100.0 | Testing Loss: 0.0015717294299975038 | Testing Acc: 100.0\n",
            "Epoch: 3458 | Training Loss: 0.0007457026513293386 | Training Acc: 100.0 | Testing Loss: 0.0015716888010501862 | Testing Acc: 100.0\n",
            "Epoch: 3459 | Training Loss: 0.0007454031147062778 | Training Acc: 100.0 | Testing Loss: 0.001570908585563302 | Testing Acc: 100.0\n",
            "Epoch: 3460 | Training Loss: 0.0007450951961800456 | Training Acc: 100.0 | Testing Loss: 0.0015708680730313063 | Testing Acc: 100.0\n",
            "Epoch: 3461 | Training Loss: 0.0007448086980730295 | Training Acc: 100.0 | Testing Loss: 0.0015700648073107004 | Testing Acc: 100.0\n",
            "Epoch: 3462 | Training Loss: 0.0007444979273714125 | Training Acc: 100.0 | Testing Loss: 0.0015693133464083076 | Testing Acc: 100.0\n",
            "Epoch: 3463 | Training Loss: 0.0007442001951858401 | Training Acc: 100.0 | Testing Loss: 0.0015692844754084945 | Testing Acc: 100.0\n",
            "Epoch: 3464 | Training Loss: 0.00074389623478055 | Training Acc: 100.0 | Testing Loss: 0.0015685275429859757 | Testing Acc: 100.0\n",
            "Epoch: 3465 | Training Loss: 0.0007435983861796558 | Training Acc: 100.0 | Testing Loss: 0.001568498439155519 | Testing Acc: 100.0\n",
            "Epoch: 3466 | Training Loss: 0.0007433049031533301 | Training Acc: 100.0 | Testing Loss: 0.0015677122864872217 | Testing Acc: 100.0\n",
            "Epoch: 3467 | Training Loss: 0.0007429999532178044 | Training Acc: 100.0 | Testing Loss: 0.001566960709169507 | Testing Acc: 100.0\n",
            "Epoch: 3468 | Training Loss: 0.0007427107775583863 | Training Acc: 100.0 | Testing Loss: 0.0015669433632865548 | Testing Acc: 100.0\n",
            "Epoch: 3469 | Training Loss: 0.0007424009963870049 | Training Acc: 100.0 | Testing Loss: 0.001566174440085888 | Testing Acc: 100.0\n",
            "Epoch: 3470 | Training Loss: 0.0007421047776006162 | Training Acc: 100.0 | Testing Loss: 0.0015661397483199835 | Testing Acc: 100.0\n",
            "Epoch: 3471 | Training Loss: 0.0007418080931529403 | Training Acc: 100.0 | Testing Loss: 0.0015653653535991907 | Testing Acc: 100.0\n",
            "Epoch: 3472 | Training Loss: 0.0007415014551952481 | Training Acc: 100.0 | Testing Loss: 0.001565324841067195 | Testing Acc: 100.0\n",
            "Epoch: 3473 | Training Loss: 0.0007412107661366463 | Training Acc: 100.0 | Testing Loss: 0.0015645388048142195 | Testing Acc: 100.0\n",
            "Epoch: 3474 | Training Loss: 0.0007409057579934597 | Training Acc: 100.0 | Testing Loss: 0.0015638163313269615 | Testing Acc: 100.0\n",
            "Epoch: 3475 | Training Loss: 0.0007406242657452822 | Training Acc: 100.0 | Testing Loss: 0.0015637815231457353 | Testing Acc: 100.0\n",
            "Epoch: 3476 | Training Loss: 0.0007403114577755332 | Training Acc: 100.0 | Testing Loss: 0.0015630125999450684 | Testing Acc: 100.0\n",
            "Epoch: 3477 | Training Loss: 0.0007400182075798512 | Training Acc: 100.0 | Testing Loss: 0.0015629895497113466 | Testing Acc: 100.0\n",
            "Epoch: 3478 | Training Loss: 0.0007397198933176696 | Training Acc: 100.0 | Testing Loss: 0.0015622033970430493 | Testing Acc: 100.0\n",
            "Epoch: 3479 | Training Loss: 0.0007394177955575287 | Training Acc: 100.0 | Testing Loss: 0.0015621513593941927 | Testing Acc: 100.0\n",
            "Epoch: 3480 | Training Loss: 0.0007391314720734954 | Training Acc: 100.0 | Testing Loss: 0.0015613592695444822 | Testing Acc: 100.0\n",
            "Epoch: 3481 | Training Loss: 0.0007388234371319413 | Training Acc: 100.0 | Testing Loss: 0.0015606137458235025 | Testing Acc: 100.0\n",
            "Epoch: 3482 | Training Loss: 0.0007385329226963222 | Training Acc: 100.0 | Testing Loss: 0.0015606137458235025 | Testing Acc: 100.0\n",
            "Epoch: 3483 | Training Loss: 0.0007382336189039052 | Training Acc: 100.0 | Testing Loss: 0.0015598388854414225 | Testing Acc: 100.0\n",
            "Epoch: 3484 | Training Loss: 0.0007379388553090394 | Training Acc: 100.0 | Testing Loss: 0.0015598040772601962 | Testing Acc: 100.0\n",
            "Epoch: 3485 | Training Loss: 0.0007376478752121329 | Training Acc: 100.0 | Testing Loss: 0.001559017808176577 | Testing Acc: 100.0\n",
            "Epoch: 3486 | Training Loss: 0.0007373428670689464 | Training Acc: 100.0 | Testing Loss: 0.0015589774120599031 | Testing Acc: 100.0\n",
            "Epoch: 3487 | Training Loss: 0.0007370609091594815 | Training Acc: 100.0 | Testing Loss: 0.001558202668093145 | Testing Acc: 100.0\n",
            "Epoch: 3488 | Training Loss: 0.0007367526995949447 | Training Acc: 100.0 | Testing Loss: 0.0015574452700093389 | Testing Acc: 100.0\n",
            "Epoch: 3489 | Training Loss: 0.0007364653283730149 | Training Acc: 100.0 | Testing Loss: 0.0015574163990095258 | Testing Acc: 100.0\n",
            "Epoch: 3490 | Training Loss: 0.0007361657917499542 | Training Acc: 100.0 | Testing Loss: 0.001556658884510398 | Testing Acc: 100.0\n",
            "Epoch: 3491 | Training Loss: 0.0007358680013567209 | Training Acc: 100.0 | Testing Loss: 0.001556647359393537 | Testing Acc: 100.0\n",
            "Epoch: 3492 | Training Loss: 0.0007355801644735038 | Training Acc: 100.0 | Testing Loss: 0.0015558728482574224 | Testing Acc: 100.0\n",
            "Epoch: 3493 | Training Loss: 0.000735276669729501 | Training Acc: 100.0 | Testing Loss: 0.0015551329124718904 | Testing Acc: 100.0\n",
            "Epoch: 3494 | Training Loss: 0.0007349920924752951 | Training Acc: 100.0 | Testing Loss: 0.0015551154501736164 | Testing Acc: 100.0\n",
            "Epoch: 3495 | Training Loss: 0.0007346953498199582 | Training Acc: 100.0 | Testing Loss: 0.0015543580520898104 | Testing Acc: 100.0\n",
            "Epoch: 3496 | Training Loss: 0.0007344019832089543 | Training Acc: 100.0 | Testing Loss: 0.001554317306727171 | Testing Acc: 100.0\n",
            "Epoch: 3497 | Training Loss: 0.0007341068121604621 | Training Acc: 100.0 | Testing Loss: 0.0015535425627604127 | Testing Acc: 100.0\n",
            "Epoch: 3498 | Training Loss: 0.000733813620172441 | Training Acc: 100.0 | Testing Loss: 0.0015535135753452778 | Testing Acc: 100.0\n",
            "Epoch: 3499 | Training Loss: 0.0007335254922509193 | Training Acc: 100.0 | Testing Loss: 0.0015527389477938414 | Testing Acc: 100.0\n",
            "Epoch: 3500 | Training Loss: 0.0007332204259000719 | Training Acc: 100.0 | Testing Loss: 0.0015520164743065834 | Testing Acc: 100.0\n",
            "Epoch: 3501 | Training Loss: 0.0007329376530833542 | Training Acc: 100.0 | Testing Loss: 0.0015519930748268962 | Testing Acc: 100.0\n",
            "Epoch: 3502 | Training Loss: 0.0007326452760025859 | Training Acc: 100.0 | Testing Loss: 0.0015512416139245033 | Testing Acc: 100.0\n",
            "Epoch: 3503 | Training Loss: 0.0007323459722101688 | Training Acc: 100.0 | Testing Loss: 0.0015512066893279552 | Testing Acc: 100.0\n",
            "Epoch: 3504 | Training Loss: 0.0007320566801354289 | Training Acc: 100.0 | Testing Loss: 0.0015504377661272883 | Testing Acc: 100.0\n",
            "Epoch: 3505 | Training Loss: 0.0007317588315345347 | Training Acc: 100.0 | Testing Loss: 0.0015503913164138794 | Testing Acc: 100.0\n",
            "Epoch: 3506 | Training Loss: 0.0007314768154174089 | Training Acc: 100.0 | Testing Loss: 0.001549610635265708 | Testing Acc: 100.0\n",
            "Epoch: 3507 | Training Loss: 0.0007311731460504234 | Training Acc: 100.0 | Testing Loss: 0.0015488648787140846 | Testing Acc: 100.0\n",
            "Epoch: 3508 | Training Loss: 0.0007308917120099068 | Training Acc: 100.0 | Testing Loss: 0.0015488704666495323 | Testing Acc: 100.0\n",
            "Epoch: 3509 | Training Loss: 0.000730593572370708 | Training Acc: 100.0 | Testing Loss: 0.001548113184981048 | Testing Acc: 100.0\n",
            "Epoch: 3510 | Training Loss: 0.0007303002057597041 | Training Acc: 100.0 | Testing Loss: 0.0015480666188523173 | Testing Acc: 100.0\n",
            "Epoch: 3511 | Training Loss: 0.0007300167926587164 | Training Acc: 100.0 | Testing Loss: 0.0015473151579499245 | Testing Acc: 100.0\n",
            "Epoch: 3512 | Training Loss: 0.0007297188858501613 | Training Acc: 100.0 | Testing Loss: 0.0015465689357370138 | Testing Acc: 100.0\n",
            "Epoch: 3513 | Training Loss: 0.0007294432143680751 | Training Acc: 100.0 | Testing Loss: 0.0015465456526726484 | Testing Acc: 100.0\n",
            "Epoch: 3514 | Training Loss: 0.0007291495567187667 | Training Acc: 100.0 | Testing Loss: 0.0015457997797057033 | Testing Acc: 100.0\n",
            "Epoch: 3515 | Training Loss: 0.0007288577617146075 | Training Acc: 100.0 | Testing Loss: 0.0015457706758752465 | Testing Acc: 100.0\n",
            "Epoch: 3516 | Training Loss: 0.0007285667234100401 | Training Acc: 100.0 | Testing Loss: 0.0015450192149728537 | Testing Acc: 100.0\n",
            "Epoch: 3517 | Training Loss: 0.00072827487019822 | Training Acc: 100.0 | Testing Loss: 0.001545001519843936 | Testing Acc: 100.0\n",
            "Epoch: 3518 | Training Loss: 0.0007279945421032608 | Training Acc: 100.0 | Testing Loss: 0.0015442383009940386 | Testing Acc: 100.0\n",
            "Epoch: 3519 | Training Loss: 0.0007276980904862285 | Training Acc: 100.0 | Testing Loss: 0.0015434982487931848 | Testing Acc: 100.0\n",
            "Epoch: 3520 | Training Loss: 0.0007274105446413159 | Training Acc: 100.0 | Testing Loss: 0.001543469144962728 | Testing Acc: 100.0\n",
            "Epoch: 3521 | Training Loss: 0.0007271183421835303 | Training Acc: 100.0 | Testing Loss: 0.0015427233884111047 | Testing Acc: 100.0\n",
            "Epoch: 3522 | Training Loss: 0.0007268309709616005 | Training Acc: 100.0 | Testing Loss: 0.001542694284580648 | Testing Acc: 100.0\n",
            "Epoch: 3523 | Training Loss: 0.0007265444146469235 | Training Acc: 100.0 | Testing Loss: 0.0015419370029121637 | Testing Acc: 100.0\n",
            "Epoch: 3524 | Training Loss: 0.0007262540748342872 | Training Acc: 100.0 | Testing Loss: 0.0015418787952512503 | Testing Acc: 100.0\n",
            "Epoch: 3525 | Training Loss: 0.0007259735139086843 | Training Acc: 100.0 | Testing Loss: 0.0015411211643368006 | Testing Acc: 100.0\n",
            "Epoch: 3526 | Training Loss: 0.0007256800308823586 | Training Acc: 100.0 | Testing Loss: 0.0015404040459543467 | Testing Acc: 100.0\n",
            "Epoch: 3527 | Training Loss: 0.0007253986550495028 | Training Acc: 100.0 | Testing Loss: 0.001540380995720625 | Testing Acc: 100.0\n",
            "Epoch: 3528 | Training Loss: 0.0007251093629747629 | Training Acc: 100.0 | Testing Loss: 0.0015396465314552188 | Testing Acc: 100.0\n",
            "Epoch: 3529 | Training Loss: 0.0007248201873153448 | Training Acc: 100.0 | Testing Loss: 0.0015396001981571317 | Testing Acc: 100.0\n",
            "Epoch: 3530 | Training Loss: 0.0007245367160066962 | Training Acc: 100.0 | Testing Loss: 0.0015388543251901865 | Testing Acc: 100.0\n",
            "Epoch: 3531 | Training Loss: 0.0007242404972203076 | Training Acc: 100.0 | Testing Loss: 0.0015381256816908717 | Testing Acc: 100.0\n",
            "Epoch: 3532 | Training Loss: 0.000723961740732193 | Training Acc: 100.0 | Testing Loss: 0.0015381141565740108 | Testing Acc: 100.0\n",
            "Epoch: 3533 | Training Loss: 0.0007236723904497921 | Training Acc: 100.0 | Testing Loss: 0.0015373679343611002 | Testing Acc: 100.0\n",
            "Epoch: 3534 | Training Loss: 0.0007233922369778156 | Training Acc: 100.0 | Testing Loss: 0.0015373504720628262 | Testing Acc: 100.0\n",
            "Epoch: 3535 | Training Loss: 0.0007231059717014432 | Training Acc: 100.0 | Testing Loss: 0.0015366104198619723 | Testing Acc: 100.0\n",
            "Epoch: 3536 | Training Loss: 0.000722816854249686 | Training Acc: 100.0 | Testing Loss: 0.0015365580329671502 | Testing Acc: 100.0\n",
            "Epoch: 3537 | Training Loss: 0.0007225378067232668 | Training Acc: 100.0 | Testing Loss: 0.0015358064556494355 | Testing Acc: 100.0\n",
            "Epoch: 3538 | Training Loss: 0.0007222444983199239 | Training Acc: 100.0 | Testing Loss: 0.001535077695734799 | Testing Acc: 100.0\n",
            "Epoch: 3539 | Training Loss: 0.0007219658000394702 | Training Acc: 100.0 | Testing Loss: 0.001535066170617938 | Testing Acc: 100.0\n",
            "Epoch: 3540 | Training Loss: 0.0007216748781502247 | Training Acc: 100.0 | Testing Loss: 0.0015343199484050274 | Testing Acc: 100.0\n",
            "Epoch: 3541 | Training Loss: 0.0007213918725028634 | Training Acc: 100.0 | Testing Loss: 0.0015342909609898925 | Testing Acc: 100.0\n",
            "Epoch: 3542 | Training Loss: 0.0007211068295873702 | Training Acc: 100.0 | Testing Loss: 0.0015335507923737168 | Testing Acc: 100.0\n",
            "Epoch: 3543 | Training Loss: 0.0007208192837424576 | Training Acc: 100.0 | Testing Loss: 0.0015335215721279383 | Testing Acc: 100.0\n",
            "Epoch: 3544 | Training Loss: 0.0007205477450042963 | Training Acc: 100.0 | Testing Loss: 0.0015327522996813059 | Testing Acc: 100.0\n",
            "Epoch: 3545 | Training Loss: 0.0007202529232017696 | Training Acc: 100.0 | Testing Loss: 0.0015320234233513474 | Testing Acc: 100.0\n",
            "Epoch: 3546 | Training Loss: 0.0007199771935120225 | Training Acc: 100.0 | Testing Loss: 0.0015320060774683952 | Testing Acc: 100.0\n",
            "Epoch: 3547 | Training Loss: 0.0007196936057880521 | Training Acc: 100.0 | Testing Loss: 0.0015312657924368978 | Testing Acc: 100.0\n",
            "Epoch: 3548 | Training Loss: 0.0007194031495600939 | Training Acc: 100.0 | Testing Loss: 0.0015312250470742583 | Testing Acc: 100.0\n",
            "Epoch: 3549 | Training Loss: 0.0007191269542090595 | Training Acc: 100.0 | Testing Loss: 0.0015304849948734045 | Testing Acc: 100.0\n",
            "Epoch: 3550 | Training Loss: 0.0007188349845819175 | Training Acc: 100.0 | Testing Loss: 0.0015297677600756288 | Testing Acc: 100.0\n",
            "Epoch: 3551 | Training Loss: 0.0007185666472651064 | Training Acc: 100.0 | Testing Loss: 0.0015297618228942156 | Testing Acc: 100.0\n",
            "Epoch: 3552 | Training Loss: 0.0007182729314081371 | Training Acc: 100.0 | Testing Loss: 0.0015290387673303485 | Testing Acc: 100.0\n",
            "Epoch: 3553 | Training Loss: 0.0007179867243394256 | Training Acc: 100.0 | Testing Loss: 0.0015290157170966268 | Testing Acc: 100.0\n",
            "Epoch: 3554 | Training Loss: 0.0007177076186053455 | Training Acc: 100.0 | Testing Loss: 0.0015282753156498075 | Testing Acc: 100.0\n",
            "Epoch: 3555 | Training Loss: 0.0007174172205850482 | Training Acc: 100.0 | Testing Loss: 0.0015282288659363985 | Testing Acc: 100.0\n",
            "Epoch: 3556 | Training Loss: 0.0007171410252340138 | Training Acc: 100.0 | Testing Loss: 0.0015274884644895792 | Testing Acc: 100.0\n",
            "Epoch: 3557 | Training Loss: 0.0007168549345806241 | Training Acc: 100.0 | Testing Loss: 0.001526765525341034 | Testing Acc: 100.0\n",
            "Epoch: 3558 | Training Loss: 0.0007165821734815836 | Training Acc: 100.0 | Testing Loss: 0.001526736537925899 | Testing Acc: 100.0\n",
            "Epoch: 3559 | Training Loss: 0.0007162942201830447 | Training Acc: 100.0 | Testing Loss: 0.0015260018408298492 | Testing Acc: 100.0\n",
            "Epoch: 3560 | Training Loss: 0.0007160139502957463 | Training Acc: 100.0 | Testing Loss: 0.001525996020063758 | Testing Acc: 100.0\n",
            "Epoch: 3561 | Training Loss: 0.0007157379877753556 | Training Acc: 100.0 | Testing Loss: 0.0015252558514475822 | Testing Acc: 100.0\n",
            "Epoch: 3562 | Training Loss: 0.000715450500138104 | Training Acc: 100.0 | Testing Loss: 0.001525203580968082 | Testing Acc: 100.0\n",
            "Epoch: 3563 | Training Loss: 0.0007151772151701152 | Training Acc: 100.0 | Testing Loss: 0.0015244691167026758 | Testing Acc: 100.0\n",
            "Epoch: 3564 | Training Loss: 0.000714888155926019 | Training Acc: 100.0 | Testing Loss: 0.001523739891126752 | Testing Acc: 100.0\n",
            "Epoch: 3565 | Training Loss: 0.0007146124262362719 | Training Acc: 100.0 | Testing Loss: 0.0015237050829455256 | Testing Acc: 100.0\n",
            "Epoch: 3566 | Training Loss: 0.0007143347756937146 | Training Acc: 100.0 | Testing Loss: 0.00152298784814775 | Testing Acc: 100.0\n",
            "Epoch: 3567 | Training Loss: 0.0007140546222217381 | Training Acc: 100.0 | Testing Loss: 0.0015229471027851105 | Testing Acc: 100.0\n",
            "Epoch: 3568 | Training Loss: 0.0007137724896892905 | Training Acc: 100.0 | Testing Loss: 0.0015222126385197043 | Testing Acc: 100.0\n",
            "Epoch: 3569 | Training Loss: 0.0007134850020520389 | Training Acc: 100.0 | Testing Loss: 0.0015215242747217417 | Testing Acc: 100.0\n",
            "Epoch: 3570 | Training Loss: 0.0007132181781344116 | Training Acc: 100.0 | Testing Loss: 0.0015215069288387895 | Testing Acc: 100.0\n",
            "Epoch: 3571 | Training Loss: 0.000712936045601964 | Training Acc: 100.0 | Testing Loss: 0.0015207778196781874 | Testing Acc: 100.0\n",
            "Epoch: 3572 | Training Loss: 0.0007126588607206941 | Training Acc: 100.0 | Testing Loss: 0.0015207428950816393 | Testing Acc: 100.0\n",
            "Epoch: 3573 | Training Loss: 0.0007123827235773206 | Training Acc: 100.0 | Testing Loss: 0.0015200198395177722 | Testing Acc: 100.0\n",
            "Epoch: 3574 | Training Loss: 0.0007120995433069766 | Training Acc: 100.0 | Testing Loss: 0.00151996745262295 | Testing Acc: 100.0\n",
            "Epoch: 3575 | Training Loss: 0.0007118262583389878 | Training Acc: 100.0 | Testing Loss: 0.001519226934760809 | Testing Acc: 100.0\n",
            "Epoch: 3576 | Training Loss: 0.0007115387124940753 | Training Acc: 100.0 | Testing Loss: 0.0015185094671323895 | Testing Acc: 100.0\n",
            "Epoch: 3577 | Training Loss: 0.000711271830368787 | Training Acc: 100.0 | Testing Loss: 0.001518515171483159 | Testing Acc: 100.0\n",
            "Epoch: 3578 | Training Loss: 0.000710985332261771 | Training Acc: 100.0 | Testing Loss: 0.0015177919995039701 | Testing Acc: 100.0\n",
            "Epoch: 3579 | Training Loss: 0.0007107096025720239 | Training Acc: 100.0 | Testing Loss: 0.0015177573077380657 | Testing Acc: 100.0\n",
            "Epoch: 3580 | Training Loss: 0.0007104318356141448 | Training Acc: 100.0 | Testing Loss: 0.0015170342521741986 | Testing Acc: 100.0\n",
            "Epoch: 3581 | Training Loss: 0.0007101501687429845 | Training Acc: 100.0 | Testing Loss: 0.0015169878024607897 | Testing Acc: 100.0\n",
            "Epoch: 3582 | Training Loss: 0.0007098782807588577 | Training Acc: 100.0 | Testing Loss: 0.0015162474010139704 | Testing Acc: 100.0\n",
            "Epoch: 3583 | Training Loss: 0.0007095952751114964 | Training Acc: 100.0 | Testing Loss: 0.0015155414585024118 | Testing Acc: 100.0\n",
            "Epoch: 3584 | Training Loss: 0.0007093222811818123 | Training Acc: 100.0 | Testing Loss: 0.0015155298169702291 | Testing Acc: 100.0\n",
            "Epoch: 3585 | Training Loss: 0.0007090477738529444 | Training Acc: 100.0 | Testing Loss: 0.0015148005913943052 | Testing Acc: 100.0\n",
            "Epoch: 3586 | Training Loss: 0.0007087647099979222 | Training Acc: 100.0 | Testing Loss: 0.001514765783213079 | Testing Acc: 100.0\n",
            "Epoch: 3587 | Training Loss: 0.0007084973040036857 | Training Acc: 100.0 | Testing Loss: 0.0015140485484153032 | Testing Acc: 100.0\n",
            "Epoch: 3588 | Training Loss: 0.0007082155789248645 | Training Acc: 100.0 | Testing Loss: 0.0015133366687223315 | Testing Acc: 100.0\n",
            "Epoch: 3589 | Training Loss: 0.0007079485803842545 | Training Acc: 100.0 | Testing Loss: 0.0015133193228393793 | Testing Acc: 100.0\n",
            "Epoch: 3590 | Training Loss: 0.0007076665642671287 | Training Acc: 100.0 | Testing Loss: 0.001512607792392373 | Testing Acc: 100.0\n",
            "Epoch: 3591 | Training Loss: 0.0007073981105349958 | Training Acc: 100.0 | Testing Loss: 0.001512572867795825 | Testing Acc: 100.0\n",
            "Epoch: 3592 | Training Loss: 0.0007071189465932548 | Training Acc: 100.0 | Testing Loss: 0.0015118556329980493 | Testing Acc: 100.0\n",
            "Epoch: 3593 | Training Loss: 0.0007068418199196458 | Training Acc: 100.0 | Testing Loss: 0.0015118207084015012 | Testing Acc: 100.0\n",
            "Epoch: 3594 | Training Loss: 0.0007065774989314377 | Training Acc: 100.0 | Testing Loss: 0.001511109177954495 | Testing Acc: 100.0\n",
            "Epoch: 3595 | Training Loss: 0.000706292747054249 | Training Acc: 100.0 | Testing Loss: 0.00151039136108011 | Testing Acc: 100.0\n",
            "Epoch: 3596 | Training Loss: 0.0007060258649289608 | Training Acc: 100.0 | Testing Loss: 0.0015103622572496533 | Testing Acc: 100.0\n",
            "Epoch: 3597 | Training Loss: 0.0007057482143864036 | Training Acc: 100.0 | Testing Loss: 0.0015096623683348298 | Testing Acc: 100.0\n",
            "Epoch: 3598 | Training Loss: 0.0007054752786643803 | Training Acc: 100.0 | Testing Loss: 0.0015096216229721904 | Testing Acc: 100.0\n",
            "Epoch: 3599 | Training Loss: 0.0007051974534988403 | Training Acc: 100.0 | Testing Loss: 0.0015088983345776796 | Testing Acc: 100.0\n",
            "Epoch: 3600 | Training Loss: 0.0007049188134260476 | Training Acc: 100.0 | Testing Loss: 0.0015088521176949143 | Testing Acc: 100.0\n",
            "Epoch: 3601 | Training Loss: 0.0007046571699902415 | Training Acc: 100.0 | Testing Loss: 0.0015081230085343122 | Testing Acc: 100.0\n",
            "Epoch: 3602 | Training Loss: 0.0007043755613267422 | Training Acc: 100.0 | Testing Loss: 0.0015074461698532104 | Testing Acc: 100.0\n",
            "Epoch: 3603 | Training Loss: 0.000704108620993793 | Training Acc: 100.0 | Testing Loss: 0.001507405424490571 | Testing Acc: 100.0\n",
            "Epoch: 3604 | Training Loss: 0.0007038339390419424 | Training Acc: 100.0 | Testing Loss: 0.001506705186329782 | Testing Acc: 100.0\n",
            "Epoch: 3605 | Training Loss: 0.0007035610033199191 | Training Acc: 100.0 | Testing Loss: 0.0015066527994349599 | Testing Acc: 100.0\n",
            "Epoch: 3606 | Training Loss: 0.0007032846915535629 | Training Acc: 100.0 | Testing Loss: 0.0015059293946251273 | Testing Acc: 100.0\n",
            "Epoch: 3607 | Training Loss: 0.0007030030828900635 | Training Acc: 100.0 | Testing Loss: 0.0015052349772304296 | Testing Acc: 100.0\n",
            "Epoch: 3608 | Training Loss: 0.0007027420797385275 | Training Acc: 100.0 | Testing Loss: 0.0015052175149321556 | Testing Acc: 100.0\n",
            "Epoch: 3609 | Training Loss: 0.0007024657097645104 | Training Acc: 100.0 | Testing Loss: 0.0015045175096020103 | Testing Acc: 100.0\n",
            "Epoch: 3610 | Training Loss: 0.0007021959172561765 | Training Acc: 100.0 | Testing Loss: 0.0015045058680698276 | Testing Acc: 100.0\n",
            "Epoch: 3611 | Training Loss: 0.0007019227487035096 | Training Acc: 100.0 | Testing Loss: 0.0015037942212074995 | Testing Acc: 100.0\n",
            "Epoch: 3612 | Training Loss: 0.0007016482995823026 | Training Acc: 100.0 | Testing Loss: 0.0015037537086755037 | Testing Acc: 100.0\n",
            "Epoch: 3613 | Training Loss: 0.0007013838039711118 | Training Acc: 100.0 | Testing Loss: 0.001503024366684258 | Testing Acc: 100.0\n",
            "Epoch: 3614 | Training Loss: 0.0007011051056906581 | Training Acc: 100.0 | Testing Loss: 0.0015023357700556517 | Testing Acc: 100.0\n",
            "Epoch: 3615 | Training Loss: 0.0007008425891399384 | Training Acc: 100.0 | Testing Loss: 0.0015023008454591036 | Testing Acc: 100.0\n",
            "Epoch: 3616 | Training Loss: 0.0007005692459642887 | Training Acc: 100.0 | Testing Loss: 0.0015015947865322232 | Testing Acc: 100.0\n",
            "Epoch: 3617 | Training Loss: 0.0007002993952482939 | Training Acc: 100.0 | Testing Loss: 0.001501548453234136 | Testing Acc: 100.0\n",
            "Epoch: 3618 | Training Loss: 0.0007000273908488452 | Training Acc: 100.0 | Testing Loss: 0.001500848215073347 | Testing Acc: 100.0\n",
            "Epoch: 3619 | Training Loss: 0.0006997486343607306 | Training Acc: 100.0 | Testing Loss: 0.0015008251648396254 | Testing Acc: 100.0\n",
            "Epoch: 3620 | Training Loss: 0.0006994900177232921 | Training Acc: 100.0 | Testing Loss: 0.0015000959392637014 | Testing Acc: 100.0\n",
            "Epoch: 3621 | Training Loss: 0.0006992113776504993 | Training Acc: 100.0 | Testing Loss: 0.0014993955846875906 | Testing Acc: 100.0\n",
            "Epoch: 3622 | Training Loss: 0.0006989488610997796 | Training Acc: 100.0 | Testing Loss: 0.0014993840595707297 | Testing Acc: 100.0\n",
            "Epoch: 3623 | Training Loss: 0.0006986798834986985 | Training Acc: 100.0 | Testing Loss: 0.0014986604219302535 | Testing Acc: 100.0\n",
            "Epoch: 3624 | Training Loss: 0.000698407064191997 | Training Acc: 100.0 | Testing Loss: 0.0014986373716965318 | Testing Acc: 100.0\n",
            "Epoch: 3625 | Training Loss: 0.0006981455953791738 | Training Acc: 100.0 | Testing Loss: 0.00149792549200356 | Testing Acc: 100.0\n",
            "Epoch: 3626 | Training Loss: 0.0006978726014494896 | Training Acc: 100.0 | Testing Loss: 0.0014972250210121274 | Testing Acc: 100.0\n",
            "Epoch: 3627 | Training Loss: 0.000697608629707247 | Training Acc: 100.0 | Testing Loss: 0.001497230725362897 | Testing Acc: 100.0\n",
            "Epoch: 3628 | Training Loss: 0.0006973369745537639 | Training Acc: 100.0 | Testing Loss: 0.001496530487202108 | Testing Acc: 100.0\n",
            "Epoch: 3629 | Training Loss: 0.0006970655522309244 | Training Acc: 100.0 | Testing Loss: 0.0014965073205530643 | Testing Acc: 100.0\n",
            "Epoch: 3630 | Training Loss: 0.0006968009984120727 | Training Acc: 100.0 | Testing Loss: 0.0014957895036786795 | Testing Acc: 100.0\n",
            "Epoch: 3631 | Training Loss: 0.0006965252105146646 | Training Acc: 100.0 | Testing Loss: 0.0014957431703805923 | Testing Acc: 100.0\n",
            "Epoch: 3632 | Training Loss: 0.000696268049068749 | Training Acc: 100.0 | Testing Loss: 0.0014950369950383902 | Testing Acc: 100.0\n",
            "Epoch: 3633 | Training Loss: 0.0006959921447560191 | Training Acc: 100.0 | Testing Loss: 0.001494348281994462 | Testing Acc: 100.0\n",
            "Epoch: 3634 | Training Loss: 0.0006957296282052994 | Training Acc: 100.0 | Testing Loss: 0.0014943191781640053 | Testing Acc: 100.0\n",
            "Epoch: 3635 | Training Loss: 0.0006954607670195401 | Training Acc: 100.0 | Testing Loss: 0.0014936189400032163 | Testing Acc: 100.0\n",
            "Epoch: 3636 | Training Loss: 0.0006951953400857747 | Training Acc: 100.0 | Testing Loss: 0.001493613002821803 | Testing Acc: 100.0\n",
            "Epoch: 3637 | Training Loss: 0.0006949322414584458 | Training Acc: 100.0 | Testing Loss: 0.0014929069438949227 | Testing Acc: 100.0\n",
            "Epoch: 3638 | Training Loss: 0.0006946564535610378 | Training Acc: 100.0 | Testing Loss: 0.0014928544405847788 | Testing Acc: 100.0\n",
            "Epoch: 3639 | Training Loss: 0.0006943977205082774 | Training Acc: 100.0 | Testing Loss: 0.0014921484980732203 | Testing Acc: 100.0\n",
            "Epoch: 3640 | Training Loss: 0.0006941277533769608 | Training Acc: 100.0 | Testing Loss: 0.001491453731432557 | Testing Acc: 100.0\n",
            "Epoch: 3641 | Training Loss: 0.0006938607548363507 | Training Acc: 100.0 | Testing Loss: 0.0014914365019649267 | Testing Acc: 100.0\n",
            "Epoch: 3642 | Training Loss: 0.000693594862241298 | Training Acc: 100.0 | Testing Loss: 0.0014907361473888159 | Testing Acc: 100.0\n",
            "Epoch: 3643 | Training Loss: 0.0006933279801160097 | Training Acc: 100.0 | Testing Loss: 0.0014906952856108546 | Testing Acc: 100.0\n",
            "Epoch: 3644 | Training Loss: 0.0006930691306479275 | Training Acc: 100.0 | Testing Loss: 0.001489989343099296 | Testing Acc: 100.0\n",
            "Epoch: 3645 | Training Loss: 0.0006927993963472545 | Training Acc: 100.0 | Testing Loss: 0.001489323447458446 | Testing Acc: 100.0\n",
            "Epoch: 3646 | Training Loss: 0.0006925368797965348 | Training Acc: 100.0 | Testing Loss: 0.0014893061015754938 | Testing Acc: 100.0\n",
            "Epoch: 3647 | Training Loss: 0.0006922692409716547 | Training Acc: 100.0 | Testing Loss: 0.0014886113349348307 | Testing Acc: 100.0\n",
            "Epoch: 3648 | Training Loss: 0.0006919977604411542 | Training Acc: 100.0 | Testing Loss: 0.0014885704731568694 | Testing Acc: 100.0\n",
            "Epoch: 3649 | Training Loss: 0.00069173623342067 | Training Acc: 100.0 | Testing Loss: 0.001487875822931528 | Testing Acc: 100.0\n",
            "Epoch: 3650 | Training Loss: 0.0006914662080816925 | Training Acc: 100.0 | Testing Loss: 0.0014878293732181191 | Testing Acc: 100.0\n",
            "Epoch: 3651 | Training Loss: 0.0006912135286256671 | Training Acc: 100.0 | Testing Loss: 0.0014871233142912388 | Testing Acc: 100.0\n",
            "Epoch: 3652 | Training Loss: 0.0006909407093189657 | Training Acc: 100.0 | Testing Loss: 0.0014864520635455847 | Testing Acc: 100.0\n",
            "Epoch: 3653 | Training Loss: 0.0006906766211614013 | Training Acc: 100.0 | Testing Loss: 0.0014864460099488497 | Testing Acc: 100.0\n",
            "Epoch: 3654 | Training Loss: 0.0006904075853526592 | Training Acc: 100.0 | Testing Loss: 0.0014857572969049215 | Testing Acc: 100.0\n",
            "Epoch: 3655 | Training Loss: 0.0006901451852172613 | Training Acc: 100.0 | Testing Loss: 0.0014857223723083735 | Testing Acc: 100.0\n",
            "Epoch: 3656 | Training Loss: 0.0006898880237713456 | Training Acc: 100.0 | Testing Loss: 0.0014850159641355276 | Testing Acc: 100.0\n",
            "Epoch: 3657 | Training Loss: 0.0006896209670230746 | Training Acc: 100.0 | Testing Loss: 0.0014849638100713491 | Testing Acc: 100.0\n",
            "Epoch: 3658 | Training Loss: 0.0006893651443533599 | Training Acc: 100.0 | Testing Loss: 0.0014842457603663206 | Testing Acc: 100.0\n",
            "Epoch: 3659 | Training Loss: 0.0006890938384458423 | Training Acc: 100.0 | Testing Loss: 0.0014835800975561142 | Testing Acc: 100.0\n",
            "Epoch: 3660 | Training Loss: 0.0006888298084959388 | Training Acc: 100.0 | Testing Loss: 0.0014835509937256575 | Testing Acc: 100.0\n",
            "Epoch: 3661 | Training Loss: 0.0006885710172355175 | Training Acc: 100.0 | Testing Loss: 0.0014828563435003161 | Testing Acc: 100.0\n",
            "Epoch: 3662 | Training Loss: 0.0006883026217110455 | Training Acc: 100.0 | Testing Loss: 0.0014828447019681334 | Testing Acc: 100.0\n",
            "Epoch: 3663 | Training Loss: 0.0006880455184727907 | Training Acc: 100.0 | Testing Loss: 0.0014821500517427921 | Testing Acc: 100.0\n",
            "Epoch: 3664 | Training Loss: 0.000687776948325336 | Training Acc: 100.0 | Testing Loss: 0.0014814784517511725 | Testing Acc: 100.0\n",
            "Epoch: 3665 | Training Loss: 0.0006875158287584782 | Training Acc: 100.0 | Testing Loss: 0.0014814550522714853 | Testing Acc: 100.0\n",
            "Epoch: 3666 | Training Loss: 0.000687246851157397 | Training Acc: 100.0 | Testing Loss: 0.0014807662228122354 | Testing Acc: 100.0\n",
            "Epoch: 3667 | Training Loss: 0.0006869917269796133 | Training Acc: 100.0 | Testing Loss: 0.0014807312982156873 | Testing Acc: 100.0\n",
            "Epoch: 3668 | Training Loss: 0.0006867328775115311 | Training Acc: 100.0 | Testing Loss: 0.0014800423523411155 | Testing Acc: 100.0\n",
            "Epoch: 3669 | Training Loss: 0.0006864630850031972 | Training Acc: 100.0 | Testing Loss: 0.0014799840282648802 | Testing Acc: 100.0\n",
            "Epoch: 3670 | Training Loss: 0.0006862101727165282 | Training Acc: 100.0 | Testing Loss: 0.0014793011359870434 | Testing Acc: 100.0\n",
            "Epoch: 3671 | Training Loss: 0.0006859430577605963 | Training Acc: 100.0 | Testing Loss: 0.001478646881878376 | Testing Acc: 100.0\n",
            "Epoch: 3672 | Training Loss: 0.000685683568008244 | Training Acc: 100.0 | Testing Loss: 0.001478617894463241 | Testing Acc: 100.0\n",
            "Epoch: 3673 | Training Loss: 0.000685421924572438 | Training Acc: 100.0 | Testing Loss: 0.0014779230114072561 | Testing Acc: 100.0\n",
            "Epoch: 3674 | Training Loss: 0.0006851578946225345 | Training Acc: 100.0 | Testing Loss: 0.0014778937911614776 | Testing Acc: 100.0\n",
            "Epoch: 3675 | Training Loss: 0.000684901955537498 | Training Acc: 100.0 | Testing Loss: 0.0014772049617022276 | Testing Acc: 100.0\n",
            "Epoch: 3676 | Training Loss: 0.0006846381584182382 | Training Acc: 100.0 | Testing Loss: 0.0014765391824766994 | Testing Acc: 100.0\n",
            "Epoch: 3677 | Training Loss: 0.0006843843730166554 | Training Acc: 100.0 | Testing Loss: 0.0014765218365937471 | Testing Acc: 100.0\n",
            "Epoch: 3678 | Training Loss: 0.0006841197609901428 | Training Acc: 100.0 | Testing Loss: 0.0014758502366021276 | Testing Acc: 100.0\n",
            "Epoch: 3679 | Training Loss: 0.0006838630069978535 | Training Acc: 100.0 | Testing Loss: 0.0014758385950699449 | Testing Acc: 100.0\n",
            "Epoch: 3680 | Training Loss: 0.0006836014799773693 | Training Acc: 100.0 | Testing Loss: 0.0014751495327800512 | Testing Acc: 100.0\n",
            "Epoch: 3681 | Training Loss: 0.0006833375082351267 | Training Acc: 100.0 | Testing Loss: 0.0014751089038327336 | Testing Acc: 100.0\n",
            "Epoch: 3682 | Training Loss: 0.0006830859929323196 | Training Acc: 100.0 | Testing Loss: 0.0014744022628292441 | Testing Acc: 100.0\n",
            "Epoch: 3683 | Training Loss: 0.0006828205659985542 | Training Acc: 100.0 | Testing Loss: 0.001473736367188394 | Testing Acc: 100.0\n",
            "Epoch: 3684 | Training Loss: 0.0006825713207945228 | Training Acc: 100.0 | Testing Loss: 0.0014737133169546723 | Testing Acc: 100.0\n",
            "Epoch: 3685 | Training Loss: 0.0006823096191510558 | Training Acc: 100.0 | Testing Loss: 0.0014730533584952354 | Testing Acc: 100.0\n",
            "Epoch: 3686 | Training Loss: 0.0006820543203502893 | Training Acc: 100.0 | Testing Loss: 0.0014730069087818265 | Testing Acc: 100.0\n",
            "Epoch: 3687 | Training Loss: 0.0006817956455051899 | Training Acc: 100.0 | Testing Loss: 0.001472335308790207 | Testing Acc: 100.0\n",
            "Epoch: 3688 | Training Loss: 0.0006815301021561027 | Training Acc: 100.0 | Testing Loss: 0.0014723122585564852 | Testing Acc: 100.0\n",
            "Epoch: 3689 | Training Loss: 0.0006812786450609565 | Training Acc: 100.0 | Testing Loss: 0.0014716173755005002 | Testing Acc: 100.0\n",
            "Epoch: 3690 | Training Loss: 0.0006810177001170814 | Training Acc: 100.0 | Testing Loss: 0.0014709573006257415 | Testing Acc: 100.0\n",
            "Epoch: 3691 | Training Loss: 0.0006807624595239758 | Training Acc: 100.0 | Testing Loss: 0.0014708987437188625 | Testing Acc: 100.0\n",
            "Epoch: 3692 | Training Loss: 0.0006805022130720317 | Training Acc: 100.0 | Testing Loss: 0.0014702270273119211 | Testing Acc: 100.0\n",
            "Epoch: 3693 | Training Loss: 0.0006802455172874033 | Training Acc: 100.0 | Testing Loss: 0.0014701923355460167 | Testing Acc: 100.0\n",
            "Epoch: 3694 | Training Loss: 0.0006799955735914409 | Training Acc: 100.0 | Testing Loss: 0.001469503273256123 | Testing Acc: 100.0\n",
            "Epoch: 3695 | Training Loss: 0.0006797286332584918 | Training Acc: 100.0 | Testing Loss: 0.0014688490191474557 | Testing Acc: 100.0\n",
            "Epoch: 3696 | Training Loss: 0.0006794806104153395 | Training Acc: 100.0 | Testing Loss: 0.001468854839913547 | Testing Acc: 100.0\n",
            "Epoch: 3697 | Training Loss: 0.0006792161730118096 | Training Acc: 100.0 | Testing Loss: 0.0014681888278573751 | Testing Acc: 100.0\n",
            "Epoch: 3698 | Training Loss: 0.0006789624458178878 | Training Acc: 100.0 | Testing Loss: 0.0014681483153253794 | Testing Acc: 100.0\n",
            "Epoch: 3699 | Training Loss: 0.0006787081947550178 | Training Acc: 100.0 | Testing Loss: 0.0014674706617370248 | Testing Acc: 100.0\n",
            "Epoch: 3700 | Training Loss: 0.0006784454453736544 | Training Acc: 100.0 | Testing Loss: 0.0014674299163743854 | Testing Acc: 100.0\n",
            "Epoch: 3701 | Training Loss: 0.0006781970150768757 | Training Acc: 100.0 | Testing Loss: 0.0014667350333184004 | Testing Acc: 100.0\n",
            "Epoch: 3702 | Training Loss: 0.0006779359537176788 | Training Acc: 100.0 | Testing Loss: 0.001466074725612998 | Testing Acc: 100.0\n",
            "Epoch: 3703 | Training Loss: 0.0006776835070922971 | Training Acc: 100.0 | Testing Loss: 0.0014660514425486326 | Testing Acc: 100.0\n",
            "Epoch: 3704 | Training Loss: 0.0006774263456463814 | Training Acc: 100.0 | Testing Loss: 0.0014653739053755999 | Testing Acc: 100.0\n",
            "Epoch: 3705 | Training Loss: 0.0006771726766601205 | Training Acc: 100.0 | Testing Loss: 0.0014653622638434172 | Testing Acc: 100.0\n",
            "Epoch: 3706 | Training Loss: 0.000676921394187957 | Training Acc: 100.0 | Testing Loss: 0.001464696484617889 | Testing Acc: 100.0\n",
            "Epoch: 3707 | Training Loss: 0.0006766585865989327 | Training Acc: 100.0 | Testing Loss: 0.0014646383933722973 | Testing Acc: 100.0\n",
            "Epoch: 3708 | Training Loss: 0.0006764116697013378 | Training Acc: 100.0 | Testing Loss: 0.0014639492146670818 | Testing Acc: 100.0\n",
            "Epoch: 3709 | Training Loss: 0.000676156603731215 | Training Acc: 100.0 | Testing Loss: 0.001463300664909184 | Testing Acc: 100.0\n",
            "Epoch: 3710 | Training Loss: 0.0006759041571058333 | Training Acc: 100.0 | Testing Loss: 0.0014632714446634054 | Testing Acc: 100.0\n",
            "Epoch: 3711 | Training Loss: 0.0006756425136700273 | Training Acc: 100.0 | Testing Loss: 0.0014626055490225554 | Testing Acc: 100.0\n",
            "Epoch: 3712 | Training Loss: 0.0006753872730769217 | Training Acc: 100.0 | Testing Loss: 0.0014625824987888336 | Testing Acc: 100.0\n",
            "Epoch: 3713 | Training Loss: 0.000675144896376878 | Training Acc: 100.0 | Testing Loss: 0.0014619047287851572 | Testing Acc: 100.0\n",
            "Epoch: 3714 | Training Loss: 0.0006748821469955146 | Training Acc: 100.0 | Testing Loss: 0.0014612383674830198 | Testing Acc: 100.0\n",
            "Epoch: 3715 | Training Loss: 0.0006746372091583908 | Training Acc: 100.0 | Testing Loss: 0.001461209380067885 | Testing Acc: 100.0\n",
            "Epoch: 3716 | Training Loss: 0.0006743755075149238 | Training Acc: 100.0 | Testing Loss: 0.0014605550095438957 | Testing Acc: 100.0\n",
            "Epoch: 3717 | Training Loss: 0.0006741276010870934 | Training Acc: 100.0 | Testing Loss: 0.0014605200849473476 | Testing Acc: 100.0\n",
            "Epoch: 3718 | Training Loss: 0.0006738732918165624 | Training Acc: 100.0 | Testing Loss: 0.0014598364941775799 | Testing Acc: 100.0\n",
            "Epoch: 3719 | Training Loss: 0.0006736136274412274 | Training Acc: 100.0 | Testing Loss: 0.0014597957488149405 | Testing Acc: 100.0\n",
            "Epoch: 3720 | Training Loss: 0.0006733695627190173 | Training Acc: 100.0 | Testing Loss: 0.0014591239159926772 | Testing Acc: 100.0\n",
            "Epoch: 3721 | Training Loss: 0.000673112808726728 | Training Acc: 100.0 | Testing Loss: 0.001458487007766962 | Testing Acc: 100.0\n",
            "Epoch: 3722 | Training Loss: 0.0006728606531396508 | Training Acc: 100.0 | Testing Loss: 0.0014584638411179185 | Testing Acc: 100.0\n",
            "Epoch: 3723 | Training Loss: 0.0006726077408529818 | Training Acc: 100.0 | Testing Loss: 0.0014577977126464248 | Testing Acc: 100.0\n",
            "Epoch: 3724 | Training Loss: 0.0006723568076267838 | Training Acc: 100.0 | Testing Loss: 0.0014577630208805203 | Testing Acc: 100.0\n",
            "Epoch: 3725 | Training Loss: 0.0006721083191223443 | Training Acc: 100.0 | Testing Loss: 0.0014570910716429353 | Testing Acc: 100.0\n",
            "Epoch: 3726 | Training Loss: 0.0006718517979606986 | Training Acc: 100.0 | Testing Loss: 0.0014564364682883024 | Testing Acc: 100.0\n",
            "Epoch: 3727 | Training Loss: 0.0006716126808896661 | Training Acc: 100.0 | Testing Loss: 0.0014564248267561197 | Testing Acc: 100.0\n",
            "Epoch: 3728 | Training Loss: 0.0006713523762300611 | Training Acc: 100.0 | Testing Loss: 0.0014557584654539824 | Testing Acc: 100.0\n",
            "Epoch: 3729 | Training Loss: 0.0006711060414090753 | Training Acc: 100.0 | Testing Loss: 0.0014557469403371215 | Testing Acc: 100.0\n",
            "Epoch: 3730 | Training Loss: 0.0006708487635478377 | Training Acc: 100.0 | Testing Loss: 0.0014550809282809496 | Testing Acc: 100.0\n",
            "Epoch: 3731 | Training Loss: 0.0006705920677632093 | Training Acc: 100.0 | Testing Loss: 0.0014550460036844015 | Testing Acc: 100.0\n",
            "Epoch: 3732 | Training Loss: 0.0006703478284180164 | Training Acc: 100.0 | Testing Loss: 0.00145437428727746 | Testing Acc: 100.0\n",
            "Epoch: 3733 | Training Loss: 0.0006700912490487099 | Training Acc: 100.0 | Testing Loss: 0.001453719800338149 | Testing Acc: 100.0\n",
            "Epoch: 3734 | Training Loss: 0.000669846311211586 | Training Acc: 100.0 | Testing Loss: 0.0014536965172737837 | Testing Acc: 100.0\n",
            "Epoch: 3735 | Training Loss: 0.0006695888005197048 | Training Acc: 100.0 | Testing Loss: 0.0014530301559716463 | Testing Acc: 100.0\n",
            "Epoch: 3736 | Training Loss: 0.0006693365867249668 | Training Acc: 100.0 | Testing Loss: 0.0014529775362461805 | Testing Acc: 100.0\n",
            "Epoch: 3737 | Training Loss: 0.0006690880982205272 | Training Acc: 100.0 | Testing Loss: 0.0014523173449561 | Testing Acc: 100.0\n",
            "Epoch: 3738 | Training Loss: 0.0006688358844257891 | Training Acc: 100.0 | Testing Loss: 0.001452288357540965 | Testing Acc: 100.0\n",
            "Epoch: 3739 | Training Loss: 0.000668599153868854 | Training Acc: 100.0 | Testing Loss: 0.0014516104711219668 | Testing Acc: 100.0\n",
            "Epoch: 3740 | Training Loss: 0.0006683409446850419 | Training Acc: 100.0 | Testing Loss: 0.0014509677421301603 | Testing Acc: 100.0\n",
            "Epoch: 3741 | Training Loss: 0.0006680930964648724 | Training Acc: 100.0 | Testing Loss: 0.0014509502798318863 | Testing Acc: 100.0\n",
            "Epoch: 3742 | Training Loss: 0.0006678460049442947 | Training Acc: 100.0 | Testing Loss: 0.0014502841513603926 | Testing Acc: 100.0\n",
            "Epoch: 3743 | Training Loss: 0.0006675907643511891 | Training Acc: 100.0 | Testing Loss: 0.0014502493431791663 | Testing Acc: 100.0\n",
            "Epoch: 3744 | Training Loss: 0.0006673511234112084 | Training Acc: 100.0 | Testing Loss: 0.001449582981877029 | Testing Acc: 100.0\n",
            "Epoch: 3745 | Training Loss: 0.0006670973962172866 | Training Acc: 100.0 | Testing Loss: 0.0014489458408206701 | Testing Acc: 100.0\n",
            "Epoch: 3746 | Training Loss: 0.00066685228375718 | Training Acc: 100.0 | Testing Loss: 0.0014489458408206701 | Testing Acc: 100.0\n",
            "Epoch: 3747 | Training Loss: 0.0006665994878858328 | Training Acc: 100.0 | Testing Loss: 0.001448279363103211 | Testing Acc: 100.0\n",
            "Epoch: 3748 | Training Loss: 0.000666353153064847 | Training Acc: 100.0 | Testing Loss: 0.0014482561964541674 | Testing Acc: 100.0\n",
            "Epoch: 3749 | Training Loss: 0.0006661074585281312 | Training Acc: 100.0 | Testing Loss: 0.0014476017095148563 | Testing Acc: 100.0\n",
            "Epoch: 3750 | Training Loss: 0.0006658522179350257 | Training Acc: 100.0 | Testing Loss: 0.001447549439035356 | Testing Acc: 100.0\n",
            "Epoch: 3751 | Training Loss: 0.0006656096084043384 | Training Acc: 100.0 | Testing Loss: 0.0014468834269791842 | Testing Acc: 100.0\n",
            "Epoch: 3752 | Training Loss: 0.0006653528544120491 | Training Acc: 100.0 | Testing Loss: 0.0014462398830801249 | Testing Acc: 100.0\n",
            "Epoch: 3753 | Training Loss: 0.0006651136791333556 | Training Acc: 100.0 | Testing Loss: 0.0014462167164310813 | Testing Acc: 100.0\n",
            "Epoch: 3754 | Training Loss: 0.0006648652488365769 | Training Acc: 100.0 | Testing Loss: 0.0014455680502578616 | Testing Acc: 100.0\n",
            "Epoch: 3755 | Training Loss: 0.0006646129186265171 | Training Acc: 100.0 | Testing Loss: 0.0014455389464274049 | Testing Acc: 100.0\n",
            "Epoch: 3756 | Training Loss: 0.0006643749075010419 | Training Acc: 100.0 | Testing Loss: 0.0014448902802541852 | Testing Acc: 100.0\n",
            "Epoch: 3757 | Training Loss: 0.000664122577290982 | Training Acc: 100.0 | Testing Loss: 0.0014442529063671827 | Testing Acc: 100.0\n",
            "Epoch: 3758 | Training Loss: 0.0006638790364377201 | Training Acc: 100.0 | Testing Loss: 0.001444223802536726 | Testing Acc: 100.0\n",
            "Epoch: 3759 | Training Loss: 0.000663626124151051 | Training Acc: 100.0 | Testing Loss: 0.0014435806078836322 | Testing Acc: 100.0\n",
            "Epoch: 3760 | Training Loss: 0.0006633856100961566 | Training Acc: 100.0 | Testing Loss: 0.001443545683287084 | Testing Acc: 100.0\n",
            "Epoch: 3761 | Training Loss: 0.0006631385185755789 | Training Acc: 100.0 | Testing Loss: 0.00144288525916636 | Testing Acc: 100.0\n",
            "Epoch: 3762 | Training Loss: 0.0006628876435570419 | Training Acc: 100.0 | Testing Loss: 0.0014428386930376291 | Testing Acc: 100.0\n",
            "Epoch: 3763 | Training Loss: 0.0006626479444094002 | Training Acc: 100.0 | Testing Loss: 0.0014421897940337658 | Testing Acc: 100.0\n",
            "Epoch: 3764 | Training Loss: 0.0006623971275985241 | Training Acc: 100.0 | Testing Loss: 0.0014415699988603592 | Testing Acc: 100.0\n",
            "Epoch: 3765 | Training Loss: 0.0006621596403419971 | Training Acc: 100.0 | Testing Loss: 0.0014415410114452243 | Testing Acc: 100.0\n",
            "Epoch: 3766 | Training Loss: 0.0006619111518375576 | Training Acc: 100.0 | Testing Loss: 0.0014408979332074523 | Testing Acc: 100.0\n",
            "Epoch: 3767 | Training Loss: 0.0006616645841859281 | Training Acc: 100.0 | Testing Loss: 0.0014408513670787215 | Testing Acc: 100.0\n",
            "Epoch: 3768 | Training Loss: 0.0006614234880544245 | Training Acc: 100.0 | Testing Loss: 0.0014402084052562714 | Testing Acc: 100.0\n",
            "Epoch: 3769 | Training Loss: 0.0006611711578443646 | Training Acc: 100.0 | Testing Loss: 0.0014401619555428624 | Testing Acc: 100.0\n",
            "Epoch: 3770 | Training Loss: 0.0006609343690797687 | Training Acc: 100.0 | Testing Loss: 0.0014394896570593119 | Testing Acc: 100.0\n",
            "Epoch: 3771 | Training Loss: 0.000660681922454387 | Training Acc: 100.0 | Testing Loss: 0.0014388462295755744 | Testing Acc: 100.0\n",
            "Epoch: 3772 | Training Loss: 0.0006604474037885666 | Training Acc: 100.0 | Testing Loss: 0.0014388405252248049 | Testing Acc: 100.0\n",
            "Epoch: 3773 | Training Loss: 0.0006602004868909717 | Training Acc: 100.0 | Testing Loss: 0.0014382032677531242 | Testing Acc: 100.0\n",
            "Epoch: 3774 | Training Loss: 0.0006599524058401585 | Training Acc: 100.0 | Testing Loss: 0.0014381685759872198 | Testing Acc: 100.0\n",
            "Epoch: 3775 | Training Loss: 0.0006597171886824071 | Training Acc: 100.0 | Testing Loss: 0.0014375137398019433 | Testing Acc: 100.0\n",
            "Epoch: 3776 | Training Loss: 0.0006594544393010437 | Training Acc: 100.0 | Testing Loss: 0.001436882303096354 | Testing Acc: 100.0\n",
            "Epoch: 3777 | Training Loss: 0.0006592169520445168 | Training Acc: 100.0 | Testing Loss: 0.001436858787201345 | Testing Acc: 100.0\n",
            "Epoch: 3778 | Training Loss: 0.000658972654491663 | Training Acc: 100.0 | Testing Loss: 0.0014362216461449862 | Testing Acc: 100.0\n",
            "Epoch: 3779 | Training Loss: 0.0006587277166545391 | Training Acc: 100.0 | Testing Loss: 0.0014361926587298512 | Testing Acc: 100.0\n",
            "Epoch: 3780 | Training Loss: 0.0006584866205230355 | Training Acc: 100.0 | Testing Loss: 0.001435537589713931 | Testing Acc: 100.0\n",
            "Epoch: 3781 | Training Loss: 0.0006582430796697736 | Training Acc: 100.0 | Testing Loss: 0.0014355144230648875 | Testing Acc: 100.0\n",
            "Epoch: 3782 | Training Loss: 0.0006580063491128385 | Training Acc: 100.0 | Testing Loss: 0.0014348538825288415 | Testing Acc: 100.0\n",
            "Epoch: 3783 | Training Loss: 0.000657758500892669 | Training Acc: 100.0 | Testing Loss: 0.001434216508641839 | Testing Acc: 100.0\n",
            "Epoch: 3784 | Training Loss: 0.0006575179868377745 | Training Acc: 100.0 | Testing Loss: 0.0014341989299282432 | Testing Acc: 100.0\n",
            "Epoch: 3785 | Training Loss: 0.0006572752026841044 | Training Acc: 100.0 | Testing Loss: 0.0014335557352751493 | Testing Acc: 100.0\n",
            "Epoch: 3786 | Training Loss: 0.0006570302648469806 | Training Acc: 100.0 | Testing Loss: 0.001433520927093923 | Testing Acc: 100.0\n",
            "Epoch: 3787 | Training Loss: 0.0006567920790985227 | Training Acc: 100.0 | Testing Loss: 0.001432871911674738 | Testing Acc: 100.0\n",
            "Epoch: 3788 | Training Loss: 0.0006565485382452607 | Training Acc: 100.0 | Testing Loss: 0.001432825461961329 | Testing Acc: 100.0\n",
            "Epoch: 3789 | Training Loss: 0.0006563072092831135 | Training Acc: 100.0 | Testing Loss: 0.0014321648050099611 | Testing Acc: 100.0\n",
            "Epoch: 3790 | Training Loss: 0.0006560608744621277 | Training Acc: 100.0 | Testing Loss: 0.0014315564185380936 | Testing Acc: 100.0\n",
            "Epoch: 3791 | Training Loss: 0.0006558249006047845 | Training Acc: 100.0 | Testing Loss: 0.0014315331354737282 | Testing Acc: 100.0\n",
            "Epoch: 3792 | Training Loss: 0.000655585085041821 | Training Acc: 100.0 | Testing Loss: 0.0014308900572359562 | Testing Acc: 100.0\n",
            "Epoch: 3793 | Training Loss: 0.0006553401472046971 | Training Acc: 100.0 | Testing Loss: 0.00143085524905473 | Testing Acc: 100.0\n",
            "Epoch: 3794 | Training Loss: 0.0006551018450409174 | Training Acc: 100.0 | Testing Loss: 0.0014302001800388098 | Testing Acc: 100.0\n",
            "Epoch: 3795 | Training Loss: 0.0006548584206029773 | Training Acc: 100.0 | Testing Loss: 0.0014295860892161727 | Testing Acc: 100.0\n",
            "Epoch: 3796 | Training Loss: 0.0006546206423081458 | Training Acc: 100.0 | Testing Loss: 0.0014295626897364855 | Testing Acc: 100.0\n",
            "Epoch: 3797 | Training Loss: 0.0006543720373883843 | Training Acc: 100.0 | Testing Loss: 0.0014289073878899217 | Testing Acc: 100.0\n",
            "Epoch: 3798 | Training Loss: 0.0006541389739140868 | Training Acc: 100.0 | Testing Loss: 0.0014289015671238303 | Testing Acc: 100.0\n",
            "Epoch: 3799 | Training Loss: 0.0006539007881656289 | Training Acc: 100.0 | Testing Loss: 0.0014282464981079102 | Testing Acc: 100.0\n",
            "Epoch: 3800 | Training Loss: 0.0006536573055200279 | Training Acc: 100.0 | Testing Loss: 0.0014282118063420057 | Testing Acc: 100.0\n",
            "Epoch: 3801 | Training Loss: 0.0006534234853461385 | Training Acc: 100.0 | Testing Loss: 0.001427557086572051 | Testing Acc: 100.0\n",
            "Epoch: 3802 | Training Loss: 0.0006531724939122796 | Training Acc: 100.0 | Testing Loss: 0.0014269368257373571 | Testing Acc: 100.0\n",
            "Epoch: 3803 | Training Loss: 0.0006529422244057059 | Training Acc: 100.0 | Testing Loss: 0.001426907954737544 | Testing Acc: 100.0\n",
            "Epoch: 3804 | Training Loss: 0.0006526935612782836 | Training Acc: 100.0 | Testing Loss: 0.0014262821059674025 | Testing Acc: 100.0\n",
            "Epoch: 3805 | Training Loss: 0.0006524501950480044 | Training Acc: 100.0 | Testing Loss: 0.0014262645272538066 | Testing Acc: 100.0\n",
            "Epoch: 3806 | Training Loss: 0.0006522178882732987 | Training Acc: 100.0 | Testing Loss: 0.001425621216185391 | Testing Acc: 100.0\n",
            "Epoch: 3807 | Training Loss: 0.0006519699236378074 | Training Acc: 100.0 | Testing Loss: 0.0014249831438064575 | Testing Acc: 100.0\n",
            "Epoch: 3808 | Training Loss: 0.0006517366273328662 | Training Acc: 100.0 | Testing Loss: 0.0014249479863792658 | Testing Acc: 100.0\n",
            "Epoch: 3809 | Training Loss: 0.000651493959594518 | Training Acc: 100.0 | Testing Loss: 0.0014243280747905374 | Testing Acc: 100.0\n",
            "Epoch: 3810 | Training Loss: 0.0006512547261081636 | Training Acc: 100.0 | Testing Loss: 0.001424293266609311 | Testing Acc: 100.0\n",
            "Epoch: 3811 | Training Loss: 0.0006510179955512285 | Training Acc: 100.0 | Testing Loss: 0.0014236499555408955 | Testing Acc: 100.0\n",
            "Epoch: 3812 | Training Loss: 0.0006507731159217656 | Training Acc: 100.0 | Testing Loss: 0.0014236093265935779 | Testing Acc: 100.0\n",
            "Epoch: 3813 | Training Loss: 0.0006505466299131513 | Training Acc: 100.0 | Testing Loss: 0.0014229717198759317 | Testing Acc: 100.0\n",
            "Epoch: 3814 | Training Loss: 0.0006503074546344578 | Training Acc: 100.0 | Testing Loss: 0.0014223749749362469 | Testing Acc: 100.0\n",
            "Epoch: 3815 | Training Loss: 0.0006500669987872243 | Training Acc: 100.0 | Testing Loss: 0.0014223518082872033 | Testing Acc: 100.0\n",
            "Epoch: 3816 | Training Loss: 0.0006498302100226283 | Training Acc: 100.0 | Testing Loss: 0.001421725726686418 | Testing Acc: 100.0\n",
            "Epoch: 3817 | Training Loss: 0.0006495924317277968 | Training Acc: 100.0 | Testing Loss: 0.0014216910349205136 | Testing Acc: 100.0\n",
            "Epoch: 3818 | Training Loss: 0.000649354187771678 | Training Acc: 100.0 | Testing Loss: 0.0014210357330739498 | Testing Acc: 100.0\n",
            "Epoch: 3819 | Training Loss: 0.0006491152453236282 | Training Acc: 100.0 | Testing Loss: 0.0014204269973561168 | Testing Acc: 100.0\n",
            "Epoch: 3820 | Training Loss: 0.0006488849758170545 | Training Acc: 100.0 | Testing Loss: 0.0014204096514731646 | Testing Acc: 100.0\n",
            "Epoch: 3821 | Training Loss: 0.0006486421916633844 | Training Acc: 100.0 | Testing Loss: 0.0014197836862877011 | Testing Acc: 100.0\n",
            "Epoch: 3822 | Training Loss: 0.0006484090117737651 | Training Acc: 100.0 | Testing Loss: 0.001419766223989427 | Testing Acc: 100.0\n",
            "Epoch: 3823 | Training Loss: 0.0006481693126261234 | Training Acc: 100.0 | Testing Loss: 0.0014191461959853768 | Testing Acc: 100.0\n",
            "Epoch: 3824 | Training Loss: 0.0006479331059381366 | Training Acc: 100.0 | Testing Loss: 0.001419105683453381 | Testing Acc: 100.0\n",
            "Epoch: 3825 | Training Loss: 0.0006477021379396319 | Training Acc: 100.0 | Testing Loss: 0.0014184622559696436 | Testing Acc: 100.0\n",
            "Epoch: 3826 | Training Loss: 0.0006474587135016918 | Training Acc: 100.0 | Testing Loss: 0.0014178479323163629 | Testing Acc: 100.0\n",
            "Epoch: 3827 | Training Loss: 0.0006472239620052278 | Training Acc: 100.0 | Testing Loss: 0.0014178127748891711 | Testing Acc: 100.0\n",
            "Epoch: 3828 | Training Loss: 0.0006469869986176491 | Training Acc: 100.0 | Testing Loss: 0.0014171868097037077 | Testing Acc: 100.0\n",
            "Epoch: 3829 | Training Loss: 0.000646750966552645 | Training Acc: 100.0 | Testing Loss: 0.0014171578222885728 | Testing Acc: 100.0\n",
            "Epoch: 3830 | Training Loss: 0.000646521570160985 | Training Acc: 100.0 | Testing Loss: 0.0014165083412081003 | Testing Acc: 100.0\n",
            "Epoch: 3831 | Training Loss: 0.000646278087515384 | Training Acc: 100.0 | Testing Loss: 0.001416490995325148 | Testing Acc: 100.0\n",
            "Epoch: 3832 | Training Loss: 0.0006460543954744935 | Training Acc: 100.0 | Testing Loss: 0.0014158535050228238 | Testing Acc: 100.0\n",
            "Epoch: 3833 | Training Loss: 0.0006458124262280762 | Training Acc: 100.0 | Testing Loss: 0.0014152273070067167 | Testing Acc: 100.0\n",
            "Epoch: 3834 | Training Loss: 0.0006455807015299797 | Training Acc: 100.0 | Testing Loss: 0.0014152098447084427 | Testing Acc: 100.0\n",
            "Epoch: 3835 | Training Loss: 0.000645343738142401 | Training Acc: 100.0 | Testing Loss: 0.0014145897002890706 | Testing Acc: 100.0\n",
            "Epoch: 3836 | Training Loss: 0.0006451077060773969 | Training Acc: 100.0 | Testing Loss: 0.0014145489549264312 | Testing Acc: 100.0\n",
            "Epoch: 3837 | Training Loss: 0.0006448767380788922 | Training Acc: 100.0 | Testing Loss: 0.001413911348208785 | Testing Acc: 100.0\n",
            "Epoch: 3838 | Training Loss: 0.0006446361658163369 | Training Acc: 100.0 | Testing Loss: 0.0014133023796603084 | Testing Acc: 100.0\n",
            "Epoch: 3839 | Training Loss: 0.0006444058381021023 | Training Acc: 100.0 | Testing Loss: 0.0014132966753095388 | Testing Acc: 100.0\n",
            "Epoch: 3840 | Training Loss: 0.0006441677105613053 | Training Acc: 100.0 | Testing Loss: 0.0014126822352409363 | Testing Acc: 100.0\n",
            "Epoch: 3841 | Training Loss: 0.0006439330172725022 | Training Acc: 100.0 | Testing Loss: 0.0014126475434750319 | Testing Acc: 100.0\n",
            "Epoch: 3842 | Training Loss: 0.0006437064148485661 | Training Acc: 100.0 | Testing Loss: 0.001412021229043603 | Testing Acc: 100.0\n",
            "Epoch: 3843 | Training Loss: 0.0006434613605961204 | Training Acc: 100.0 | Testing Loss: 0.0014119807165116072 | Testing Acc: 100.0\n",
            "Epoch: 3844 | Training Loss: 0.0006432391819544137 | Training Acc: 100.0 | Testing Loss: 0.0014113428769633174 | Testing Acc: 100.0\n",
            "Epoch: 3845 | Training Loss: 0.0006429943023249507 | Training Acc: 100.0 | Testing Loss: 0.0014107341412454844 | Testing Acc: 100.0\n",
            "Epoch: 3846 | Training Loss: 0.0006427697953768075 | Training Acc: 100.0 | Testing Loss: 0.0014107050374150276 | Testing Acc: 100.0\n",
            "Epoch: 3847 | Training Loss: 0.0006425314932130277 | Training Acc: 100.0 | Testing Loss: 0.0014100846601650119 | Testing Acc: 100.0\n",
            "Epoch: 3848 | Training Loss: 0.0006422983715310693 | Training Acc: 100.0 | Testing Loss: 0.0014100730186328292 | Testing Acc: 100.0\n",
            "Epoch: 3849 | Training Loss: 0.000642071885522455 | Training Acc: 100.0 | Testing Loss: 0.0014094526413828135 | Testing Acc: 100.0\n",
            "Epoch: 3850 | Training Loss: 0.000641835737042129 | Training Acc: 100.0 | Testing Loss: 0.001408838201314211 | Testing Acc: 100.0\n",
            "Epoch: 3851 | Training Loss: 0.0006416068645194173 | Training Acc: 100.0 | Testing Loss: 0.0014088090974837542 | Testing Acc: 100.0\n",
            "Epoch: 3852 | Training Loss: 0.0006413700757548213 | Training Acc: 100.0 | Testing Loss: 0.0014082061825320125 | Testing Acc: 100.0\n",
            "Epoch: 3853 | Training Loss: 0.0006411426584236324 | Training Acc: 100.0 | Testing Loss: 0.0014081712579354644 | Testing Acc: 100.0\n",
            "Epoch: 3854 | Training Loss: 0.0006409117486327887 | Training Acc: 100.0 | Testing Loss: 0.0014075450599193573 | Testing Acc: 100.0\n",
            "Epoch: 3855 | Training Loss: 0.0006406725733540952 | Training Acc: 100.0 | Testing Loss: 0.0014075045473873615 | Testing Acc: 100.0\n",
            "Epoch: 3856 | Training Loss: 0.0006404520245268941 | Training Acc: 100.0 | Testing Loss: 0.00140686659142375 | Testing Acc: 100.0\n",
            "Epoch: 3857 | Training Loss: 0.0006402128492482007 | Training Acc: 100.0 | Testing Loss: 0.0014062810223549604 | Testing Acc: 100.0\n",
            "Epoch: 3858 | Training Loss: 0.0006399870617315173 | Training Acc: 100.0 | Testing Loss: 0.001406257739290595 | Testing Acc: 100.0\n",
            "Epoch: 3859 | Training Loss: 0.0006397546967491508 | Training Acc: 100.0 | Testing Loss: 0.0014056373620405793 | Testing Acc: 100.0\n",
            "Epoch: 3860 | Training Loss: 0.0006395229138433933 | Training Acc: 100.0 | Testing Loss: 0.0014056024374440312 | Testing Acc: 100.0\n",
            "Epoch: 3861 | Training Loss: 0.0006392918294295669 | Training Acc: 100.0 | Testing Loss: 0.0014049821766093373 | Testing Acc: 100.0\n",
            "Epoch: 3862 | Training Loss: 0.0006390587077476084 | Training Acc: 100.0 | Testing Loss: 0.0014043791452422738 | Testing Acc: 100.0\n",
            "Epoch: 3863 | Training Loss: 0.0006388313486240804 | Training Acc: 100.0 | Testing Loss: 0.001404367620125413 | Testing Acc: 100.0\n",
            "Epoch: 3864 | Training Loss: 0.000638598867226392 | Training Acc: 100.0 | Testing Loss: 0.001403758767992258 | Testing Acc: 100.0\n",
            "Epoch: 3865 | Training Loss: 0.0006383700529113412 | Training Acc: 100.0 | Testing Loss: 0.001403741305693984 | Testing Acc: 100.0\n",
            "Epoch: 3866 | Training Loss: 0.0006381436251103878 | Training Acc: 100.0 | Testing Loss: 0.0014031149912625551 | Testing Acc: 100.0\n",
            "Epoch: 3867 | Training Loss: 0.0006379059632308781 | Training Acc: 100.0 | Testing Loss: 0.0014030860038474202 | Testing Acc: 100.0\n",
            "Epoch: 3868 | Training Loss: 0.0006376807577908039 | Training Acc: 100.0 | Testing Loss: 0.0014024598058313131 | Testing Acc: 100.0\n",
            "Epoch: 3869 | Training Loss: 0.0006374461227096617 | Training Acc: 100.0 | Testing Loss: 0.0014018567744642496 | Testing Acc: 100.0\n",
            "Epoch: 3870 | Training Loss: 0.0006372173083946109 | Training Acc: 100.0 | Testing Loss: 0.001401833607815206 | Testing Acc: 100.0\n",
            "Epoch: 3871 | Training Loss: 0.0006369847687892616 | Training Acc: 100.0 | Testing Loss: 0.001401218818500638 | Testing Acc: 100.0\n",
            "Epoch: 3872 | Training Loss: 0.0006367529858835042 | Training Acc: 100.0 | Testing Loss: 0.0014011840103194118 | Testing Acc: 100.0\n",
            "Epoch: 3873 | Training Loss: 0.0006365278968587518 | Training Acc: 100.0 | Testing Loss: 0.001400563633069396 | Testing Acc: 100.0\n",
            "Epoch: 3874 | Training Loss: 0.0006362977437674999 | Training Acc: 100.0 | Testing Loss: 0.001400546170771122 | Testing Acc: 100.0\n",
            "Epoch: 3875 | Training Loss: 0.0006360725383274257 | Training Acc: 100.0 | Testing Loss: 0.0013999140355736017 | Testing Acc: 100.0\n",
            "Epoch: 3876 | Training Loss: 0.0006358394166454673 | Training Acc: 100.0 | Testing Loss: 0.0013993110042065382 | Testing Acc: 100.0\n",
            "Epoch: 3877 | Training Loss: 0.000635613570921123 | Training Acc: 100.0 | Testing Loss: 0.0013992821332067251 | Testing Acc: 100.0\n",
            "Epoch: 3878 | Training Loss: 0.0006353839999064803 | Training Acc: 100.0 | Testing Loss: 0.0013986732810735703 | Testing Acc: 100.0\n",
            "Epoch: 3879 | Training Loss: 0.0006351581541821361 | Training Acc: 100.0 | Testing Loss: 0.0013986325357109308 | Testing Acc: 100.0\n",
            "Epoch: 3880 | Training Loss: 0.0006349271279759705 | Training Acc: 100.0 | Testing Loss: 0.0013980178628116846 | Testing Acc: 100.0\n",
            "Epoch: 3881 | Training Loss: 0.0006346924346871674 | Training Acc: 100.0 | Testing Loss: 0.0013974322937428951 | Testing Acc: 100.0\n",
            "Epoch: 3882 | Training Loss: 0.0006344782887026668 | Training Acc: 100.0 | Testing Loss: 0.0013974145986139774 | Testing Acc: 100.0\n",
            "Epoch: 3883 | Training Loss: 0.0006342444685287774 | Training Acc: 100.0 | Testing Loss: 0.0013967996928840876 | Testing Acc: 100.0\n",
            "Epoch: 3884 | Training Loss: 0.0006340172258205712 | Training Acc: 100.0 | Testing Loss: 0.0013967647682875395 | Testing Acc: 100.0\n",
            "Epoch: 3885 | Training Loss: 0.0006337935919873416 | Training Acc: 100.0 | Testing Loss: 0.0013961498625576496 | Testing Acc: 100.0\n",
            "Epoch: 3886 | Training Loss: 0.0006335543585009873 | Training Acc: 100.0 | Testing Loss: 0.0013961093500256538 | Testing Acc: 100.0\n",
            "Epoch: 3887 | Training Loss: 0.0006333306082524359 | Training Acc: 100.0 | Testing Loss: 0.0013954947935417295 | Testing Acc: 100.0\n",
            "Epoch: 3888 | Training Loss: 0.0006330959731712937 | Training Acc: 100.0 | Testing Loss: 0.001394891645759344 | Testing Acc: 100.0\n",
            "Epoch: 3889 | Training Loss: 0.0006328804302029312 | Training Acc: 100.0 | Testing Loss: 0.001394891645759344 | Testing Acc: 100.0\n",
            "Epoch: 3890 | Training Loss: 0.0006326509756036103 | Training Acc: 100.0 | Testing Loss: 0.0013942886143922806 | Testing Acc: 100.0\n",
            "Epoch: 3891 | Training Loss: 0.0006324207060970366 | Training Acc: 100.0 | Testing Loss: 0.0013942478690296412 | Testing Acc: 100.0\n",
            "Epoch: 3892 | Training Loss: 0.0006321999826468527 | Training Acc: 100.0 | Testing Loss: 0.0013936447212472558 | Testing Acc: 100.0\n",
            "Epoch: 3893 | Training Loss: 0.0006319666281342506 | Training Acc: 100.0 | Testing Loss: 0.0013930473942309618 | Testing Acc: 100.0\n",
            "Epoch: 3894 | Training Loss: 0.0006317481165751815 | Training Acc: 100.0 | Testing Loss: 0.0013930239947512746 | Testing Acc: 100.0\n",
            "Epoch: 3895 | Training Loss: 0.000631515693385154 | Training Acc: 100.0 | Testing Loss: 0.0013924150262027979 | Testing Acc: 100.0\n",
            "Epoch: 3896 | Training Loss: 0.0006312928744591773 | Training Acc: 100.0 | Testing Loss: 0.0013923918595537543 | Testing Acc: 100.0\n",
            "Epoch: 3897 | Training Loss: 0.0006310675526037812 | Training Acc: 100.0 | Testing Loss: 0.0013917828910052776 | Testing Acc: 100.0\n",
            "Epoch: 3898 | Training Loss: 0.0006308432784862816 | Training Acc: 100.0 | Testing Loss: 0.0013917539035901427 | Testing Acc: 100.0\n",
            "Epoch: 3899 | Training Loss: 0.0006306195864453912 | Training Acc: 100.0 | Testing Loss: 0.0013911508722230792 | Testing Acc: 100.0\n",
            "Epoch: 3900 | Training Loss: 0.0006303879199549556 | Training Acc: 100.0 | Testing Loss: 0.001390565070323646 | Testing Acc: 100.0\n",
            "Epoch: 3901 | Training Loss: 0.0006301678949967027 | Training Acc: 100.0 | Testing Loss: 0.0013905360829085112 | Testing Acc: 100.0\n",
            "Epoch: 3902 | Training Loss: 0.0006299413507804275 | Training Acc: 100.0 | Testing Loss: 0.001389938872307539 | Testing Acc: 100.0\n",
            "Epoch: 3903 | Training Loss: 0.0006297184154391289 | Training Acc: 100.0 | Testing Loss: 0.0013898981269448996 | Testing Acc: 100.0\n",
            "Epoch: 3904 | Training Loss: 0.0006294901249930263 | Training Acc: 100.0 | Testing Loss: 0.001389289041981101 | Testing Acc: 100.0\n",
            "Epoch: 3905 | Training Loss: 0.0006292643956840038 | Training Acc: 100.0 | Testing Loss: 0.0013886915985494852 | Testing Acc: 100.0\n",
            "Epoch: 3906 | Training Loss: 0.0006290444289334118 | Training Acc: 100.0 | Testing Loss: 0.0013886855449527502 | Testing Acc: 100.0\n",
            "Epoch: 3907 | Training Loss: 0.0006288164295256138 | Training Acc: 100.0 | Testing Loss: 0.001388099859468639 | Testing Acc: 100.0\n",
            "Epoch: 3908 | Training Loss: 0.0006285904673859477 | Training Acc: 100.0 | Testing Loss: 0.0013880651677027345 | Testing Acc: 100.0\n",
            "Epoch: 3909 | Training Loss: 0.00062836694996804 | Training Acc: 100.0 | Testing Loss: 0.001387456082738936 | Testing Acc: 100.0\n",
            "Epoch: 3910 | Training Loss: 0.0006281380774453282 | Training Acc: 100.0 | Testing Loss: 0.0013874153373762965 | Testing Acc: 100.0\n",
            "Epoch: 3911 | Training Loss: 0.000627920322585851 | Training Acc: 100.0 | Testing Loss: 0.0013868121895939112 | Testing Acc: 100.0\n",
            "Epoch: 3912 | Training Loss: 0.000627692905254662 | Training Acc: 100.0 | Testing Loss: 0.0013862205669283867 | Testing Acc: 100.0\n",
            "Epoch: 3913 | Training Loss: 0.0006274728802964091 | Training Acc: 100.0 | Testing Loss: 0.0013861974002793431 | Testing Acc: 100.0\n",
            "Epoch: 3914 | Training Loss: 0.0006272448226809502 | Training Acc: 100.0 | Testing Loss: 0.0013856058940291405 | Testing Acc: 100.0\n",
            "Epoch: 3915 | Training Loss: 0.000627026311121881 | Training Acc: 100.0 | Testing Loss: 0.0013855881989002228 | Testing Acc: 100.0\n",
            "Epoch: 3916 | Training Loss: 0.0006268086144700646 | Training Acc: 100.0 | Testing Loss: 0.001384973293170333 | Testing Acc: 100.0\n",
            "Epoch: 3917 | Training Loss: 0.0006265753763727844 | Training Acc: 100.0 | Testing Loss: 0.0013843874912708998 | Testing Acc: 100.0\n",
            "Epoch: 3918 | Training Loss: 0.0006263553514145315 | Training Acc: 100.0 | Testing Loss: 0.001384369912557304 | Testing Acc: 100.0\n",
            "Epoch: 3919 | Training Loss: 0.0006261286907829344 | Training Acc: 100.0 | Testing Loss: 0.0013837784063071012 | Testing Acc: 100.0\n",
            "Epoch: 3920 | Training Loss: 0.0006259101210162044 | Training Acc: 100.0 | Testing Loss: 0.0013837494188919663 | Testing Acc: 100.0\n",
            "Epoch: 3921 | Training Loss: 0.000625685031991452 | Training Acc: 100.0 | Testing Loss: 0.0013831461546942592 | Testing Acc: 100.0\n",
            "Epoch: 3922 | Training Loss: 0.0006254591280594468 | Training Acc: 100.0 | Testing Loss: 0.0013831171672791243 | Testing Acc: 100.0\n",
            "Epoch: 3923 | Training Loss: 0.0006252456805668771 | Training Acc: 100.0 | Testing Loss: 0.0013825080823153257 | Testing Acc: 100.0\n",
            "Epoch: 3924 | Training Loss: 0.0006250169244594872 | Training Acc: 100.0 | Testing Loss: 0.0013819279847666621 | Testing Acc: 100.0\n",
            "Epoch: 3925 | Training Loss: 0.0006248014396987855 | Training Acc: 100.0 | Testing Loss: 0.001381904585286975 | Testing Acc: 100.0\n",
            "Epoch: 3926 | Training Loss: 0.0006245822296477854 | Training Acc: 100.0 | Testing Loss: 0.001381324720568955 | Testing Acc: 100.0\n",
            "Epoch: 3927 | Training Loss: 0.0006243547541089356 | Training Acc: 100.0 | Testing Loss: 0.0013812838587909937 | Testing Acc: 100.0\n",
            "Epoch: 3928 | Training Loss: 0.0006241354858502746 | Training Acc: 100.0 | Testing Loss: 0.0013806805945932865 | Testing Acc: 100.0\n",
            "Epoch: 3929 | Training Loss: 0.0006239126669242978 | Training Acc: 100.0 | Testing Loss: 0.001380639965645969 | Testing Acc: 100.0\n",
            "Epoch: 3930 | Training Loss: 0.0006236947374418378 | Training Acc: 100.0 | Testing Loss: 0.0013800249435007572 | Testing Acc: 100.0\n",
            "Epoch: 3931 | Training Loss: 0.0006234688917174935 | Training Acc: 100.0 | Testing Loss: 0.0013794449623674154 | Testing Acc: 100.0\n",
            "Epoch: 3932 | Training Loss: 0.0006232488667592406 | Training Acc: 100.0 | Testing Loss: 0.0013794333208352327 | Testing Acc: 100.0\n",
            "Epoch: 3933 | Training Loss: 0.0006230267463251948 | Training Acc: 100.0 | Testing Loss: 0.0013788415817543864 | Testing Acc: 100.0\n",
            "Epoch: 3934 | Training Loss: 0.0006228036945685744 | Training Acc: 100.0 | Testing Loss: 0.001378806889988482 | Testing Acc: 100.0\n",
            "Epoch: 3935 | Training Loss: 0.0006225874531082809 | Training Acc: 100.0 | Testing Loss: 0.001378203509375453 | Testing Acc: 100.0\n",
            "Epoch: 3936 | Training Loss: 0.0006223615491762757 | Training Acc: 100.0 | Testing Loss: 0.0013776232954114676 | Testing Acc: 100.0\n",
            "Epoch: 3937 | Training Loss: 0.0006221458315849304 | Training Acc: 100.0 | Testing Loss: 0.0013775883708149195 | Testing Acc: 100.0\n",
            "Epoch: 3938 | Training Loss: 0.0006219192873686552 | Training Acc: 100.0 | Testing Loss: 0.001377014210447669 | Testing Acc: 100.0\n",
            "Epoch: 3939 | Training Loss: 0.0006217023474164307 | Training Acc: 100.0 | Testing Loss: 0.001376979285851121 | Testing Acc: 100.0\n",
            "Epoch: 3940 | Training Loss: 0.0006214859895408154 | Training Acc: 100.0 | Testing Loss: 0.0013763874303549528 | Testing Acc: 100.0\n",
            "Epoch: 3941 | Training Loss: 0.000621261540800333 | Training Acc: 100.0 | Testing Loss: 0.0013763642637059093 | Testing Acc: 100.0\n",
            "Epoch: 3942 | Training Loss: 0.0006210466963239014 | Training Acc: 100.0 | Testing Loss: 0.001375755062326789 | Testing Acc: 100.0\n",
            "Epoch: 3943 | Training Loss: 0.0006208238191902637 | Training Acc: 100.0 | Testing Loss: 0.0013751807855442166 | Testing Acc: 100.0\n",
            "Epoch: 3944 | Training Loss: 0.0006206113030202687 | Training Acc: 100.0 | Testing Loss: 0.001375157618895173 | Testing Acc: 100.0\n",
            "Epoch: 3945 | Training Loss: 0.0006203859229572117 | Training Acc: 100.0 | Testing Loss: 0.0013745598262175918 | Testing Acc: 100.0\n",
            "Epoch: 3946 | Training Loss: 0.0006201689247973263 | Training Acc: 100.0 | Testing Loss: 0.0013745250180363655 | Testing Acc: 100.0\n",
            "Epoch: 3947 | Training Loss: 0.0006199555937200785 | Training Acc: 100.0 | Testing Loss: 0.0013739392161369324 | Testing Acc: 100.0\n",
            "Epoch: 3948 | Training Loss: 0.000619725149590522 | Training Acc: 100.0 | Testing Loss: 0.0013733648229390383 | Testing Acc: 100.0\n",
            "Epoch: 3949 | Training Loss: 0.0006195110036060214 | Training Acc: 100.0 | Testing Loss: 0.001373359002172947 | Testing Acc: 100.0\n",
            "Epoch: 3950 | Training Loss: 0.0006192873697727919 | Training Acc: 100.0 | Testing Loss: 0.0013727729674428701 | Testing Acc: 100.0\n",
            "Epoch: 3951 | Training Loss: 0.000619073398411274 | Training Acc: 100.0 | Testing Loss: 0.001372744096443057 | Testing Acc: 100.0\n",
            "Epoch: 3952 | Training Loss: 0.000618855410721153 | Training Acc: 100.0 | Testing Loss: 0.001372146187350154 | Testing Acc: 100.0\n",
            "Epoch: 3953 | Training Loss: 0.0006186281098052859 | Training Acc: 100.0 | Testing Loss: 0.0013721056748181581 | Testing Acc: 100.0\n",
            "Epoch: 3954 | Training Loss: 0.0006184191443026066 | Training Acc: 100.0 | Testing Loss: 0.0013715082313865423 | Testing Acc: 100.0\n",
            "Epoch: 3955 | Training Loss: 0.000618196208961308 | Training Acc: 100.0 | Testing Loss: 0.0013709276681765914 | Testing Acc: 100.0\n",
            "Epoch: 3956 | Training Loss: 0.0006179864285513759 | Training Acc: 100.0 | Testing Loss: 0.0013708986807614565 | Testing Acc: 100.0\n",
            "Epoch: 3957 | Training Loss: 0.000617765705101192 | Training Acc: 100.0 | Testing Loss: 0.001370318466797471 | Testing Acc: 100.0\n",
            "Epoch: 3958 | Training Loss: 0.0006175473099574447 | Training Acc: 100.0 | Testing Loss: 0.001370301004499197 | Testing Acc: 100.0\n",
            "Epoch: 3959 | Training Loss: 0.0006173353758640587 | Training Acc: 100.0 | Testing Loss: 0.0013697149697691202 | Testing Acc: 100.0\n",
            "Epoch: 3960 | Training Loss: 0.0006171094137243927 | Training Acc: 100.0 | Testing Loss: 0.0013691405765712261 | Testing Acc: 100.0\n",
            "Epoch: 3961 | Training Loss: 0.0006169042317196727 | Training Acc: 100.0 | Testing Loss: 0.0013691114727407694 | Testing Acc: 100.0\n",
            "Epoch: 3962 | Training Loss: 0.0006166834500618279 | Training Acc: 100.0 | Testing Loss: 0.0013685430167242885 | Testing Acc: 100.0\n",
            "Epoch: 3963 | Training Loss: 0.000616464763879776 | Training Acc: 100.0 | Testing Loss: 0.0013685140293091536 | Testing Acc: 100.0\n",
            "Epoch: 3964 | Training Loss: 0.0006162484060041606 | Training Acc: 100.0 | Testing Loss: 0.001367910299450159 | Testing Acc: 100.0\n",
            "Epoch: 3965 | Training Loss: 0.000616025528870523 | Training Acc: 100.0 | Testing Loss: 0.0013678695540875196 | Testing Acc: 100.0\n",
            "Epoch: 3966 | Training Loss: 0.0006158164469525218 | Training Acc: 100.0 | Testing Loss: 0.0013672776985913515 | Testing Acc: 100.0\n",
            "Epoch: 3967 | Training Loss: 0.0006155935116112232 | Training Acc: 100.0 | Testing Loss: 0.0013667206512764096 | Testing Acc: 100.0\n",
            "Epoch: 3968 | Training Loss: 0.0006153854192234576 | Training Acc: 100.0 | Testing Loss: 0.001366691547445953 | Testing Acc: 100.0\n",
            "Epoch: 3969 | Training Loss: 0.0006151660927571356 | Training Acc: 100.0 | Testing Loss: 0.0013661171542480588 | Testing Acc: 100.0\n",
            "Epoch: 3970 | Training Loss: 0.000614951946772635 | Training Acc: 100.0 | Testing Loss: 0.0013660762924700975 | Testing Acc: 100.0\n",
            "Epoch: 3971 | Training Loss: 0.0006147370440885425 | Training Acc: 100.0 | Testing Loss: 0.0013654904905706644 | Testing Acc: 100.0\n",
            "Epoch: 3972 | Training Loss: 0.0006145141087472439 | Training Acc: 100.0 | Testing Loss: 0.0013649218017235398 | Testing Acc: 100.0\n",
            "Epoch: 3973 | Training Loss: 0.0006143074133433402 | Training Acc: 100.0 | Testing Loss: 0.001364904223009944 | Testing Acc: 100.0\n",
            "Epoch: 3974 | Training Loss: 0.0006140894256532192 | Training Acc: 100.0 | Testing Loss: 0.0013643295969814062 | Testing Acc: 100.0\n",
            "Epoch: 3975 | Training Loss: 0.0006138753960840404 | Training Acc: 100.0 | Testing Loss: 0.001364312251098454 | Testing Acc: 100.0\n",
            "Epoch: 3976 | Training Loss: 0.0006136561278253794 | Training Acc: 100.0 | Testing Loss: 0.0013637201627716422 | Testing Acc: 100.0\n",
            "Epoch: 3977 | Training Loss: 0.0006134406430646777 | Training Acc: 100.0 | Testing Loss: 0.0013636794174090028 | Testing Acc: 100.0\n",
            "Epoch: 3978 | Training Loss: 0.0006132315611466765 | Training Acc: 100.0 | Testing Loss: 0.0013630876783281565 | Testing Acc: 100.0\n",
            "Epoch: 3979 | Training Loss: 0.000613008625805378 | Training Acc: 100.0 | Testing Loss: 0.001362518989481032 | Testing Acc: 100.0\n",
            "Epoch: 3980 | Training Loss: 0.0006127989618107677 | Training Acc: 100.0 | Testing Loss: 0.0013624958228319883 | Testing Acc: 100.0\n",
            "Epoch: 3981 | Training Loss: 0.000612585514318198 | Training Acc: 100.0 | Testing Loss: 0.0013619272504001856 | Testing Acc: 100.0\n",
            "Epoch: 3982 | Training Loss: 0.0006123713101260364 | Training Acc: 100.0 | Testing Loss: 0.0013619038509204984 | Testing Acc: 100.0\n",
            "Epoch: 3983 | Training Loss: 0.0006121565820649266 | Training Acc: 100.0 | Testing Loss: 0.0013613118790090084 | Testing Acc: 100.0\n",
            "Epoch: 3984 | Training Loss: 0.0006119365571066737 | Training Acc: 100.0 | Testing Loss: 0.0013607607688754797 | Testing Acc: 100.0\n",
            "Epoch: 3985 | Training Loss: 0.000611725146882236 | Training Acc: 100.0 | Testing Loss: 0.0013607317814603448 | Testing Acc: 100.0\n",
            "Epoch: 3986 | Training Loss: 0.0006115088472142816 | Training Acc: 100.0 | Testing Loss: 0.0013601629761978984 | Testing Acc: 100.0\n",
            "Epoch: 3987 | Training Loss: 0.0006112962146289647 | Training Acc: 100.0 | Testing Loss: 0.0013601338723674417 | Testing Acc: 100.0\n",
            "Epoch: 3988 | Training Loss: 0.0006110828253440559 | Training Acc: 100.0 | Testing Loss: 0.0013595534255728126 | Testing Acc: 100.0\n",
            "Epoch: 3989 | Training Loss: 0.0006108671659603715 | Training Acc: 100.0 | Testing Loss: 0.0013595068594440818 | Testing Acc: 100.0\n",
            "Epoch: 3990 | Training Loss: 0.0006106595974415541 | Training Acc: 100.0 | Testing Loss: 0.0013589266454800963 | Testing Acc: 100.0\n",
            "Epoch: 3991 | Training Loss: 0.0006104426574893296 | Training Acc: 100.0 | Testing Loss: 0.0013583812396973372 | Testing Acc: 100.0\n",
            "Epoch: 3992 | Training Loss: 0.000610234506893903 | Training Acc: 100.0 | Testing Loss: 0.0013583461986854672 | Testing Acc: 100.0\n",
            "Epoch: 3993 | Training Loss: 0.0006100194295868278 | Training Acc: 100.0 | Testing Loss: 0.0013577776262536645 | Testing Acc: 100.0\n",
            "Epoch: 3994 | Training Loss: 0.0006098068552091718 | Training Acc: 100.0 | Testing Loss: 0.0013577485224232078 | Testing Acc: 100.0\n",
            "Epoch: 3995 | Training Loss: 0.0006095963180996478 | Training Acc: 100.0 | Testing Loss: 0.0013571680756285787 | Testing Acc: 100.0\n",
            "Epoch: 3996 | Training Loss: 0.0006093777483329177 | Training Acc: 100.0 | Testing Loss: 0.001357127446681261 | Testing Acc: 100.0\n",
            "Epoch: 3997 | Training Loss: 0.0006091746618039906 | Training Acc: 100.0 | Testing Loss: 0.0013565472327172756 | Testing Acc: 100.0\n",
            "Epoch: 3998 | Training Loss: 0.0006089516682550311 | Training Acc: 100.0 | Testing Loss: 0.0013559666695073247 | Testing Acc: 100.0\n",
            "Epoch: 3999 | Training Loss: 0.0006087449728511274 | Training Acc: 100.0 | Testing Loss: 0.0013559608487412333 | Testing Acc: 100.0\n",
            "Epoch: 4000 | Training Loss: 0.0006085359491407871 | Training Acc: 100.0 | Testing Loss: 0.0013553862227126956 | Testing Acc: 100.0\n",
            "Epoch: 4001 | Training Loss: 0.0006083159823901951 | Training Acc: 100.0 | Testing Loss: 0.0013553514145314693 | Testing Acc: 100.0\n",
            "Epoch: 4002 | Training Loss: 0.0006081099272705615 | Training Acc: 100.0 | Testing Loss: 0.0013547652633860707 | Testing Acc: 100.0\n",
            "Epoch: 4003 | Training Loss: 0.0006078899023123085 | Training Acc: 100.0 | Testing Loss: 0.0013542140368372202 | Testing Acc: 100.0\n",
            "Epoch: 4004 | Training Loss: 0.0006076873978599906 | Training Acc: 100.0 | Testing Loss: 0.0013541965745389462 | Testing Acc: 100.0\n",
            "Epoch: 4005 | Training Loss: 0.0006074680713936687 | Training Acc: 100.0 | Testing Loss: 0.0013536219485104084 | Testing Acc: 100.0\n",
            "Epoch: 4006 | Training Loss: 0.0006072599207982421 | Training Acc: 100.0 | Testing Loss: 0.001353587256744504 | Testing Acc: 100.0\n",
            "Epoch: 4007 | Training Loss: 0.0006070508388802409 | Training Acc: 100.0 | Testing Loss: 0.001353018218651414 | Testing Acc: 100.0\n",
            "Epoch: 4008 | Training Loss: 0.0006068322109058499 | Training Acc: 100.0 | Testing Loss: 0.0013529949355870485 | Testing Acc: 100.0\n",
            "Epoch: 4009 | Training Loss: 0.0006066292407922447 | Training Acc: 100.0 | Testing Loss: 0.0013523970264941454 | Testing Acc: 100.0\n",
            "Epoch: 4010 | Training Loss: 0.0006064166082069278 | Training Acc: 100.0 | Testing Loss: 0.001351845683529973 | Testing Acc: 100.0\n",
            "Epoch: 4011 | Training Loss: 0.000606211309786886 | Training Acc: 100.0 | Testing Loss: 0.0013518165796995163 | Testing Acc: 100.0\n",
            "Epoch: 4012 | Training Loss: 0.000606000772677362 | Training Acc: 100.0 | Testing Loss: 0.001351253711618483 | Testing Acc: 100.0\n",
            "Epoch: 4013 | Training Loss: 0.0006057851715013385 | Training Acc: 100.0 | Testing Loss: 0.0013512128498405218 | Testing Acc: 100.0\n",
            "Epoch: 4014 | Training Loss: 0.000605577661190182 | Training Acc: 100.0 | Testing Loss: 0.001350638223811984 | Testing Acc: 100.0\n",
            "Epoch: 4015 | Training Loss: 0.000605366425588727 | Training Acc: 100.0 | Testing Loss: 0.0013500928180292249 | Testing Acc: 100.0\n",
            "Epoch: 4016 | Training Loss: 0.0006051611271686852 | Training Acc: 100.0 | Testing Loss: 0.0013500811764970422 | Testing Acc: 100.0\n",
            "Epoch: 4017 | Training Loss: 0.0006049477960914373 | Training Acc: 100.0 | Testing Loss: 0.0013495180755853653 | Testing Acc: 100.0\n",
            "Epoch: 4018 | Training Loss: 0.0006047395872883499 | Training Acc: 100.0 | Testing Loss: 0.0013494949089363217 | Testing Acc: 100.0\n",
            "Epoch: 4019 | Training Loss: 0.0006045274203643203 | Training Acc: 100.0 | Testing Loss: 0.0013489143457263708 | Testing Acc: 100.0\n",
            "Epoch: 4020 | Training Loss: 0.0006043148459866643 | Training Acc: 100.0 | Testing Loss: 0.001348873833194375 | Testing Acc: 100.0\n",
            "Epoch: 4021 | Training Loss: 0.0006041101878508925 | Training Acc: 100.0 | Testing Loss: 0.0013482989743351936 | Testing Acc: 100.0\n",
            "Epoch: 4022 | Training Loss: 0.000603895983658731 | Training Acc: 100.0 | Testing Loss: 0.001347741694189608 | Testing Acc: 100.0\n",
            "Epoch: 4023 | Training Loss: 0.0006036892300471663 | Training Acc: 100.0 | Testing Loss: 0.0013477185275405645 | Testing Acc: 100.0\n",
            "Epoch: 4024 | Training Loss: 0.0006034726975485682 | Training Acc: 100.0 | Testing Loss: 0.0013471496058627963 | Testing Acc: 100.0\n",
            "Epoch: 4025 | Training Loss: 0.0006032675737515092 | Training Acc: 100.0 | Testing Loss: 0.0013471379643306136 | Testing Acc: 100.0\n",
            "Epoch: 4026 | Training Loss: 0.0006030613440088928 | Training Acc: 100.0 | Testing Loss: 0.0013465691590681672 | Testing Acc: 100.0\n",
            "Epoch: 4027 | Training Loss: 0.0006028518546372652 | Training Acc: 100.0 | Testing Loss: 0.0013460118789225817 | Testing Acc: 100.0\n",
            "Epoch: 4028 | Training Loss: 0.0006026480114087462 | Training Acc: 100.0 | Testing Loss: 0.0013459944166243076 | Testing Acc: 100.0\n",
            "Epoch: 4029 | Training Loss: 0.0006024359609000385 | Training Acc: 100.0 | Testing Loss: 0.0013454430736601353 | Testing Acc: 100.0\n",
            "Epoch: 4030 | Training Loss: 0.0006022261222824454 | Training Acc: 100.0 | Testing Loss: 0.001345402211882174 | Testing Acc: 100.0\n",
            "Epoch: 4031 | Training Loss: 0.0006020216387696564 | Training Acc: 100.0 | Testing Loss: 0.001344839227385819 | Testing Acc: 100.0\n",
            "Epoch: 4032 | Training Loss: 0.000601805979385972 | Training Acc: 100.0 | Testing Loss: 0.0013448044192045927 | Testing Acc: 100.0\n",
            "Epoch: 4033 | Training Loss: 0.0006016058032400906 | Training Acc: 100.0 | Testing Loss: 0.0013442296767607331 | Testing Acc: 100.0\n",
            "Epoch: 4034 | Training Loss: 0.0006013900274410844 | Training Acc: 100.0 | Testing Loss: 0.0013437015004456043 | Testing Acc: 100.0\n",
            "Epoch: 4035 | Training Loss: 0.0006011833902448416 | Training Acc: 100.0 | Testing Loss: 0.0013436725130304694 | Testing Acc: 100.0\n",
            "Epoch: 4036 | Training Loss: 0.0006009743083268404 | Training Acc: 100.0 | Testing Loss: 0.0013431091792881489 | Testing Acc: 100.0\n",
            "Epoch: 4037 | Training Loss: 0.0006007674382999539 | Training Acc: 100.0 | Testing Loss: 0.001343080191873014 | Testing Acc: 100.0\n",
            "Epoch: 4038 | Training Loss: 0.0006005628383718431 | Training Acc: 100.0 | Testing Loss: 0.0013425115030258894 | Testing Acc: 100.0\n",
            "Epoch: 4039 | Training Loss: 0.0006003531743772328 | Training Acc: 100.0 | Testing Loss: 0.0013419539900496602 | Testing Acc: 100.0\n",
            "Epoch: 4040 | Training Loss: 0.0006001523579470813 | Training Acc: 100.0 | Testing Loss: 0.0013419423485174775 | Testing Acc: 100.0\n",
            "Epoch: 4041 | Training Loss: 0.0005999446730129421 | Training Acc: 100.0 | Testing Loss: 0.0013413791311904788 | Testing Acc: 100.0\n",
            "Epoch: 4042 | Training Loss: 0.0005997364642098546 | Training Acc: 100.0 | Testing Loss: 0.0013413557317107916 | Testing Acc: 100.0\n",
            "Epoch: 4043 | Training Loss: 0.0005995304672978818 | Training Acc: 100.0 | Testing Loss: 0.0013407927472144365 | Testing Acc: 100.0\n",
            "Epoch: 4044 | Training Loss: 0.0005993207450956106 | Training Acc: 100.0 | Testing Loss: 0.0013407578226178885 | Testing Acc: 100.0\n",
            "Epoch: 4045 | Training Loss: 0.0005991203943267465 | Training Acc: 100.0 | Testing Loss: 0.0013401948381215334 | Testing Acc: 100.0\n",
            "Epoch: 4046 | Training Loss: 0.0005989092169329524 | Training Acc: 100.0 | Testing Loss: 0.0013396432623267174 | Testing Acc: 100.0\n",
            "Epoch: 4047 | Training Loss: 0.0005987008917145431 | Training Acc: 100.0 | Testing Loss: 0.0013396200956776738 | Testing Acc: 100.0\n",
            "Epoch: 4048 | Training Loss: 0.0005984978633932769 | Training Acc: 100.0 | Testing Loss: 0.0013390686362981796 | Testing Acc: 100.0\n",
            "Epoch: 4049 | Training Loss: 0.0005982925067655742 | Training Acc: 100.0 | Testing Loss: 0.0013390396488830447 | Testing Acc: 100.0\n",
            "Epoch: 4050 | Training Loss: 0.0005980878486298025 | Training Acc: 100.0 | Testing Loss: 0.001338452915661037 | Testing Acc: 100.0\n",
            "Epoch: 4051 | Training Loss: 0.0005978781846351922 | Training Acc: 100.0 | Testing Loss: 0.0013379361480474472 | Testing Acc: 100.0\n",
            "Epoch: 4052 | Training Loss: 0.0005976787069812417 | Training Acc: 100.0 | Testing Loss: 0.001337895286269486 | Testing Acc: 100.0\n",
            "Epoch: 4053 | Training Loss: 0.0005974682280793786 | Training Acc: 100.0 | Testing Loss: 0.001337343594059348 | Testing Acc: 100.0\n",
            "Epoch: 4054 | Training Loss: 0.000597261474467814 | Training Acc: 100.0 | Testing Loss: 0.0013373087858781219 | Testing Acc: 100.0\n",
            "Epoch: 4055 | Training Loss: 0.000597058329731226 | Training Acc: 100.0 | Testing Loss: 0.0013367632636800408 | Testing Acc: 100.0\n",
            "Epoch: 4056 | Training Loss: 0.0005968514597043395 | Training Acc: 100.0 | Testing Loss: 0.0013367165811359882 | Testing Acc: 100.0\n",
            "Epoch: 4057 | Training Loss: 0.0005966497701592743 | Training Acc: 100.0 | Testing Loss: 0.0013361594174057245 | Testing Acc: 100.0\n",
            "Epoch: 4058 | Training Loss: 0.0005964415613561869 | Training Acc: 100.0 | Testing Loss: 0.0013356136623769999 | Testing Acc: 100.0\n",
            "Epoch: 4059 | Training Loss: 0.0005962421419098973 | Training Acc: 100.0 | Testing Loss: 0.0013356020208448172 | Testing Acc: 100.0\n",
            "Epoch: 4060 | Training Loss: 0.0005960374837741256 | Training Acc: 100.0 | Testing Loss: 0.0013350562658160925 | Testing Acc: 100.0\n",
            "Epoch: 4061 | Training Loss: 0.0005958307301625609 | Training Acc: 100.0 | Testing Loss: 0.0013350214576348662 | Testing Acc: 100.0\n",
            "Epoch: 4062 | Training Loss: 0.0005956290406174958 | Training Acc: 100.0 | Testing Loss: 0.0013344583567231894 | Testing Acc: 100.0\n",
            "Epoch: 4063 | Training Loss: 0.000595416291616857 | Training Acc: 100.0 | Testing Loss: 0.0013339183060452342 | Testing Acc: 100.0\n",
            "Epoch: 4064 | Training Loss: 0.0005952227511443198 | Training Acc: 100.0 | Testing Loss: 0.001333894906565547 | Testing Acc: 100.0\n",
            "Epoch: 4065 | Training Loss: 0.0005950122722424567 | Training Acc: 100.0 | Testing Loss: 0.0013333375100046396 | Testing Acc: 100.0\n",
            "Epoch: 4066 | Training Loss: 0.0005948099424131215 | Training Acc: 100.0 | Testing Loss: 0.0013333199312910438 | Testing Acc: 100.0\n",
            "Epoch: 4067 | Training Loss: 0.0005946112214587629 | Training Acc: 100.0 | Testing Loss: 0.001332774292677641 | Testing Acc: 100.0\n",
            "Epoch: 4068 | Training Loss: 0.0005944029544480145 | Training Acc: 100.0 | Testing Loss: 0.0013327335473150015 | Testing Acc: 100.0\n",
            "Epoch: 4069 | Training Loss: 0.0005942041170783341 | Training Acc: 100.0 | Testing Loss: 0.0013321703299880028 | Testing Acc: 100.0\n",
            "Epoch: 4070 | Training Loss: 0.0005939974216744304 | Training Acc: 100.0 | Testing Loss: 0.0013316363329067826 | Testing Acc: 100.0\n",
            "Epoch: 4071 | Training Loss: 0.0005937993410043418 | Training Acc: 100.0 | Testing Loss: 0.001331613166257739 | Testing Acc: 100.0\n",
            "Epoch: 4072 | Training Loss: 0.0005935902590863407 | Training Acc: 100.0 | Testing Loss: 0.0013310556532815099 | Testing Acc: 100.0\n",
            "Epoch: 4073 | Training Loss: 0.0005933895008638501 | Training Acc: 100.0 | Testing Loss: 0.0013310208451002836 | Testing Acc: 100.0\n",
            "Epoch: 4074 | Training Loss: 0.0005931906634941697 | Training Acc: 100.0 | Testing Loss: 0.0013304632157087326 | Testing Acc: 100.0\n",
            "Epoch: 4075 | Training Loss: 0.0005929793696850538 | Training Acc: 100.0 | Testing Loss: 0.0013299465645104647 | Testing Acc: 100.0\n",
            "Epoch: 4076 | Training Loss: 0.0005927860038354993 | Training Acc: 100.0 | Testing Loss: 0.0013299115234985948 | Testing Acc: 100.0\n",
            "Epoch: 4077 | Training Loss: 0.0005925769219174981 | Training Acc: 100.0 | Testing Loss: 0.001329359831288457 | Testing Acc: 100.0\n",
            "Epoch: 4078 | Training Loss: 0.0005923759890720248 | Training Acc: 100.0 | Testing Loss: 0.0013293306110426784 | Testing Acc: 100.0\n",
            "Epoch: 4079 | Training Loss: 0.0005921741831116378 | Training Acc: 100.0 | Testing Loss: 0.0013287730980664492 | Testing Acc: 100.0\n",
            "Epoch: 4080 | Training Loss: 0.0005919659743085504 | Training Acc: 100.0 | Testing Loss: 0.001328732236288488 | Testing Acc: 100.0\n",
            "Epoch: 4081 | Training Loss: 0.00059176713693887 | Training Acc: 100.0 | Testing Loss: 0.0013281807769089937 | Testing Acc: 100.0\n",
            "Epoch: 4082 | Training Loss: 0.0005915617803111672 | Training Acc: 100.0 | Testing Loss: 0.0013276406098157167 | Testing Acc: 100.0\n",
            "Epoch: 4083 | Training Loss: 0.0005913652712479234 | Training Acc: 100.0 | Testing Loss: 0.001327628968283534 | Testing Acc: 100.0\n",
            "Epoch: 4084 | Training Loss: 0.0005911577609367669 | Training Acc: 100.0 | Testing Loss: 0.0013270833296701312 | Testing Acc: 100.0\n",
            "Epoch: 4085 | Training Loss: 0.0005909524625167251 | Training Acc: 100.0 | Testing Loss: 0.0013270543422549963 | Testing Acc: 100.0\n",
            "Epoch: 4086 | Training Loss: 0.0005907580489292741 | Training Acc: 100.0 | Testing Loss: 0.001326490892097354 | Testing Acc: 100.0\n",
            "Epoch: 4087 | Training Loss: 0.0005905528087168932 | Training Acc: 100.0 | Testing Loss: 0.001325956778600812 | Testing Acc: 100.0\n",
            "Epoch: 4088 | Training Loss: 0.0005903593264520168 | Training Acc: 100.0 | Testing Loss: 0.001325939316302538 | Testing Acc: 100.0\n",
            "Epoch: 4089 | Training Loss: 0.0005901516997255385 | Training Acc: 100.0 | Testing Loss: 0.0013253873912617564 | Testing Acc: 100.0\n",
            "Epoch: 4090 | Training Loss: 0.0005899566458538175 | Training Acc: 100.0 | Testing Loss: 0.0013253584038466215 | Testing Acc: 100.0\n",
            "Epoch: 4091 | Training Loss: 0.0005897548981010914 | Training Acc: 100.0 | Testing Loss: 0.0013248184695839882 | Testing Acc: 100.0\n",
            "Epoch: 4092 | Training Loss: 0.0005895525682717562 | Training Acc: 100.0 | Testing Loss: 0.0013247892493382096 | Testing Acc: 100.0\n",
            "Epoch: 4093 | Training Loss: 0.0005893583293072879 | Training Acc: 100.0 | Testing Loss: 0.0013242257991805673 | Testing Acc: 100.0\n",
            "Epoch: 4094 | Training Loss: 0.0005891514592804015 | Training Acc: 100.0 | Testing Loss: 0.0013236916856840253 | Testing Acc: 100.0\n",
            "Epoch: 4095 | Training Loss: 0.0005889549502171576 | Training Acc: 100.0 | Testing Loss: 0.0013236741069704294 | Testing Acc: 100.0\n",
            "Epoch: 4096 | Training Loss: 0.0005887517472729087 | Training Acc: 100.0 | Testing Loss: 0.0013231164775788784 | Testing Acc: 100.0\n",
            "Epoch: 4097 | Training Loss: 0.0005885538412258029 | Training Acc: 100.0 | Testing Loss: 0.0013230874901637435 | Testing Acc: 100.0\n",
            "Epoch: 4098 | Training Loss: 0.0005883534322492778 | Training Acc: 100.0 | Testing Loss: 0.001322541618719697 | Testing Acc: 100.0\n",
            "Epoch: 4099 | Training Loss: 0.0005881481920368969 | Training Acc: 100.0 | Testing Loss: 0.00132200145162642 | Testing Acc: 100.0\n",
            "Epoch: 4100 | Training Loss: 0.0005879547097720206 | Training Acc: 100.0 | Testing Loss: 0.0013219956308603287 | Testing Acc: 100.0\n",
            "Epoch: 4101 | Training Loss: 0.0005877500516362488 | Training Acc: 100.0 | Testing Loss: 0.0013214672217145562 | Testing Acc: 100.0\n",
            "Epoch: 4102 | Training Loss: 0.000587553542573005 | Training Acc: 100.0 | Testing Loss: 0.00132143241353333 | Testing Acc: 100.0\n",
            "Epoch: 4103 | Training Loss: 0.000587351736612618 | Training Acc: 100.0 | Testing Loss: 0.0013208864256739616 | Testing Acc: 100.0\n",
            "Epoch: 4104 | Training Loss: 0.0005871494067832828 | Training Acc: 100.0 | Testing Loss: 0.0013208574382588267 | Testing Acc: 100.0\n",
            "Epoch: 4105 | Training Loss: 0.00058695487678051 | Training Acc: 100.0 | Testing Loss: 0.0013203058624640107 | Testing Acc: 100.0\n",
            "Epoch: 4106 | Training Loss: 0.0005867481813766062 | Training Acc: 100.0 | Testing Loss: 0.0013197537045925856 | Testing Acc: 100.0\n",
            "Epoch: 4107 | Training Loss: 0.0005865531857125461 | Training Acc: 100.0 | Testing Loss: 0.0013197247171774507 | Testing Acc: 100.0\n",
            "Epoch: 4108 | Training Loss: 0.0005863499827682972 | Training Acc: 100.0 | Testing Loss: 0.0013191902544349432 | Testing Acc: 100.0\n",
            "Epoch: 4109 | Training Loss: 0.0005861578392796218 | Training Acc: 100.0 | Testing Loss: 0.0013191669713705778 | Testing Acc: 100.0\n",
            "Epoch: 4110 | Training Loss: 0.0005859605735167861 | Training Acc: 100.0 | Testing Loss: 0.0013186269206926227 | Testing Acc: 100.0\n",
            "Epoch: 4111 | Training Loss: 0.0005857538199052215 | Training Acc: 100.0 | Testing Loss: 0.001318092574365437 | Testing Acc: 100.0\n",
            "Epoch: 4112 | Training Loss: 0.0005855587660335004 | Training Acc: 100.0 | Testing Loss: 0.001318075112067163 | Testing Acc: 100.0\n",
            "Epoch: 4113 | Training Loss: 0.0005853569600731134 | Training Acc: 100.0 | Testing Loss: 0.0013175408821552992 | Testing Acc: 100.0\n",
            "Epoch: 4114 | Training Loss: 0.0005851619644090533 | Training Acc: 100.0 | Testing Loss: 0.0013175175990909338 | Testing Acc: 100.0\n",
            "Epoch: 4115 | Training Loss: 0.000584969122428447 | Training Acc: 100.0 | Testing Loss: 0.00131698336917907 | Testing Acc: 100.0\n",
            "Epoch: 4116 | Training Loss: 0.0005847681313753128 | Training Acc: 100.0 | Testing Loss: 0.0013169425074011087 | Testing Acc: 100.0\n",
            "Epoch: 4117 | Training Loss: 0.0005845737759955227 | Training Acc: 100.0 | Testing Loss: 0.001316396752372384 | Testing Acc: 100.0\n",
            "Epoch: 4118 | Training Loss: 0.0005843729013577104 | Training Acc: 100.0 | Testing Loss: 0.0013158798683434725 | Testing Acc: 100.0\n",
            "Epoch: 4119 | Training Loss: 0.000584179419092834 | Training Acc: 100.0 | Testing Loss: 0.0013158622896298766 | Testing Acc: 100.0\n",
            "Epoch: 4120 | Training Loss: 0.0005839790683239698 | Training Acc: 100.0 | Testing Loss: 0.0013153045438230038 | Testing Acc: 100.0\n",
            "Epoch: 4121 | Training Loss: 0.0005837811040692031 | Training Acc: 100.0 | Testing Loss: 0.0013152637984603643 | Testing Acc: 100.0\n",
            "Epoch: 4122 | Training Loss: 0.0005835822084918618 | Training Acc: 100.0 | Testing Loss: 0.0013147236313670874 | Testing Acc: 100.0\n",
            "Epoch: 4123 | Training Loss: 0.0005833799368701875 | Training Acc: 100.0 | Testing Loss: 0.0013142067473381758 | Testing Acc: 100.0\n",
            "Epoch: 4124 | Training Loss: 0.0005831877933815122 | Training Acc: 100.0 | Testing Loss: 0.0013141834642738104 | Testing Acc: 100.0\n",
            "Epoch: 4125 | Training Loss: 0.0005829889560118318 | Training Acc: 100.0 | Testing Loss: 0.001313654938712716 | Testing Acc: 100.0\n",
            "Epoch: 4126 | Training Loss: 0.0005827940185554326 | Training Acc: 100.0 | Testing Loss: 0.0013136316556483507 | Testing Acc: 100.0\n",
            "Epoch: 4127 | Training Loss: 0.0005825952393934131 | Training Acc: 100.0 | Testing Loss: 0.0013131089508533478 | Testing Acc: 100.0\n",
            "Epoch: 4128 | Training Loss: 0.0005823973333463073 | Training Acc: 100.0 | Testing Loss: 0.0013130625011399388 | Testing Acc: 100.0\n",
            "Epoch: 4129 | Training Loss: 0.000582205830141902 | Training Acc: 100.0 | Testing Loss: 0.0013125163968652487 | Testing Acc: 100.0\n",
            "Epoch: 4130 | Training Loss: 0.0005820064106956124 | Training Acc: 100.0 | Testing Loss: 0.0013119938084855676 | Testing Acc: 100.0\n",
            "Epoch: 4131 | Training Loss: 0.0005818188656121492 | Training Acc: 100.0 | Testing Loss: 0.001311964588239789 | Testing Acc: 100.0\n",
            "Epoch: 4132 | Training Loss: 0.0005816184566356242 | Training Acc: 100.0 | Testing Loss: 0.0013114360626786947 | Testing Acc: 100.0\n",
            "Epoch: 4133 | Training Loss: 0.0005814189789816737 | Training Acc: 100.0 | Testing Loss: 0.0013114127796143293 | Testing Acc: 100.0\n",
            "Epoch: 4134 | Training Loss: 0.0005812246818095446 | Training Acc: 100.0 | Testing Loss: 0.0013108841376379132 | Testing Acc: 100.0\n",
            "Epoch: 4135 | Training Loss: 0.000581025262363255 | Training Acc: 100.0 | Testing Loss: 0.0013103554956614971 | Testing Acc: 100.0\n",
            "Epoch: 4136 | Training Loss: 0.0005808361456729472 | Training Acc: 100.0 | Testing Loss: 0.0013103263918310404 | Testing Acc: 100.0\n",
            "Epoch: 4137 | Training Loss: 0.0005806373665109277 | Training Acc: 100.0 | Testing Loss: 0.001309797866269946 | Testing Acc: 100.0\n",
            "Epoch: 4138 | Training Loss: 0.0005804423126392066 | Training Acc: 100.0 | Testing Loss: 0.0013097745832055807 | Testing Acc: 100.0\n",
            "Epoch: 4139 | Training Loss: 0.0005802478408440948 | Training Acc: 100.0 | Testing Loss: 0.001309240236878395 | Testing Acc: 100.0\n",
            "Epoch: 4140 | Training Loss: 0.0005800469662062824 | Training Acc: 100.0 | Testing Loss: 0.0013091994915157557 | Testing Acc: 100.0\n",
            "Epoch: 4141 | Training Loss: 0.0005798628553748131 | Training Acc: 100.0 | Testing Loss: 0.0013086594408378005 | Testing Acc: 100.0\n",
            "Epoch: 4142 | Training Loss: 0.0005796574987471104 | Training Acc: 100.0 | Testing Loss: 0.0013081480283290148 | Testing Acc: 100.0\n",
            "Epoch: 4143 | Training Loss: 0.0005794669268652797 | Training Acc: 100.0 | Testing Loss: 0.0013081131037324667 | Testing Acc: 100.0\n",
            "Epoch: 4144 | Training Loss: 0.0005792755400761962 | Training Acc: 100.0 | Testing Loss: 0.001307590282522142 | Testing Acc: 100.0\n",
            "Epoch: 4145 | Training Loss: 0.0005790761206299067 | Training Acc: 100.0 | Testing Loss: 0.0013075495371595025 | Testing Acc: 100.0\n",
            "Epoch: 4146 | Training Loss: 0.0005788831622339785 | Training Acc: 100.0 | Testing Loss: 0.0013070210115984082 | Testing Acc: 100.0\n",
            "Epoch: 4147 | Training Loss: 0.0005786836263723671 | Training Acc: 100.0 | Testing Loss: 0.001306503894738853 | Testing Acc: 100.0\n",
            "Epoch: 4148 | Training Loss: 0.0005784989334642887 | Training Acc: 100.0 | Testing Loss: 0.0013064747909083962 | Testing Acc: 100.0\n",
            "Epoch: 4149 | Training Loss: 0.000578301667701453 | Training Acc: 100.0 | Testing Loss: 0.001305946265347302 | Testing Acc: 100.0\n",
            "Epoch: 4150 | Training Loss: 0.0005781051586382091 | Training Acc: 100.0 | Testing Loss: 0.0013059284538030624 | Testing Acc: 100.0\n",
            "Epoch: 4151 | Training Loss: 0.0005779137136414647 | Training Acc: 100.0 | Testing Loss: 0.0013053999282419682 | Testing Acc: 100.0\n",
            "Epoch: 4152 | Training Loss: 0.0005777186597697437 | Training Acc: 100.0 | Testing Loss: 0.001305365120060742 | Testing Acc: 100.0\n",
            "Epoch: 4153 | Training Loss: 0.0005775286699645221 | Training Acc: 100.0 | Testing Loss: 0.0013048306573182344 | Testing Acc: 100.0\n",
            "Epoch: 4154 | Training Loss: 0.0005773291341029108 | Training Acc: 100.0 | Testing Loss: 0.0013043018989264965 | Testing Acc: 100.0\n",
            "Epoch: 4155 | Training Loss: 0.0005771429860033095 | Training Acc: 100.0 | Testing Loss: 0.001304278732277453 | Testing Acc: 100.0\n",
            "Epoch: 4156 | Training Loss: 0.0005769441486336291 | Training Acc: 100.0 | Testing Loss: 0.0013037500903010368 | Testing Acc: 100.0\n",
            "Epoch: 4157 | Training Loss: 0.0005767521215602756 | Training Acc: 100.0 | Testing Loss: 0.00130372098647058 | Testing Acc: 100.0\n",
            "Epoch: 4158 | Training Loss: 0.0005765620735473931 | Training Acc: 100.0 | Testing Loss: 0.001303192344494164 | Testing Acc: 100.0\n",
            "Epoch: 4159 | Training Loss: 0.0005763595690950751 | Training Acc: 100.0 | Testing Loss: 0.0013026928063482046 | Testing Acc: 100.0\n",
            "Epoch: 4160 | Training Loss: 0.0005761735956184566 | Training Acc: 100.0 | Testing Loss: 0.0013026754604652524 | Testing Acc: 100.0\n",
            "Epoch: 4161 | Training Loss: 0.0005759792402386665 | Training Acc: 100.0 | Testing Loss: 0.0013021465856581926 | Testing Acc: 100.0\n",
            "Epoch: 4162 | Training Loss: 0.000575788551941514 | Training Acc: 100.0 | Testing Loss: 0.0013021177146583796 | Testing Acc: 100.0\n",
            "Epoch: 4163 | Training Loss: 0.0005755971069447696 | Training Acc: 100.0 | Testing Loss: 0.0013015888398513198 | Testing Acc: 100.0\n",
            "Epoch: 4164 | Training Loss: 0.0005753976874984801 | Training Acc: 100.0 | Testing Loss: 0.0013015480944886804 | Testing Acc: 100.0\n",
            "Epoch: 4165 | Training Loss: 0.000575212121475488 | Training Acc: 100.0 | Testing Loss: 0.0013010076945647597 | Testing Acc: 100.0\n",
            "Epoch: 4166 | Training Loss: 0.0005750140408053994 | Training Acc: 100.0 | Testing Loss: 0.0013004906941205263 | Testing Acc: 100.0\n",
            "Epoch: 4167 | Training Loss: 0.0005748294061049819 | Training Acc: 100.0 | Testing Loss: 0.0013004790525883436 | Testing Acc: 100.0\n",
            "Epoch: 4168 | Training Loss: 0.0005746305687353015 | Training Acc: 100.0 | Testing Loss: 0.0012999502941966057 | Testing Acc: 100.0\n",
            "Epoch: 4169 | Training Loss: 0.0005744400550611317 | Training Acc: 100.0 | Testing Loss: 0.001299921190366149 | Testing Acc: 100.0\n",
            "Epoch: 4170 | Training Loss: 0.0005742498906329274 | Training Acc: 100.0 | Testing Loss: 0.0012993983691558242 | Testing Acc: 100.0\n",
            "Epoch: 4171 | Training Loss: 0.0005740534979850054 | Training Acc: 100.0 | Testing Loss: 0.0012988870730623603 | Testing Acc: 100.0\n",
            "Epoch: 4172 | Training Loss: 0.000573868805076927 | Training Acc: 100.0 | Testing Loss: 0.0012988580856472254 | Testing Acc: 100.0\n",
            "Epoch: 4173 | Training Loss: 0.0005736728780902922 | Training Acc: 100.0 | Testing Loss: 0.0012983467895537615 | Testing Acc: 100.0\n",
            "Epoch: 4174 | Training Loss: 0.0005734822480008006 | Training Acc: 100.0 | Testing Loss: 0.0012983058113604784 | Testing Acc: 100.0\n",
            "Epoch: 4175 | Training Loss: 0.0005732967401854694 | Training Acc: 100.0 | Testing Loss: 0.0012977772857993841 | Testing Acc: 100.0\n",
            "Epoch: 4176 | Training Loss: 0.0005731001729145646 | Training Acc: 100.0 | Testing Loss: 0.0012977597070857882 | Testing Acc: 100.0\n",
            "Epoch: 4177 | Training Loss: 0.0005729175172746181 | Training Acc: 100.0 | Testing Loss: 0.0012972310651093721 | Testing Acc: 100.0\n",
            "Epoch: 4178 | Training Loss: 0.0005727166426368058 | Training Acc: 100.0 | Testing Loss: 0.0012967020738869905 | Testing Acc: 100.0\n",
            "Epoch: 4179 | Training Loss: 0.000572527467738837 | Training Acc: 100.0 | Testing Loss: 0.0012966787908226252 | Testing Acc: 100.0\n",
            "Epoch: 4180 | Training Loss: 0.0005723374197259545 | Training Acc: 100.0 | Testing Loss: 0.001296161557547748 | Testing Acc: 100.0\n",
            "Epoch: 4181 | Training Loss: 0.0005721468478441238 | Training Acc: 100.0 | Testing Loss: 0.0012961208121851087 | Testing Acc: 100.0\n",
            "Epoch: 4182 | Training Loss: 0.000571959768421948 | Training Acc: 100.0 | Testing Loss: 0.001295591937378049 | Testing Acc: 100.0\n",
            "Epoch: 4183 | Training Loss: 0.0005717618041671813 | Training Acc: 100.0 | Testing Loss: 0.0012950865784659982 | Testing Acc: 100.0\n",
            "Epoch: 4184 | Training Loss: 0.0005715741426683962 | Training Acc: 100.0 | Testing Loss: 0.0012950807576999068 | Testing Acc: 100.0\n",
            "Epoch: 4185 | Training Loss: 0.00057138258125633 | Training Acc: 100.0 | Testing Loss: 0.001294569345191121 | Testing Acc: 100.0\n",
            "Epoch: 4186 | Training Loss: 0.0005711920675821602 | Training Acc: 100.0 | Testing Loss: 0.0012945287162438035 | Testing Acc: 100.0\n",
            "Epoch: 4187 | Training Loss: 0.0005710079567506909 | Training Acc: 100.0 | Testing Loss: 0.00129401171579957 | Testing Acc: 100.0\n",
            "Epoch: 4188 | Training Loss: 0.0005708099924959242 | Training Acc: 100.0 | Testing Loss: 0.001293976791203022 | Testing Acc: 100.0\n",
            "Epoch: 4189 | Training Loss: 0.0005706286756321788 | Training Acc: 100.0 | Testing Loss: 0.0012934538535773754 | Testing Acc: 100.0\n",
            "Epoch: 4190 | Training Loss: 0.0005704337963834405 | Training Acc: 100.0 | Testing Loss: 0.0012929366203024983 | Testing Acc: 100.0\n",
            "Epoch: 4191 | Training Loss: 0.0005702461348846555 | Training Acc: 100.0 | Testing Loss: 0.0012929134536534548 | Testing Acc: 100.0\n",
            "Epoch: 4192 | Training Loss: 0.0005700575420632958 | Training Acc: 100.0 | Testing Loss: 0.0012923844624310732 | Testing Acc: 100.0\n",
            "Epoch: 4193 | Training Loss: 0.0005698727909475565 | Training Acc: 100.0 | Testing Loss: 0.0012923670001327991 | Testing Acc: 100.0\n",
            "Epoch: 4194 | Training Loss: 0.0005696828593499959 | Training Acc: 100.0 | Testing Loss: 0.001291849883273244 | Testing Acc: 100.0\n",
            "Epoch: 4195 | Training Loss: 0.0005694864084944129 | Training Acc: 100.0 | Testing Loss: 0.0012913265964016318 | Testing Acc: 100.0\n",
            "Epoch: 4196 | Training Loss: 0.0005693089915439487 | Training Acc: 100.0 | Testing Loss: 0.0012913091341033578 | Testing Acc: 100.0\n",
            "Epoch: 4197 | Training Loss: 0.000569113006349653 | Training Acc: 100.0 | Testing Loss: 0.0012908036587759852 | Testing Acc: 100.0\n",
            "Epoch: 4198 | Training Loss: 0.0005689238896593451 | Training Acc: 100.0 | Testing Loss: 0.001290768850594759 | Testing Acc: 100.0\n",
            "Epoch: 4199 | Training Loss: 0.0005687352968379855 | Training Acc: 100.0 | Testing Loss: 0.001290251617319882 | Testing Acc: 100.0\n",
            "Epoch: 4200 | Training Loss: 0.0005685446667484939 | Training Acc: 100.0 | Testing Loss: 0.0012902341550216079 | Testing Acc: 100.0\n",
            "Epoch: 4201 | Training Loss: 0.0005683650961145759 | Training Acc: 100.0 | Testing Loss: 0.0012897112173959613 | Testing Acc: 100.0\n",
            "Epoch: 4202 | Training Loss: 0.000568168587051332 | Training Acc: 100.0 | Testing Loss: 0.0012891938677057624 | Testing Acc: 100.0\n",
            "Epoch: 4203 | Training Loss: 0.000567980925552547 | Training Acc: 100.0 | Testing Loss: 0.0012891707010567188 | Testing Acc: 100.0\n",
            "Epoch: 4204 | Training Loss: 0.0005677937297150493 | Training Acc: 100.0 | Testing Loss: 0.0012886535841971636 | Testing Acc: 100.0\n",
            "Epoch: 4205 | Training Loss: 0.0005676061846315861 | Training Acc: 100.0 | Testing Loss: 0.0012886303011327982 | Testing Acc: 100.0\n",
            "Epoch: 4206 | Training Loss: 0.0005674205021932721 | Training Acc: 100.0 | Testing Loss: 0.0012881014263257384 | Testing Acc: 100.0\n",
            "Epoch: 4207 | Training Loss: 0.000567225506529212 | Training Acc: 100.0 | Testing Loss: 0.0012875956017524004 | Testing Acc: 100.0\n",
            "Epoch: 4208 | Training Loss: 0.0005670436657965183 | Training Acc: 100.0 | Testing Loss: 0.0012875780230388045 | Testing Acc: 100.0\n",
            "Epoch: 4209 | Training Loss: 0.0005668596713803709 | Training Acc: 100.0 | Testing Loss: 0.0012870783684775233 | Testing Acc: 100.0\n",
            "Epoch: 4210 | Training Loss: 0.0005666720680892467 | Training Acc: 100.0 | Testing Loss: 0.0012870316859334707 | Testing Acc: 100.0\n",
            "Epoch: 4211 | Training Loss: 0.0005664908676408231 | Training Acc: 100.0 | Testing Loss: 0.0012865203898400068 | Testing Acc: 100.0\n",
            "Epoch: 4212 | Training Loss: 0.0005662972689606249 | Training Acc: 100.0 | Testing Loss: 0.0012864854652434587 | Testing Acc: 100.0\n",
            "Epoch: 4213 | Training Loss: 0.0005661160103045404 | Training Acc: 100.0 | Testing Loss: 0.0012859507696703076 | Testing Acc: 100.0\n",
            "Epoch: 4214 | Training Loss: 0.0005659254966303706 | Training Acc: 100.0 | Testing Loss: 0.0012854508822783828 | Testing Acc: 100.0\n",
            "Epoch: 4215 | Training Loss: 0.0005657408619299531 | Training Acc: 100.0 | Testing Loss: 0.0012854218948632479 | Testing Acc: 100.0\n",
            "Epoch: 4216 | Training Loss: 0.0005655506392940879 | Training Acc: 100.0 | Testing Loss: 0.0012849163031205535 | Testing Acc: 100.0\n",
            "Epoch: 4217 | Training Loss: 0.0005653600092045963 | Training Acc: 100.0 | Testing Loss: 0.0012848987244069576 | Testing Acc: 100.0\n",
            "Epoch: 4218 | Training Loss: 0.0005651760147884488 | Training Acc: 100.0 | Testing Loss: 0.0012843816075474024 | Testing Acc: 100.0\n",
            "Epoch: 4219 | Training Loss: 0.0005649853264912963 | Training Acc: 100.0 | Testing Loss: 0.0012838877737522125 | Testing Acc: 100.0\n",
            "Epoch: 4220 | Training Loss: 0.0005648050573654473 | Training Acc: 100.0 | Testing Loss: 0.001283858553506434 | Testing Acc: 100.0\n",
            "Epoch: 4221 | Training Loss: 0.0005646134959533811 | Training Acc: 100.0 | Testing Loss: 0.001283358782529831 | Testing Acc: 100.0\n",
            "Epoch: 4222 | Training Loss: 0.0005644274060614407 | Training Acc: 100.0 | Testing Loss: 0.0012833296786993742 | Testing Acc: 100.0\n",
            "Epoch: 4223 | Training Loss: 0.0005642476608045399 | Training Acc: 100.0 | Testing Loss: 0.0012828124454244971 | Testing Acc: 100.0\n",
            "Epoch: 4224 | Training Loss: 0.0005640584276989102 | Training Acc: 100.0 | Testing Loss: 0.0012827774044126272 | Testing Acc: 100.0\n",
            "Epoch: 4225 | Training Loss: 0.0005638786824420094 | Training Acc: 100.0 | Testing Loss: 0.0012822543503716588 | Testing Acc: 100.0\n",
            "Epoch: 4226 | Training Loss: 0.0005636881105601788 | Training Acc: 100.0 | Testing Loss: 0.0012817659880965948 | Testing Acc: 100.0\n",
            "Epoch: 4227 | Training Loss: 0.0005635077832266688 | Training Acc: 100.0 | Testing Loss: 0.0012817428214475513 | Testing Acc: 100.0\n",
            "Epoch: 4228 | Training Loss: 0.0005633192486129701 | Training Acc: 100.0 | Testing Loss: 0.0012812312925234437 | Testing Acc: 100.0\n",
            "Epoch: 4229 | Training Loss: 0.0005631301319226623 | Training Acc: 100.0 | Testing Loss: 0.0012811964843422174 | Testing Acc: 100.0\n",
            "Epoch: 4230 | Training Loss: 0.0005629519000649452 | Training Acc: 100.0 | Testing Loss: 0.0012806967133656144 | Testing Acc: 100.0\n",
            "Epoch: 4231 | Training Loss: 0.0005627641803584993 | Training Acc: 100.0 | Testing Loss: 0.0012801910052075982 | Testing Acc: 100.0\n",
            "Epoch: 4232 | Training Loss: 0.0005625824560411274 | Training Acc: 100.0 | Testing Loss: 0.0012801678385585546 | Testing Acc: 100.0\n",
            "Epoch: 4233 | Training Loss: 0.0005623938050121069 | Training Acc: 100.0 | Testing Loss: 0.0012796679511666298 | Testing Acc: 100.0\n",
            "Epoch: 4234 | Training Loss: 0.0005622091703116894 | Training Acc: 100.0 | Testing Loss: 0.0012796621304005384 | Testing Acc: 100.0\n",
            "Epoch: 4235 | Training Loss: 0.000562026456464082 | Training Acc: 100.0 | Testing Loss: 0.0012791447807103395 | Testing Acc: 100.0\n",
            "Epoch: 4236 | Training Loss: 0.0005618358263745904 | Training Acc: 100.0 | Testing Loss: 0.0012791099725291133 | Testing Acc: 100.0\n",
            "Epoch: 4237 | Training Loss: 0.0005616590497083962 | Training Acc: 100.0 | Testing Loss: 0.0012785985600203276 | Testing Acc: 100.0\n",
            "Epoch: 4238 | Training Loss: 0.0005614729598164558 | Training Acc: 100.0 | Testing Loss: 0.0012780925026163459 | Testing Acc: 100.0\n",
            "Epoch: 4239 | Training Loss: 0.0005612880922853947 | Training Acc: 100.0 | Testing Loss: 0.001278057461604476 | Testing Acc: 100.0\n",
            "Epoch: 4240 | Training Loss: 0.0005611024098470807 | Training Acc: 100.0 | Testing Loss: 0.0012775519862771034 | Testing Acc: 100.0\n",
            "Epoch: 4241 | Training Loss: 0.0005609148065559566 | Training Acc: 100.0 | Testing Loss: 0.0012775110080838203 | Testing Acc: 100.0\n",
            "Epoch: 4242 | Training Loss: 0.0005607350612990558 | Training Acc: 100.0 | Testing Loss: 0.0012769994791597128 | Testing Acc: 100.0\n",
            "Epoch: 4243 | Training Loss: 0.0005605473415926099 | Training Acc: 100.0 | Testing Loss: 0.0012765228748321533 | Testing Acc: 100.0\n",
            "Epoch: 4244 | Training Loss: 0.0005603671306744218 | Training Acc: 100.0 | Testing Loss: 0.0012764997081831098 | Testing Acc: 100.0\n",
            "Epoch: 4245 | Training Loss: 0.0005601770826615393 | Training Acc: 100.0 | Testing Loss: 0.001275999820791185 | Testing Acc: 100.0\n",
            "Epoch: 4246 | Training Loss: 0.0005599967553280294 | Training Acc: 100.0 | Testing Loss: 0.001275964779779315 | Testing Acc: 100.0\n",
            "Epoch: 4247 | Training Loss: 0.0005598154966719449 | Training Acc: 100.0 | Testing Loss: 0.0012754590716212988 | Testing Acc: 100.0\n",
            "Epoch: 4248 | Training Loss: 0.0005596278351731598 | Training Acc: 100.0 | Testing Loss: 0.0012754241470247507 | Testing Acc: 100.0\n",
            "Epoch: 4249 | Training Loss: 0.0005594510585069656 | Training Acc: 100.0 | Testing Loss: 0.0012749126181006432 | Testing Acc: 100.0\n",
            "Epoch: 4250 | Training Loss: 0.0005592632805928588 | Training Acc: 100.0 | Testing Loss: 0.0012744186678901315 | Testing Acc: 100.0\n",
            "Epoch: 4251 | Training Loss: 0.0005590785876847804 | Training Acc: 100.0 | Testing Loss: 0.0012744010891765356 | Testing Acc: 100.0\n",
            "Epoch: 4252 | Training Loss: 0.0005588944186456501 | Training Acc: 100.0 | Testing Loss: 0.001273907022550702 | Testing Acc: 100.0\n",
            "Epoch: 4253 | Training Loss: 0.000558706815354526 | Training Acc: 100.0 | Testing Loss: 0.001273872097954154 | Testing Acc: 100.0\n",
            "Epoch: 4254 | Training Loss: 0.0005585328908637166 | Training Acc: 100.0 | Testing Loss: 0.0012733604526147246 | Testing Acc: 100.0\n",
            "Epoch: 4255 | Training Loss: 0.0005583468009717762 | Training Acc: 100.0 | Testing Loss: 0.0012728662695735693 | Testing Acc: 100.0\n",
            "Epoch: 4256 | Training Loss: 0.0005581694422289729 | Training Acc: 100.0 | Testing Loss: 0.001272842986509204 | Testing Acc: 100.0\n",
            "Epoch: 4257 | Training Loss: 0.0005579852149821818 | Training Acc: 100.0 | Testing Loss: 0.0012723428662866354 | Testing Acc: 100.0\n",
            "Epoch: 4258 | Training Loss: 0.0005578019190579653 | Training Acc: 100.0 | Testing Loss: 0.0012723255204036832 | Testing Acc: 100.0\n",
            "Epoch: 4259 | Training Loss: 0.0005576222320087254 | Training Acc: 100.0 | Testing Loss: 0.0012718199286609888 | Testing Acc: 100.0\n",
            "Epoch: 4260 | Training Loss: 0.0005574360839091241 | Training Acc: 100.0 | Testing Loss: 0.001271331449970603 | Testing Acc: 100.0\n",
            "Epoch: 4261 | Training Loss: 0.0005572602385655046 | Training Acc: 100.0 | Testing Loss: 0.0012713141040876508 | Testing Acc: 100.0\n",
            "Epoch: 4262 | Training Loss: 0.0005570760113187134 | Training Acc: 100.0 | Testing Loss: 0.0012708200374618173 | Testing Acc: 100.0\n",
            "Epoch: 4263 | Training Loss: 0.0005568956839852035 | Training Acc: 100.0 | Testing Loss: 0.0012707909336313605 | Testing Acc: 100.0\n",
            "Epoch: 4264 | Training Loss: 0.000556713086552918 | Training Acc: 100.0 | Testing Loss: 0.0012702791718766093 | Testing Acc: 100.0\n",
            "Epoch: 4265 | Training Loss: 0.0005565297906287014 | Training Acc: 100.0 | Testing Loss: 0.0012702501844614744 | Testing Acc: 100.0\n",
            "Epoch: 4266 | Training Loss: 0.0005563528975471854 | Training Acc: 100.0 | Testing Loss: 0.0012697444763034582 | Testing Acc: 100.0\n",
            "Epoch: 4267 | Training Loss: 0.0005561637226492167 | Training Acc: 100.0 | Testing Loss: 0.00126927369274199 | Testing Acc: 100.0\n",
            "Epoch: 4268 | Training Loss: 0.0005559864803217351 | Training Acc: 100.0 | Testing Loss: 0.0012692444724962115 | Testing Acc: 100.0\n",
            "Epoch: 4269 | Training Loss: 0.0005558037664741278 | Training Acc: 100.0 | Testing Loss: 0.0012687563430517912 | Testing Acc: 100.0\n",
            "Epoch: 4270 | Training Loss: 0.0005556234391406178 | Training Acc: 100.0 | Testing Loss: 0.0012687214184552431 | Testing Acc: 100.0\n",
            "Epoch: 4271 | Training Loss: 0.0005554391536861658 | Training Acc: 100.0 | Testing Loss: 0.001268209656700492 | Testing Acc: 100.0\n",
            "Epoch: 4272 | Training Loss: 0.0005552530055865645 | Training Acc: 100.0 | Testing Loss: 0.0012677153572440147 | Testing Acc: 100.0\n",
            "Epoch: 4273 | Training Loss: 0.0005550786154344678 | Training Acc: 100.0 | Testing Loss: 0.001267703715711832 | Testing Acc: 100.0\n",
            "Epoch: 4274 | Training Loss: 0.0005548959015868604 | Training Acc: 100.0 | Testing Loss: 0.0012672097655013204 | Testing Acc: 100.0\n",
            "Epoch: 4275 | Training Loss: 0.0005547170294448733 | Training Acc: 100.0 | Testing Loss: 0.0012671803124248981 | Testing Acc: 100.0\n",
            "Epoch: 4276 | Training Loss: 0.0005545328604057431 | Training Acc: 100.0 | Testing Loss: 0.0012666803086176515 | Testing Acc: 100.0\n",
            "Epoch: 4277 | Training Loss: 0.0005543541046790779 | Training Acc: 100.0 | Testing Loss: 0.0012666512047871947 | Testing Acc: 100.0\n",
            "Epoch: 4278 | Training Loss: 0.0005541816353797913 | Training Acc: 100.0 | Testing Loss: 0.0012661514338105917 | Testing Acc: 100.0\n",
            "Epoch: 4279 | Training Loss: 0.00055399548728019 | Training Acc: 100.0 | Testing Loss: 0.0012656571343541145 | Testing Acc: 100.0\n",
            "Epoch: 4280 | Training Loss: 0.0005538151017390192 | Training Acc: 100.0 | Testing Loss: 0.0012656280305236578 | Testing Acc: 100.0\n",
            "Epoch: 4281 | Training Loss: 0.0005536353564821184 | Training Acc: 100.0 | Testing Loss: 0.001265145605430007 | Testing Acc: 100.0\n",
            "Epoch: 4282 | Training Loss: 0.0005534535157494247 | Training Acc: 100.0 | Testing Loss: 0.001265110680833459 | Testing Acc: 100.0\n",
            "Epoch: 4283 | Training Loss: 0.0005532752256840467 | Training Acc: 100.0 | Testing Loss: 0.0012646106770262122 | Testing Acc: 100.0\n",
            "Epoch: 4284 | Training Loss: 0.0005530904745683074 | Training Acc: 100.0 | Testing Loss: 0.001264151418581605 | Testing Acc: 100.0\n",
            "Epoch: 4285 | Training Loss: 0.0005529145710170269 | Training Acc: 100.0 | Testing Loss: 0.0012641281355172396 | Testing Acc: 100.0\n",
            "Epoch: 4286 | Training Loss: 0.0005527304601855576 | Training Acc: 100.0 | Testing Loss: 0.001263634068891406 | Testing Acc: 100.0\n",
            "Epoch: 4287 | Training Loss: 0.0005525516462512314 | Training Acc: 100.0 | Testing Loss: 0.0012636048486456275 | Testing Acc: 100.0\n",
            "Epoch: 4288 | Training Loss: 0.0005523762665688992 | Training Acc: 100.0 | Testing Loss: 0.0012630989076569676 | Testing Acc: 100.0\n",
            "Epoch: 4289 | Training Loss: 0.0005521959392353892 | Training Acc: 100.0 | Testing Loss: 0.0012630580458790064 | Testing Acc: 100.0\n",
            "Epoch: 4290 | Training Loss: 0.0005520235863514245 | Training Acc: 100.0 | Testing Loss: 0.0012625522213056684 | Testing Acc: 100.0\n",
            "Epoch: 4291 | Training Loss: 0.0005518373800441623 | Training Acc: 100.0 | Testing Loss: 0.0012620696797966957 | Testing Acc: 100.0\n",
            "Epoch: 4292 | Training Loss: 0.0005516629898920655 | Training Acc: 100.0 | Testing Loss: 0.001262058038264513 | Testing Acc: 100.0\n",
            "Epoch: 4293 | Training Loss: 0.0005514846416190267 | Training Acc: 100.0 | Testing Loss: 0.0012615697924047709 | Testing Acc: 100.0\n",
            "Epoch: 4294 | Training Loss: 0.000551308854483068 | Training Acc: 100.0 | Testing Loss: 0.0012615289306268096 | Testing Acc: 100.0\n",
            "Epoch: 4295 | Training Loss: 0.0005511320196092129 | Training Acc: 100.0 | Testing Loss: 0.0012610347475856543 | Testing Acc: 100.0\n",
            "Epoch: 4296 | Training Loss: 0.0005509472684934735 | Training Acc: 100.0 | Testing Loss: 0.0012605581432580948 | Testing Acc: 100.0\n",
            "Epoch: 4297 | Training Loss: 0.0005507757887244225 | Training Acc: 100.0 | Testing Loss: 0.0012605289230123162 | Testing Acc: 100.0\n",
            "Epoch: 4298 | Training Loss: 0.0005505915032699704 | Training Acc: 100.0 | Testing Loss: 0.0012600404443219304 | Testing Acc: 100.0\n",
            "Epoch: 4299 | Training Loss: 0.0005504171713255346 | Training Acc: 100.0 | Testing Loss: 0.0012600054033100605 | Testing Acc: 100.0\n",
            "Epoch: 4300 | Training Loss: 0.0005502343992702663 | Training Acc: 100.0 | Testing Loss: 0.0012595170410349965 | Testing Acc: 100.0\n",
            "Epoch: 4301 | Training Loss: 0.0005500510451383889 | Training Acc: 100.0 | Testing Loss: 0.001259487820789218 | Testing Acc: 100.0\n",
            "Epoch: 4302 | Training Loss: 0.000549878750462085 | Training Acc: 100.0 | Testing Loss: 0.0012589935213327408 | Testing Acc: 100.0\n",
            "Epoch: 4303 | Training Loss: 0.0005496969679370522 | Training Acc: 100.0 | Testing Loss: 0.0012585052754729986 | Testing Acc: 100.0\n",
            "Epoch: 4304 | Training Loss: 0.0005495225777849555 | Training Acc: 100.0 | Testing Loss: 0.0012584816431626678 | Testing Acc: 100.0\n",
            "Epoch: 4305 | Training Loss: 0.0005493442295119166 | Training Acc: 100.0 | Testing Loss: 0.0012579815229400992 | Testing Acc: 100.0\n",
            "Epoch: 4306 | Training Loss: 0.0005491654155775905 | Training Acc: 100.0 | Testing Loss: 0.0012579582398757339 | Testing Acc: 100.0\n",
            "Epoch: 4307 | Training Loss: 0.000548991491086781 | Training Acc: 100.0 | Testing Loss: 0.001257458352483809 | Testing Acc: 100.0\n",
            "Epoch: 4308 | Training Loss: 0.0005488112801685929 | Training Acc: 100.0 | Testing Loss: 0.0012569815153256059 | Testing Acc: 100.0\n",
            "Epoch: 4309 | Training Loss: 0.0005486411973834038 | Training Acc: 100.0 | Testing Loss: 0.0012569755781441927 | Testing Acc: 100.0\n",
            "Epoch: 4310 | Training Loss: 0.0005484629655256867 | Training Acc: 100.0 | Testing Loss: 0.0012564815115183592 | Testing Acc: 100.0\n",
            "Epoch: 4311 | Training Loss: 0.00054828857537359 | Training Acc: 100.0 | Testing Loss: 0.0012564522912725806 | Testing Acc: 100.0\n",
            "Epoch: 4312 | Training Loss: 0.0005481117987073958 | Training Acc: 100.0 | Testing Loss: 0.001255963696166873 | Testing Acc: 100.0\n",
            "Epoch: 4313 | Training Loss: 0.0005479298997670412 | Training Acc: 100.0 | Testing Loss: 0.0012559288879856467 | Testing Acc: 100.0\n",
            "Epoch: 4314 | Training Loss: 0.0005477589438669384 | Training Acc: 100.0 | Testing Loss: 0.0012554287677630782 | Testing Acc: 100.0\n",
            "Epoch: 4315 | Training Loss: 0.0005475801881402731 | Training Acc: 100.0 | Testing Loss: 0.001254952047020197 | Testing Acc: 100.0\n",
            "Epoch: 4316 | Training Loss: 0.0005474102217704058 | Training Acc: 100.0 | Testing Loss: 0.0012549228267744184 | Testing Acc: 100.0\n",
            "Epoch: 4317 | Training Loss: 0.0005472288466989994 | Training Acc: 100.0 | Testing Loss: 0.0012544401688501239 | Testing Acc: 100.0\n",
            "Epoch: 4318 | Training Loss: 0.0005470500327646732 | Training Acc: 100.0 | Testing Loss: 0.0012544228229671717 | Testing Acc: 100.0\n",
            "Epoch: 4319 | Training Loss: 0.000546874653082341 | Training Acc: 100.0 | Testing Loss: 0.001253940281458199 | Testing Acc: 100.0\n",
            "Epoch: 4320 | Training Loss: 0.0005467002629302442 | Training Acc: 100.0 | Testing Loss: 0.0012534514535218477 | Testing Acc: 100.0\n",
            "Epoch: 4321 | Training Loss: 0.0005465243593789637 | Training Acc: 100.0 | Testing Loss: 0.001253422349691391 | Testing Acc: 100.0\n",
            "Epoch: 4322 | Training Loss: 0.000546344555914402 | Training Acc: 100.0 | Testing Loss: 0.0012529396917670965 | Testing Acc: 100.0\n",
            "Epoch: 4323 | Training Loss: 0.000546171679161489 | Training Acc: 100.0 | Testing Loss: 0.0012529107043519616 | Testing Acc: 100.0\n",
            "Epoch: 4324 | Training Loss: 0.0005459977546706796 | Training Acc: 100.0 | Testing Loss: 0.001252422109246254 | Testing Acc: 100.0\n",
            "Epoch: 4325 | Training Loss: 0.0005458128871396184 | Training Acc: 100.0 | Testing Loss: 0.0012524104677140713 | Testing Acc: 100.0\n",
            "Epoch: 4326 | Training Loss: 0.0005456435028463602 | Training Acc: 100.0 | Testing Loss: 0.001251916284672916 | Testing Acc: 100.0\n",
            "Epoch: 4327 | Training Loss: 0.0005454631755128503 | Training Acc: 100.0 | Testing Loss: 0.0012514276895672083 | Testing Acc: 100.0\n",
            "Epoch: 4328 | Training Loss: 0.0005452887271530926 | Training Acc: 100.0 | Testing Loss: 0.001251404406502843 | Testing Acc: 100.0\n",
            "Epoch: 4329 | Training Loss: 0.0005451148608699441 | Training Acc: 100.0 | Testing Loss: 0.0012509217485785484 | Testing Acc: 100.0\n",
            "Epoch: 4330 | Training Loss: 0.0005449374439194798 | Training Acc: 100.0 | Testing Loss: 0.001250881003215909 | Testing Acc: 100.0\n",
            "Epoch: 4331 | Training Loss: 0.0005447635194286704 | Training Acc: 100.0 | Testing Loss: 0.0012503983452916145 | Testing Acc: 100.0\n",
            "Epoch: 4332 | Training Loss: 0.0005445891292765737 | Training Acc: 100.0 | Testing Loss: 0.0012499273288995028 | Testing Acc: 100.0\n",
            "Epoch: 4333 | Training Loss: 0.0005444161361083388 | Training Acc: 100.0 | Testing Loss: 0.0012499215081334114 | Testing Acc: 100.0\n",
            "Epoch: 4334 | Training Loss: 0.0005442408728413284 | Training Acc: 100.0 | Testing Loss: 0.0012494447873905301 | Testing Acc: 100.0\n",
            "Epoch: 4335 | Training Loss: 0.000544063514098525 | Training Acc: 100.0 | Testing Loss: 0.0012494096299633384 | Testing Acc: 100.0\n",
            "Epoch: 4336 | Training Loss: 0.0005438954685814679 | Training Acc: 100.0 | Testing Loss: 0.0012489270884543657 | Testing Acc: 100.0\n",
            "Epoch: 4337 | Training Loss: 0.000543715083040297 | Training Acc: 100.0 | Testing Loss: 0.0012484559556469321 | Testing Acc: 100.0\n",
            "Epoch: 4338 | Training Loss: 0.0005435466300696135 | Training Acc: 100.0 | Testing Loss: 0.0012484267354011536 | Testing Acc: 100.0\n",
            "Epoch: 4339 | Training Loss: 0.0005433668266050518 | Training Acc: 100.0 | Testing Loss: 0.00124795560259372 | Testing Acc: 100.0\n",
            "Epoch: 4340 | Training Loss: 0.0005431968020275235 | Training Acc: 100.0 | Testing Loss: 0.00124792056158185 | Testing Acc: 100.0\n",
            "Epoch: 4341 | Training Loss: 0.0005430228775367141 | Training Acc: 100.0 | Testing Loss: 0.0012474379036575556 | Testing Acc: 100.0\n",
            "Epoch: 4342 | Training Loss: 0.0005428485455922782 | Training Acc: 100.0 | Testing Loss: 0.0012474204413592815 | Testing Acc: 100.0\n",
            "Epoch: 4343 | Training Loss: 0.0005426790448836982 | Training Acc: 100.0 | Testing Loss: 0.001246937783434987 | Testing Acc: 100.0\n",
            "Epoch: 4344 | Training Loss: 0.0005425031995400786 | Training Acc: 100.0 | Testing Loss: 0.001246460946276784 | Testing Acc: 100.0\n",
            "Epoch: 4345 | Training Loss: 0.0005423346301540732 | Training Acc: 100.0 | Testing Loss: 0.0012464376632124186 | Testing Acc: 100.0\n",
            "Epoch: 4346 | Training Loss: 0.0005421577370725572 | Training Acc: 100.0 | Testing Loss: 0.0012459608260542154 | Testing Acc: 100.0\n",
            "Epoch: 4347 | Training Loss: 0.0005419804365374148 | Training Acc: 100.0 | Testing Loss: 0.0012459198478609324 | Testing Acc: 100.0\n",
            "Epoch: 4348 | Training Loss: 0.0005418093642219901 | Training Acc: 100.0 | Testing Loss: 0.0012454371899366379 | Testing Acc: 100.0\n",
            "Epoch: 4349 | Training Loss: 0.0005416320054791868 | Training Acc: 100.0 | Testing Loss: 0.0012449717614799738 | Testing Acc: 100.0\n",
            "Epoch: 4350 | Training Loss: 0.0005414634943008423 | Training Acc: 100.0 | Testing Loss: 0.001244960119947791 | Testing Acc: 100.0\n",
            "Epoch: 4351 | Training Loss: 0.0005412882310338318 | Training Acc: 100.0 | Testing Loss: 0.001244477229192853 | Testing Acc: 100.0\n",
            "Epoch: 4352 | Training Loss: 0.0005411181482486427 | Training Acc: 100.0 | Testing Loss: 0.0012444481253623962 | Testing Acc: 100.0\n",
            "Epoch: 4353 | Training Loss: 0.0005409441655501723 | Training Acc: 100.0 | Testing Loss: 0.0012439535930752754 | Testing Acc: 100.0\n",
            "Epoch: 4354 | Training Loss: 0.0005407653516158462 | Training Acc: 100.0 | Testing Loss: 0.0012439127312973142 | Testing Acc: 100.0\n",
            "Epoch: 4355 | Training Loss: 0.0005405988194979727 | Training Acc: 100.0 | Testing Loss: 0.0012434239033609629 | Testing Acc: 100.0\n",
            "Epoch: 4356 | Training Loss: 0.0005404214607551694 | Training Acc: 100.0 | Testing Loss: 0.0012429587077349424 | Testing Acc: 100.0\n",
            "Epoch: 4357 | Training Loss: 0.0005402558599598706 | Training Acc: 100.0 | Testing Loss: 0.0012429297203198075 | Testing Acc: 100.0\n",
            "Epoch: 4358 | Training Loss: 0.000540081993676722 | Training Acc: 100.0 | Testing Loss: 0.0012424526503309608 | Testing Acc: 100.0\n",
            "Epoch: 4359 | Training Loss: 0.0005399076035246253 | Training Acc: 100.0 | Testing Loss: 0.001242441008798778 | Testing Acc: 100.0\n",
            "Epoch: 4360 | Training Loss: 0.0005397394997999072 | Training Acc: 100.0 | Testing Loss: 0.0012419583508744836 | Testing Acc: 100.0\n",
            "Epoch: 4361 | Training Loss: 0.0005395666812546551 | Training Acc: 100.0 | Testing Loss: 0.0012414931552484632 | Testing Acc: 100.0\n",
            "Epoch: 4362 | Training Loss: 0.0005393966566771269 | Training Acc: 100.0 | Testing Loss: 0.0012414639350026846 | Testing Acc: 100.0\n",
            "Epoch: 4363 | Training Loss: 0.0005392227321863174 | Training Acc: 100.0 | Testing Loss: 0.001240992802195251 | Testing Acc: 100.0\n",
            "Epoch: 4364 | Training Loss: 0.0005390467122197151 | Training Acc: 100.0 | Testing Loss: 0.001240963814780116 | Testing Acc: 100.0\n",
            "Epoch: 4365 | Training Loss: 0.000538877269718796 | Training Acc: 100.0 | Testing Loss: 0.0012404868612065911 | Testing Acc: 100.0\n",
            "Epoch: 4366 | Training Loss: 0.0005387028213590384 | Training Acc: 100.0 | Testing Loss: 0.0012404459994286299 | Testing Acc: 100.0\n",
            "Epoch: 4367 | Training Loss: 0.0005385392578318715 | Training Acc: 100.0 | Testing Loss: 0.0012399631086736917 | Testing Acc: 100.0\n",
            "Epoch: 4368 | Training Loss: 0.0005383647512644529 | Training Acc: 100.0 | Testing Loss: 0.0012395153753459454 | Testing Acc: 100.0\n",
            "Epoch: 4369 | Training Loss: 0.0005381962982937694 | Training Acc: 100.0 | Testing Loss: 0.0012394862715154886 | Testing Acc: 100.0\n",
            "Epoch: 4370 | Training Loss: 0.0005380238289944828 | Training Acc: 100.0 | Testing Loss: 0.00123900908511132 | Testing Acc: 100.0\n",
            "Epoch: 4371 | Training Loss: 0.0005378524074330926 | Training Acc: 100.0 | Testing Loss: 0.0012389858020469546 | Testing Acc: 100.0\n",
            "Epoch: 4372 | Training Loss: 0.0005376827903091908 | Training Acc: 100.0 | Testing Loss: 0.0012385088484734297 | Testing Acc: 100.0\n",
            "Epoch: 4373 | Training Loss: 0.0005375024629756808 | Training Acc: 100.0 | Testing Loss: 0.0012380435364320874 | Testing Acc: 100.0\n",
            "Epoch: 4374 | Training Loss: 0.0005373340100049973 | Training Acc: 100.0 | Testing Loss: 0.001238031778484583 | Testing Acc: 100.0\n",
            "Epoch: 4375 | Training Loss: 0.000537166022695601 | Training Acc: 100.0 | Testing Loss: 0.0012375606456771493 | Testing Acc: 100.0\n",
            "Epoch: 4376 | Training Loss: 0.0005369900027289987 | Training Acc: 100.0 | Testing Loss: 0.0012375374790281057 | Testing Acc: 100.0\n",
            "Epoch: 4377 | Training Loss: 0.0005368191050365567 | Training Acc: 100.0 | Testing Loss: 0.0012370605254545808 | Testing Acc: 100.0\n",
            "Epoch: 4378 | Training Loss: 0.0005366461118683219 | Training Acc: 100.0 | Testing Loss: 0.0012365949805825949 | Testing Acc: 100.0\n",
            "Epoch: 4379 | Training Loss: 0.0005364835378713906 | Training Acc: 100.0 | Testing Loss: 0.0012365776346996427 | Testing Acc: 100.0\n",
            "Epoch: 4380 | Training Loss: 0.000536305014975369 | Training Acc: 100.0 | Testing Loss: 0.0012361063854768872 | Testing Acc: 100.0\n",
            "Epoch: 4381 | Training Loss: 0.0005361424991860986 | Training Acc: 100.0 | Testing Loss: 0.0012360772816464305 | Testing Acc: 100.0\n",
            "Epoch: 4382 | Training Loss: 0.0005359699716791511 | Training Acc: 100.0 | Testing Loss: 0.0012356002116575837 | Testing Acc: 100.0\n",
            "Epoch: 4383 | Training Loss: 0.0005357984919101 | Training Acc: 100.0 | Testing Loss: 0.0012355827493593097 | Testing Acc: 100.0\n",
            "Epoch: 4384 | Training Loss: 0.0005356319597922266 | Training Acc: 100.0 | Testing Loss: 0.001235094154253602 | Testing Acc: 100.0\n",
            "Epoch: 4385 | Training Loss: 0.0005354575696401298 | Training Acc: 100.0 | Testing Loss: 0.0012346403673291206 | Testing Acc: 100.0\n",
            "Epoch: 4386 | Training Loss: 0.0005352964508347213 | Training Acc: 100.0 | Testing Loss: 0.001234611147083342 | Testing Acc: 100.0\n",
            "Epoch: 4387 | Training Loss: 0.0005351269501261413 | Training Acc: 100.0 | Testing Loss: 0.0012341398978605866 | Testing Acc: 100.0\n",
            "Epoch: 4388 | Training Loss: 0.0005349569255486131 | Training Acc: 100.0 | Testing Loss: 0.0012341050896793604 | Testing Acc: 100.0\n",
            "Epoch: 4389 | Training Loss: 0.0005347828846424818 | Training Acc: 100.0 | Testing Loss: 0.0012336280196905136 | Testing Acc: 100.0\n",
            "Epoch: 4390 | Training Loss: 0.0005346100078895688 | Training Acc: 100.0 | Testing Loss: 0.0012331742327660322 | Testing Acc: 100.0\n",
            "Epoch: 4391 | Training Loss: 0.0005344487726688385 | Training Acc: 100.0 | Testing Loss: 0.0012331566540524364 | Testing Acc: 100.0\n",
            "Epoch: 4392 | Training Loss: 0.0005342734511941671 | Training Acc: 100.0 | Testing Loss: 0.001232691342011094 | Testing Acc: 100.0\n",
            "Epoch: 4393 | Training Loss: 0.0005341079086065292 | Training Acc: 100.0 | Testing Loss: 0.0012326622381806374 | Testing Acc: 100.0\n",
            "Epoch: 4394 | Training Loss: 0.0005339413182809949 | Training Acc: 100.0 | Testing Loss: 0.0012321907561272383 | Testing Acc: 100.0\n",
            "Epoch: 4395 | Training Loss: 0.0005337698385119438 | Training Acc: 100.0 | Testing Loss: 0.0012321558315306902 | Testing Acc: 100.0\n",
            "Epoch: 4396 | Training Loss: 0.0005336032481864095 | Training Acc: 100.0 | Testing Loss: 0.0012316848151385784 | Testing Acc: 100.0\n",
            "Epoch: 4397 | Training Loss: 0.0005334346788004041 | Training Acc: 100.0 | Testing Loss: 0.0012312134495005012 | Testing Acc: 100.0\n",
            "Epoch: 4398 | Training Loss: 0.0005332676810212433 | Training Acc: 100.0 | Testing Loss: 0.0012311901664361358 | Testing Acc: 100.0\n",
            "Epoch: 4399 | Training Loss: 0.0005330952117219567 | Training Acc: 100.0 | Testing Loss: 0.0012307306751608849 | Testing Acc: 100.0\n",
            "Epoch: 4400 | Training Loss: 0.0005329206469468772 | Training Acc: 100.0 | Testing Loss: 0.0012307190336287022 | Testing Acc: 100.0\n",
            "Epoch: 4401 | Training Loss: 0.0005327571416273713 | Training Acc: 100.0 | Testing Loss: 0.0012302420800551772 | Testing Acc: 100.0\n",
            "Epoch: 4402 | Training Loss: 0.0005325856618583202 | Training Acc: 100.0 | Testing Loss: 0.0012297939974814653 | Testing Acc: 100.0\n",
            "Epoch: 4403 | Training Loss: 0.0005324214580468833 | Training Acc: 100.0 | Testing Loss: 0.0012297707144171 | Testing Acc: 100.0\n",
            "Epoch: 4404 | Training Loss: 0.0005322489887475967 | Training Acc: 100.0 | Testing Loss: 0.0012292994651943445 | Testing Acc: 100.0\n",
            "Epoch: 4405 | Training Loss: 0.0005320849595591426 | Training Acc: 100.0 | Testing Loss: 0.0012292703613638878 | Testing Acc: 100.0\n",
            "Epoch: 4406 | Training Loss: 0.0005319168558344245 | Training Acc: 100.0 | Testing Loss: 0.0012287991121411324 | Testing Acc: 100.0\n",
            "Epoch: 4407 | Training Loss: 0.0005317438626661897 | Training Acc: 100.0 | Testing Loss: 0.0012287639547139406 | Testing Acc: 100.0\n",
            "Epoch: 4408 | Training Loss: 0.0005315816961228848 | Training Acc: 100.0 | Testing Loss: 0.0012282871175557375 | Testing Acc: 100.0\n",
            "Epoch: 4409 | Training Loss: 0.0005314117297530174 | Training Acc: 100.0 | Testing Loss: 0.0012278330978006124 | Testing Acc: 100.0\n",
            "Epoch: 4410 | Training Loss: 0.0005312461871653795 | Training Acc: 100.0 | Testing Loss: 0.001227809814736247 | Testing Acc: 100.0\n",
            "Epoch: 4411 | Training Loss: 0.0005310795968398452 | Training Acc: 100.0 | Testing Loss: 0.0012273502070456743 | Testing Acc: 100.0\n",
            "Epoch: 4412 | Training Loss: 0.0005309080588631332 | Training Acc: 100.0 | Testing Loss: 0.0012273151660338044 | Testing Acc: 100.0\n",
            "Epoch: 4413 | Training Loss: 0.0005307444371283054 | Training Acc: 100.0 | Testing Loss: 0.0012268379796296358 | Testing Acc: 100.0\n",
            "Epoch: 4414 | Training Loss: 0.0005305744707584381 | Training Acc: 100.0 | Testing Loss: 0.0012263900134712458 | Testing Acc: 100.0\n",
            "Epoch: 4415 | Training Loss: 0.0005304102669470012 | Training Acc: 100.0 | Testing Loss: 0.0012263667304068804 | Testing Acc: 100.0\n",
            "Epoch: 4416 | Training Loss: 0.0005302393110468984 | Training Acc: 100.0 | Testing Loss: 0.0012259072391316295 | Testing Acc: 100.0\n",
            "Epoch: 4417 | Training Loss: 0.0005300752818584442 | Training Acc: 100.0 | Testing Loss: 0.0012258897768333554 | Testing Acc: 100.0\n",
            "Epoch: 4418 | Training Loss: 0.0005299086915329099 | Training Acc: 100.0 | Testing Loss: 0.0012254300527274609 | Testing Acc: 100.0\n",
            "Epoch: 4419 | Training Loss: 0.0005297386669553816 | Training Acc: 100.0 | Testing Loss: 0.0012253893073648214 | Testing Acc: 100.0\n",
            "Epoch: 4420 | Training Loss: 0.0005295794690027833 | Training Acc: 100.0 | Testing Loss: 0.001224918058142066 | Testing Acc: 100.0\n",
            "Epoch: 4421 | Training Loss: 0.0005294094444252551 | Training Acc: 100.0 | Testing Loss: 0.0012244463432580233 | Testing Acc: 100.0\n",
            "Epoch: 4422 | Training Loss: 0.00052924535702914 | Training Acc: 100.0 | Testing Loss: 0.0012244288809597492 | Testing Acc: 100.0\n",
            "Epoch: 4423 | Training Loss: 0.0005290801636874676 | Training Acc: 100.0 | Testing Loss: 0.001223957515321672 | Testing Acc: 100.0\n",
            "Epoch: 4424 | Training Loss: 0.000528911710716784 | Training Acc: 100.0 | Testing Loss: 0.001223928527906537 | Testing Acc: 100.0\n",
            "Epoch: 4425 | Training Loss: 0.0005287465755827725 | Training Acc: 100.0 | Testing Loss: 0.0012234571622684598 | Testing Acc: 100.0\n",
            "Epoch: 4426 | Training Loss: 0.000528578064404428 | Training Acc: 100.0 | Testing Loss: 0.0012230324791744351 | Testing Acc: 100.0\n",
            "Epoch: 4427 | Training Loss: 0.0005284108920022845 | Training Acc: 100.0 | Testing Loss: 0.0012229973217472434 | Testing Acc: 100.0\n",
            "Epoch: 4428 | Training Loss: 0.0005282458732835948 | Training Acc: 100.0 | Testing Loss: 0.0012225317768752575 | Testing Acc: 100.0\n",
            "Epoch: 4429 | Training Loss: 0.0005280788172967732 | Training Acc: 100.0 | Testing Loss: 0.0012225026730448008 | Testing Acc: 100.0\n",
            "Epoch: 4430 | Training Loss: 0.0005279136239551008 | Training Acc: 100.0 | Testing Loss: 0.0012220371281728148 | Testing Acc: 100.0\n",
            "Epoch: 4431 | Training Loss: 0.0005277465679682791 | Training Acc: 100.0 | Testing Loss: 0.0012215891620144248 | Testing Acc: 100.0\n",
            "Epoch: 4432 | Training Loss: 0.0005275883595459163 | Training Acc: 100.0 | Testing Loss: 0.001221577520482242 | Testing Acc: 100.0\n",
            "Epoch: 4433 | Training Loss: 0.0005274160066619515 | Training Acc: 100.0 | Testing Loss: 0.0012211177963763475 | Testing Acc: 100.0\n",
            "Epoch: 4434 | Training Loss: 0.0005272518610581756 | Training Acc: 100.0 | Testing Loss: 0.0012210945133119822 | Testing Acc: 100.0\n",
            "Epoch: 4435 | Training Loss: 0.0005270896363072097 | Training Acc: 100.0 | Testing Loss: 0.0012206289684399962 | Testing Acc: 100.0\n",
            "Epoch: 4436 | Training Loss: 0.0005269240937195718 | Training Acc: 100.0 | Testing Loss: 0.0012205998646095395 | Testing Acc: 100.0\n",
            "Epoch: 4437 | Training Loss: 0.0005267589003778994 | Training Acc: 100.0 | Testing Loss: 0.0012201284989714622 | Testing Acc: 100.0\n",
            "Epoch: 4438 | Training Loss: 0.0005265963263809681 | Training Acc: 100.0 | Testing Loss: 0.0012196686584502459 | Testing Acc: 100.0\n",
            "Epoch: 4439 | Training Loss: 0.0005264350911602378 | Training Acc: 100.0 | Testing Loss: 0.0012196513125672936 | Testing Acc: 100.0\n",
            "Epoch: 4440 | Training Loss: 0.0005262625636532903 | Training Acc: 100.0 | Testing Loss: 0.0012191974092274904 | Testing Acc: 100.0\n",
            "Epoch: 4441 | Training Loss: 0.0005261014448478818 | Training Acc: 100.0 | Testing Loss: 0.0012191800633445382 | Testing Acc: 100.0\n",
            "Epoch: 4442 | Training Loss: 0.000525937881320715 | Training Acc: 100.0 | Testing Loss: 0.0012187202228233218 | Testing Acc: 100.0\n",
            "Epoch: 4443 | Training Loss: 0.0005257693119347095 | Training Acc: 100.0 | Testing Loss: 0.0012182664358988404 | Testing Acc: 100.0\n",
            "Epoch: 4444 | Training Loss: 0.0005256126169115305 | Training Acc: 100.0 | Testing Loss: 0.0012182489736005664 | Testing Acc: 100.0\n",
            "Epoch: 4445 | Training Loss: 0.0005254445131868124 | Training Acc: 100.0 | Testing Loss: 0.0012177892494946718 | Testing Acc: 100.0\n",
            "Epoch: 4446 | Training Loss: 0.0005252818809822202 | Training Acc: 100.0 | Testing Loss: 0.001217771670781076 | Testing Acc: 100.0\n",
            "Epoch: 4447 | Training Loss: 0.0005251167458482087 | Training Acc: 100.0 | Testing Loss: 0.0012173003051429987 | Testing Acc: 100.0\n",
            "Epoch: 4448 | Training Loss: 0.0005249541136436164 | Training Acc: 100.0 | Testing Loss: 0.001217271201312542 | Testing Acc: 100.0\n",
            "Epoch: 4449 | Training Loss: 0.0005247903754934669 | Training Acc: 100.0 | Testing Loss: 0.0012167940149083734 | Testing Acc: 100.0\n",
            "Epoch: 4450 | Training Loss: 0.0005246248911134899 | Training Acc: 100.0 | Testing Loss: 0.001216369098983705 | Testing Acc: 100.0\n",
            "Epoch: 4451 | Training Loss: 0.0005244637723080814 | Training Acc: 100.0 | Testing Loss: 0.0012163399951532483 | Testing Acc: 100.0\n",
            "Epoch: 4452 | Training Loss: 0.000524298578966409 | Training Acc: 100.0 | Testing Loss: 0.0012158920289948583 | Testing Acc: 100.0\n",
            "Epoch: 4453 | Training Loss: 0.0005241284961812198 | Training Acc: 100.0 | Testing Loss: 0.0012158512836322188 | Testing Acc: 100.0\n",
            "Epoch: 4454 | Training Loss: 0.0005239693564362824 | Training Acc: 100.0 | Testing Loss: 0.0012153914431110024 | Testing Acc: 100.0\n",
            "Epoch: 4455 | Training Loss: 0.0005238036974333227 | Training Acc: 100.0 | Testing Loss: 0.0012149432441219687 | Testing Acc: 100.0\n",
            "Epoch: 4456 | Training Loss: 0.0005236469442024827 | Training Acc: 100.0 | Testing Loss: 0.0012149258982390165 | Testing Acc: 100.0\n",
            "Epoch: 4457 | Training Loss: 0.0005234818672761321 | Training Acc: 100.0 | Testing Loss: 0.0012144719948992133 | Testing Acc: 100.0\n",
            "Epoch: 4458 | Training Loss: 0.0005233132978901267 | Training Acc: 100.0 | Testing Loss: 0.001214448595419526 | Testing Acc: 100.0\n",
            "Epoch: 4459 | Training Loss: 0.0005231555551290512 | Training Acc: 100.0 | Testing Loss: 0.0012139946920797229 | Testing Acc: 100.0\n",
            "Epoch: 4460 | Training Loss: 0.0005229855887591839 | Training Acc: 100.0 | Testing Loss: 0.0012139597674831748 | Testing Acc: 100.0\n",
            "Epoch: 4461 | Training Loss: 0.0005228307563811541 | Training Acc: 100.0 | Testing Loss: 0.0012135000433772802 | Testing Acc: 100.0\n",
            "Epoch: 4462 | Training Loss: 0.0005226637003943324 | Training Acc: 100.0 | Testing Loss: 0.0012130399700254202 | Testing Acc: 100.0\n",
            "Epoch: 4463 | Training Loss: 0.0005225054919719696 | Training Acc: 100.0 | Testing Loss: 0.0012130166869610548 | Testing Acc: 100.0\n",
            "Epoch: 4464 | Training Loss: 0.0005223402986302972 | Training Acc: 100.0 | Testing Loss: 0.0012125629000365734 | Testing Acc: 100.0\n",
            "Epoch: 4465 | Training Loss: 0.0005221717292442918 | Training Acc: 100.0 | Testing Loss: 0.001212539616972208 | Testing Acc: 100.0\n",
            "Epoch: 4466 | Training Loss: 0.000522016896866262 | Training Acc: 100.0 | Testing Loss: 0.0012120738392695785 | Testing Acc: 100.0\n",
            "Epoch: 4467 | Training Loss: 0.0005218483274802566 | Training Acc: 100.0 | Testing Loss: 0.001211649039760232 | Testing Acc: 100.0\n",
            "Epoch: 4468 | Training Loss: 0.0005216872086748481 | Training Acc: 100.0 | Testing Loss: 0.0012116199359297752 | Testing Acc: 100.0\n",
            "Epoch: 4469 | Training Loss: 0.0005215250421315432 | Training Acc: 100.0 | Testing Loss: 0.0012111718533560634 | Testing Acc: 100.0\n",
            "Epoch: 4470 | Training Loss: 0.0005213639233261347 | Training Acc: 100.0 | Testing Loss: 0.0012111426331102848 | Testing Acc: 100.0\n",
            "Epoch: 4471 | Training Loss: 0.0005212046089582145 | Training Acc: 100.0 | Testing Loss: 0.0012106888461858034 | Testing Acc: 100.0\n",
            "Epoch: 4472 | Training Loss: 0.0005210375529713929 | Training Acc: 100.0 | Testing Loss: 0.0012102405307814479 | Testing Acc: 100.0\n",
            "Epoch: 4473 | Training Loss: 0.0005208808579482138 | Training Acc: 100.0 | Testing Loss: 0.0012102113105356693 | Testing Acc: 100.0\n",
            "Epoch: 4474 | Training Loss: 0.0005207156063988805 | Training Acc: 100.0 | Testing Loss: 0.0012097631115466356 | Testing Acc: 100.0\n",
            "Epoch: 4475 | Training Loss: 0.0005205559427849948 | Training Acc: 100.0 | Testing Loss: 0.001209751470014453 | Testing Acc: 100.0\n",
            "Epoch: 4476 | Training Loss: 0.0005203952314332128 | Training Acc: 100.0 | Testing Loss: 0.001209303387440741 | Testing Acc: 100.0\n",
            "Epoch: 4477 | Training Loss: 0.0005202327156439424 | Training Acc: 100.0 | Testing Loss: 0.0012092625256627798 | Testing Acc: 100.0\n",
            "Epoch: 4478 | Training Loss: 0.000520074914675206 | Training Acc: 100.0 | Testing Loss: 0.0012088026851415634 | Testing Acc: 100.0\n",
            "Epoch: 4479 | Training Loss: 0.0005199093138799071 | Training Acc: 100.0 | Testing Loss: 0.0012083661276847124 | Testing Acc: 100.0\n",
            "Epoch: 4480 | Training Loss: 0.0005197510472498834 | Training Acc: 100.0 | Testing Loss: 0.001208342844620347 | Testing Acc: 100.0\n",
            "Epoch: 4481 | Training Loss: 0.0005195859121158719 | Training Acc: 100.0 | Testing Loss: 0.001207894878461957 | Testing Acc: 100.0\n",
            "Epoch: 4482 | Training Loss: 0.0005194262485019863 | Training Acc: 100.0 | Testing Loss: 0.0012078715953975916 | Testing Acc: 100.0\n",
            "Epoch: 4483 | Training Loss: 0.0005192683893255889 | Training Acc: 100.0 | Testing Loss: 0.0012074175756424665 | Testing Acc: 100.0\n",
            "Epoch: 4484 | Training Loss: 0.0005191027885302901 | Training Acc: 100.0 | Testing Loss: 0.0012069868389517069 | Testing Acc: 100.0\n",
            "Epoch: 4485 | Training Loss: 0.0005189446965232491 | Training Acc: 100.0 | Testing Loss: 0.0012069693766534328 | Testing Acc: 100.0\n",
            "Epoch: 4486 | Training Loss: 0.0005187795031815767 | Training Acc: 100.0 | Testing Loss: 0.0012065211776643991 | Testing Acc: 100.0\n",
            "Epoch: 4487 | Training Loss: 0.0005186242633499205 | Training Acc: 100.0 | Testing Loss: 0.0012064918410032988 | Testing Acc: 100.0\n",
            "Epoch: 4488 | Training Loss: 0.0005184620385989547 | Training Acc: 100.0 | Testing Loss: 0.0012060380540788174 | Testing Acc: 100.0\n",
            "Epoch: 4489 | Training Loss: 0.0005182994063943624 | Training Acc: 100.0 | Testing Loss: 0.0012060089502483606 | Testing Acc: 100.0\n",
            "Epoch: 4490 | Training Loss: 0.0005181430606171489 | Training Acc: 100.0 | Testing Loss: 0.0012055549304932356 | Testing Acc: 100.0\n",
            "Epoch: 4491 | Training Loss: 0.0005179774016141891 | Training Acc: 100.0 | Testing Loss: 0.001205118140205741 | Testing Acc: 100.0\n",
            "Epoch: 4492 | Training Loss: 0.0005178252467885613 | Training Acc: 100.0 | Testing Loss: 0.0012050889199599624 | Testing Acc: 100.0\n",
            "Epoch: 4493 | Training Loss: 0.0005176629638299346 | Training Acc: 100.0 | Testing Loss: 0.0012046408373862505 | Testing Acc: 100.0\n",
            "Epoch: 4494 | Training Loss: 0.0005174974212422967 | Training Acc: 100.0 | Testing Loss: 0.0012046117335557938 | Testing Acc: 100.0\n",
            "Epoch: 4495 | Training Loss: 0.0005173454992473125 | Training Acc: 100.0 | Testing Loss: 0.0012041577138006687 | Testing Acc: 100.0\n",
            "Epoch: 4496 | Training Loss: 0.0005171784432604909 | Training Acc: 100.0 | Testing Loss: 0.0012037211563438177 | Testing Acc: 100.0\n",
            "Epoch: 4497 | Training Loss: 0.0005170187214389443 | Training Acc: 100.0 | Testing Loss: 0.0012036978732794523 | Testing Acc: 100.0\n",
            "Epoch: 4498 | Training Loss: 0.0005168580682948232 | Training Acc: 100.0 | Testing Loss: 0.0012032614322379231 | Testing Acc: 100.0\n",
            "Epoch: 4499 | Training Loss: 0.0005166997434571385 | Training Acc: 100.0 | Testing Loss: 0.001203238032758236 | Testing Acc: 100.0\n",
            "Epoch: 4500 | Training Loss: 0.0005165435140952468 | Training Acc: 100.0 | Testing Loss: 0.0012027956545352936 | Testing Acc: 100.0\n",
            "Epoch: 4501 | Training Loss: 0.0005163764581084251 | Training Acc: 100.0 | Testing Loss: 0.0012023592134937644 | Testing Acc: 100.0\n",
            "Epoch: 4502 | Training Loss: 0.0005162211018614471 | Training Acc: 100.0 | Testing Loss: 0.0012023358140140772 | Testing Acc: 100.0\n",
            "Epoch: 4503 | Training Loss: 0.0005160633008927107 | Training Acc: 100.0 | Testing Loss: 0.001201893319375813 | Testing Acc: 100.0\n",
            "Epoch: 4504 | Training Loss: 0.0005159021238796413 | Training Acc: 100.0 | Testing Loss: 0.001201858394779265 | Testing Acc: 100.0\n",
            "Epoch: 4505 | Training Loss: 0.000515742867719382 | Training Acc: 100.0 | Testing Loss: 0.0012014159001410007 | Testing Acc: 100.0\n",
            "Epoch: 4506 | Training Loss: 0.0005155816907063127 | Training Acc: 100.0 | Testing Loss: 0.0012013866798952222 | Testing Acc: 100.0\n",
            "Epoch: 4507 | Training Loss: 0.0005154282553121448 | Training Acc: 100.0 | Testing Loss: 0.0012009325437247753 | Testing Acc: 100.0\n",
            "Epoch: 4508 | Training Loss: 0.0005152641679160297 | Training Acc: 100.0 | Testing Loss: 0.001200507627800107 | Testing Acc: 100.0\n",
            "Epoch: 4509 | Training Loss: 0.0005151090444996953 | Training Acc: 100.0 | Testing Loss: 0.0012004667660221457 | Testing Acc: 100.0\n",
            "Epoch: 4510 | Training Loss: 0.0005149497301317751 | Training Acc: 100.0 | Testing Loss: 0.001200036145746708 | Testing Acc: 100.0\n",
            "Epoch: 4511 | Training Loss: 0.0005147914635017514 | Training Acc: 100.0 | Testing Loss: 0.0012000070419162512 | Testing Acc: 100.0\n",
            "Epoch: 4512 | Training Loss: 0.0005146380863152444 | Training Acc: 100.0 | Testing Loss: 0.0011995527893304825 | Testing Acc: 100.0\n",
            "Epoch: 4513 | Training Loss: 0.0005144710303284228 | Training Acc: 100.0 | Testing Loss: 0.0011991162318736315 | Testing Acc: 100.0\n",
            "Epoch: 4514 | Training Loss: 0.000514320214278996 | Training Acc: 100.0 | Testing Loss: 0.001199092948809266 | Testing Acc: 100.0\n",
            "Epoch: 4515 | Training Loss: 0.0005141580477356911 | Training Acc: 100.0 | Testing Loss: 0.0011986622121185064 | Testing Acc: 100.0\n",
            "Epoch: 4516 | Training Loss: 0.0005140071734786034 | Training Acc: 100.0 | Testing Loss: 0.0011986388126388192 | Testing Acc: 100.0\n",
            "Epoch: 4517 | Training Loss: 0.0005138434935361147 | Training Acc: 100.0 | Testing Loss: 0.0011981906136497855 | Testing Acc: 100.0\n",
            "Epoch: 4518 | Training Loss: 0.0005136853433214128 | Training Acc: 100.0 | Testing Loss: 0.0011981673305854201 | Testing Acc: 100.0\n",
            "Epoch: 4519 | Training Loss: 0.0005135303945280612 | Training Acc: 100.0 | Testing Loss: 0.001197713310830295 | Testing Acc: 100.0\n",
            "Epoch: 4520 | Training Loss: 0.0005133677623234689 | Training Acc: 100.0 | Testing Loss: 0.001197270816192031 | Testing Acc: 100.0\n",
            "Epoch: 4521 | Training Loss: 0.0005132124642841518 | Training Acc: 100.0 | Testing Loss: 0.0011972475331276655 | Testing Acc: 100.0\n",
            "Epoch: 4522 | Training Loss: 0.0005130532081238925 | Training Acc: 100.0 | Testing Loss: 0.0011968108592554927 | Testing Acc: 100.0\n",
            "Epoch: 4523 | Training Loss: 0.0005128964548930526 | Training Acc: 100.0 | Testing Loss: 0.0011967818718403578 | Testing Acc: 100.0\n",
            "Epoch: 4524 | Training Loss: 0.0005127415643073618 | Training Acc: 100.0 | Testing Loss: 0.0011963334400206804 | Testing Acc: 100.0\n",
            "Epoch: 4525 | Training Loss: 0.00051258341409266 | Training Acc: 100.0 | Testing Loss: 0.0011959143448621035 | Testing Acc: 100.0\n",
            "Epoch: 4526 | Training Loss: 0.0005124237504787743 | Training Acc: 100.0 | Testing Loss: 0.0011958909453824162 | Testing Acc: 100.0\n",
            "Epoch: 4527 | Training Loss: 0.0005122659495100379 | Training Acc: 100.0 | Testing Loss: 0.001195448450744152 | Testing Acc: 100.0\n",
            "Epoch: 4528 | Training Loss: 0.0005121061694808304 | Training Acc: 100.0 | Testing Loss: 0.0011954193469136953 | Testing Acc: 100.0\n",
            "Epoch: 4529 | Training Loss: 0.0005119513371028006 | Training Acc: 100.0 | Testing Loss: 0.001194970915094018 | Testing Acc: 100.0\n",
            "Epoch: 4530 | Training Loss: 0.0005117886466905475 | Training Acc: 100.0 | Testing Loss: 0.0011945461155846715 | Testing Acc: 100.0\n",
            "Epoch: 4531 | Training Loss: 0.0005116363754495978 | Training Acc: 100.0 | Testing Loss: 0.0011945285368710756 | Testing Acc: 100.0\n",
            "Epoch: 4532 | Training Loss: 0.0005114799714647233 | Training Acc: 100.0 | Testing Loss: 0.001194097800180316 | Testing Acc: 100.0\n",
            "Epoch: 4533 | Training Loss: 0.000511324789840728 | Training Acc: 100.0 | Testing Loss: 0.0011940744007006288 | Testing Acc: 100.0\n",
            "Epoch: 4534 | Training Loss: 0.0005111669888719916 | Training Acc: 100.0 | Testing Loss: 0.001193637726828456 | Testing Acc: 100.0\n",
            "Epoch: 4535 | Training Loss: 0.0005110102356411517 | Training Acc: 100.0 | Testing Loss: 0.0011935967486351728 | Testing Acc: 100.0\n",
            "Epoch: 4536 | Training Loss: 0.0005108568584546447 | Training Acc: 100.0 | Testing Loss: 0.0011931543704122305 | Testing Acc: 100.0\n",
            "Epoch: 4537 | Training Loss: 0.0005106971366330981 | Training Acc: 100.0 | Testing Loss: 0.001192723517306149 | Testing Acc: 100.0\n",
            "Epoch: 4538 | Training Loss: 0.0005105478339828551 | Training Acc: 100.0 | Testing Loss: 0.0011926944134756923 | Testing Acc: 100.0\n",
            "Epoch: 4539 | Training Loss: 0.0005103856092318892 | Training Acc: 100.0 | Testing Loss: 0.0011922636767849326 | Testing Acc: 100.0\n",
            "Epoch: 4540 | Training Loss: 0.0005102302529849112 | Training Acc: 100.0 | Testing Loss: 0.0011922462144866586 | Testing Acc: 100.0\n",
            "Epoch: 4541 | Training Loss: 0.0005100754206068814 | Training Acc: 100.0 | Testing Loss: 0.0011918036034330726 | Testing Acc: 100.0\n",
            "Epoch: 4542 | Training Loss: 0.0005099172121845186 | Training Acc: 100.0 | Testing Loss: 0.0011913786875084043 | Testing Acc: 100.0\n",
            "Epoch: 4543 | Training Loss: 0.0005097663961350918 | Training Acc: 100.0 | Testing Loss: 0.0011913612252101302 | Testing Acc: 100.0\n",
            "Epoch: 4544 | Training Loss: 0.0005096070235595107 | Training Acc: 100.0 | Testing Loss: 0.001190918730571866 | Testing Acc: 100.0\n",
            "Epoch: 4545 | Training Loss: 0.0005094503285363317 | Training Acc: 100.0 | Testing Loss: 0.0011908954475075006 | Testing Acc: 100.0\n",
            "Epoch: 4546 | Training Loss: 0.0005092984647490084 | Training Acc: 100.0 | Testing Loss: 0.0011904468992725015 | Testing Acc: 100.0\n",
            "Epoch: 4547 | Training Loss: 0.0005091387429274619 | Training Acc: 100.0 | Testing Loss: 0.0011904060374945402 | Testing Acc: 100.0\n",
            "Epoch: 4548 | Training Loss: 0.0005089867627248168 | Training Acc: 100.0 | Testing Loss: 0.0011899693636223674 | Testing Acc: 100.0\n",
            "Epoch: 4549 | Training Loss: 0.0005088300677016377 | Training Acc: 100.0 | Testing Loss: 0.0011895442148670554 | Testing Acc: 100.0\n",
            "Epoch: 4550 | Training Loss: 0.0005086703458800912 | Training Acc: 100.0 | Testing Loss: 0.0011895149946212769 | Testing Acc: 100.0\n",
            "Epoch: 4551 | Training Loss: 0.0005085199372842908 | Training Acc: 100.0 | Testing Loss: 0.0011890842579305172 | Testing Acc: 100.0\n",
            "Epoch: 4552 | Training Loss: 0.0005083660944364965 | Training Acc: 100.0 | Testing Loss: 0.0011890609748661518 | Testing Acc: 100.0\n",
            "Epoch: 4553 | Training Loss: 0.0005082112620584667 | Training Acc: 100.0 | Testing Loss: 0.0011886184802278876 | Testing Acc: 100.0\n",
            "Epoch: 4554 | Training Loss: 0.0005080530536361039 | Training Acc: 100.0 | Testing Loss: 0.0011881876271218061 | Testing Acc: 100.0\n",
            "Epoch: 4555 | Training Loss: 0.0005079052643850446 | Training Acc: 100.0 | Testing Loss: 0.0011881700484082103 | Testing Acc: 100.0\n",
            "Epoch: 4556 | Training Loss: 0.0005077474052086473 | Training Acc: 100.0 | Testing Loss: 0.001187745132483542 | Testing Acc: 100.0\n",
            "Epoch: 4557 | Training Loss: 0.0005075921071693301 | Training Acc: 100.0 | Testing Loss: 0.001187727670185268 | Testing Acc: 100.0\n",
            "Epoch: 4558 | Training Loss: 0.0005074416985735297 | Training Acc: 100.0 | Testing Loss: 0.001187290996313095 | Testing Acc: 100.0\n",
            "Epoch: 4559 | Training Loss: 0.0005072790081612766 | Training Acc: 100.0 | Testing Loss: 0.001186865963973105 | Testing Acc: 100.0\n",
            "Epoch: 4560 | Training Loss: 0.0005071295890957117 | Training Acc: 100.0 | Testing Loss: 0.001186848385259509 | Testing Acc: 100.0\n",
            "Epoch: 4561 | Training Loss: 0.0005069732433184981 | Training Acc: 100.0 | Testing Loss: 0.0011864234693348408 | Testing Acc: 100.0\n",
            "Epoch: 4562 | Training Loss: 0.0005068180034868419 | Training Acc: 100.0 | Testing Loss: 0.001186394365504384 | Testing Acc: 100.0\n",
            "Epoch: 4563 | Training Loss: 0.0005066675366833806 | Training Acc: 100.0 | Testing Loss: 0.0011859575752168894 | Testing Acc: 100.0\n",
            "Epoch: 4564 | Training Loss: 0.0005065107834525406 | Training Acc: 100.0 | Testing Loss: 0.0011859165970236063 | Testing Acc: 100.0\n",
            "Epoch: 4565 | Training Loss: 0.0005063588032498956 | Training Acc: 100.0 | Testing Loss: 0.0011854798067361116 | Testing Acc: 100.0\n",
            "Epoch: 4566 | Training Loss: 0.0005061991396360099 | Training Acc: 100.0 | Testing Loss: 0.0011850723531097174 | Testing Acc: 100.0\n",
            "Epoch: 4567 | Training Loss: 0.0005060468683950603 | Training Acc: 100.0 | Testing Loss: 0.0011850373120978475 | Testing Acc: 100.0\n",
            "Epoch: 4568 | Training Loss: 0.000505896401591599 | Training Acc: 100.0 | Testing Loss: 0.0011846002889797091 | Testing Acc: 100.0\n",
            "Epoch: 4569 | Training Loss: 0.0005057366797700524 | Training Acc: 100.0 | Testing Loss: 0.001184565364383161 | Testing Acc: 100.0\n",
            "Epoch: 4570 | Training Loss: 0.0005055846995674074 | Training Acc: 100.0 | Testing Loss: 0.0011841345112770796 | Testing Acc: 100.0\n",
            "Epoch: 4571 | Training Loss: 0.0005054265493527055 | Training Acc: 100.0 | Testing Loss: 0.0011837093625217676 | Testing Acc: 100.0\n",
            "Epoch: 4572 | Training Loss: 0.0005052814958617091 | Training Acc: 100.0 | Testing Loss: 0.0011836860794574022 | Testing Acc: 100.0\n",
            "Epoch: 4573 | Training Loss: 0.0005051267216913402 | Training Acc: 100.0 | Testing Loss: 0.001183261163532734 | Testing Acc: 100.0\n",
            "Epoch: 4574 | Training Loss: 0.0005049714236520231 | Training Acc: 100.0 | Testing Loss: 0.0011832494055852294 | Testing Acc: 100.0\n",
            "Epoch: 4575 | Training Loss: 0.0005048225284554064 | Training Acc: 100.0 | Testing Loss: 0.0011828128481283784 | Testing Acc: 100.0\n",
            "Epoch: 4576 | Training Loss: 0.0005046642618253827 | Training Acc: 100.0 | Testing Loss: 0.0011827776907011867 | Testing Acc: 100.0\n",
            "Epoch: 4577 | Training Loss: 0.0005045182188041508 | Training Acc: 100.0 | Testing Loss: 0.0011823410168290138 | Testing Acc: 100.0\n",
            "Epoch: 4578 | Training Loss: 0.000504360010381788 | Training Acc: 100.0 | Testing Loss: 0.001181921805255115 | Testing Acc: 100.0\n",
            "Epoch: 4579 | Training Loss: 0.0005042062257416546 | Training Acc: 100.0 | Testing Loss: 0.0011819042265415192 | Testing Acc: 100.0\n",
            "Epoch: 4580 | Training Loss: 0.0005040527321398258 | Training Acc: 100.0 | Testing Loss: 0.0011814674362540245 | Testing Acc: 100.0\n",
            "Epoch: 4581 | Training Loss: 0.000503894523717463 | Training Acc: 100.0 | Testing Loss: 0.0011814325116574764 | Testing Acc: 100.0\n",
            "Epoch: 4582 | Training Loss: 0.0005037469672970474 | Training Acc: 100.0 | Testing Loss: 0.0011810015421360731 | Testing Acc: 100.0\n",
            "Epoch: 4583 | Training Loss: 0.0005035872454755008 | Training Acc: 100.0 | Testing Loss: 0.001180593972094357 | Testing Acc: 100.0\n",
            "Epoch: 4584 | Training Loss: 0.00050344382179901 | Training Acc: 100.0 | Testing Loss: 0.0011805763933807611 | Testing Acc: 100.0\n",
            "Epoch: 4585 | Training Loss: 0.0005032860208302736 | Training Acc: 100.0 | Testing Loss: 0.0011801396030932665 | Testing Acc: 100.0\n",
            "Epoch: 4586 | Training Loss: 0.0005031351465731859 | Training Acc: 100.0 | Testing Loss: 0.001180116320028901 | Testing Acc: 100.0\n",
            "Epoch: 4587 | Training Loss: 0.0005029847379773855 | Training Acc: 100.0 | Testing Loss: 0.0011796853505074978 | Testing Acc: 100.0\n",
            "Epoch: 4588 | Training Loss: 0.0005028279265388846 | Training Acc: 100.0 | Testing Loss: 0.0011796504259109497 | Testing Acc: 100.0\n",
            "Epoch: 4589 | Training Loss: 0.0005026789149269462 | Training Acc: 100.0 | Testing Loss: 0.0011792017612606287 | Testing Acc: 100.0\n",
            "Epoch: 4590 | Training Loss: 0.0005025280988775194 | Training Acc: 100.0 | Testing Loss: 0.001178800011985004 | Testing Acc: 100.0\n",
            "Epoch: 4591 | Training Loss: 0.0005023772828280926 | Training Acc: 100.0 | Testing Loss: 0.0011787767289206386 | Testing Acc: 100.0\n",
            "Epoch: 4592 | Training Loss: 0.0005022253608331084 | Training Acc: 100.0 | Testing Loss: 0.0011783516965806484 | Testing Acc: 100.0\n",
            "Epoch: 4593 | Training Loss: 0.0005020744865760207 | Training Acc: 100.0 | Testing Loss: 0.0011783225927501917 | Testing Acc: 100.0\n",
            "Epoch: 4594 | Training Loss: 0.0005019254749640822 | Training Acc: 100.0 | Testing Loss: 0.001177879748865962 | Testing Acc: 100.0\n",
            "Epoch: 4595 | Training Loss: 0.0005017673247493804 | Training Acc: 100.0 | Testing Loss: 0.0011774662416428328 | Testing Acc: 100.0\n",
            "Epoch: 4596 | Training Loss: 0.0005016223294660449 | Training Acc: 100.0 | Testing Loss: 0.0011774429585784674 | Testing Acc: 100.0\n",
            "Epoch: 4597 | Training Loss: 0.0005014704074710608 | Training Acc: 100.0 | Testing Loss: 0.0011770178098231554 | Testing Acc: 100.0\n",
            "Epoch: 4598 | Training Loss: 0.0005013135960325599 | Training Acc: 100.0 | Testing Loss: 0.0011770003475248814 | Testing Acc: 100.0\n",
            "Epoch: 4599 | Training Loss: 0.0005011647008359432 | Training Acc: 100.0 | Testing Loss: 0.001176569378003478 | Testing Acc: 100.0\n",
            "Epoch: 4600 | Training Loss: 0.000501009461004287 | Training Acc: 100.0 | Testing Loss: 0.0011761499335989356 | Testing Acc: 100.0\n",
            "Epoch: 4601 | Training Loss: 0.0005008644075132906 | Training Acc: 100.0 | Testing Loss: 0.0011761324713006616 | Testing Acc: 100.0\n",
            "Epoch: 4602 | Training Loss: 0.0005007050931453705 | Training Acc: 100.0 | Testing Loss: 0.0011757012689486146 | Testing Acc: 100.0\n",
            "Epoch: 4603 | Training Loss: 0.000500560156069696 | Training Acc: 100.0 | Testing Loss: 0.0011756663443520665 | Testing Acc: 100.0\n",
            "Epoch: 4604 | Training Loss: 0.0005004097474738955 | Training Acc: 100.0 | Testing Loss: 0.001175252953544259 | Testing Acc: 100.0\n",
            "Epoch: 4605 | Training Loss: 0.0005002573598176241 | Training Acc: 100.0 | Testing Loss: 0.001175218028947711 | Testing Acc: 100.0\n",
            "Epoch: 4606 | Training Loss: 0.0005001113167963922 | Training Acc: 100.0 | Testing Loss: 0.0011747811222448945 | Testing Acc: 100.0\n",
            "Epoch: 4607 | Training Loss: 0.0004999589873477817 | Training Acc: 100.0 | Testing Loss: 0.0011743734357878566 | Testing Acc: 100.0\n",
            "Epoch: 4608 | Training Loss: 0.0004998081712983549 | Training Acc: 100.0 | Testing Loss: 0.0011743501527234912 | Testing Acc: 100.0\n",
            "Epoch: 4609 | Training Loss: 0.0004996577044948936 | Training Acc: 100.0 | Testing Loss: 0.0011739307083189487 | Testing Acc: 100.0\n",
            "Epoch: 4610 | Training Loss: 0.0004995038616470993 | Training Acc: 100.0 | Testing Loss: 0.0011738957837224007 | Testing Acc: 100.0\n",
            "Epoch: 4611 | Training Loss: 0.0004993623006157577 | Training Acc: 100.0 | Testing Loss: 0.0011734587606042624 | Testing Acc: 100.0\n",
            "Epoch: 4612 | Training Loss: 0.0004992055473849177 | Training Acc: 100.0 | Testing Loss: 0.001173045253381133 | Testing Acc: 100.0\n",
            "Epoch: 4613 | Training Loss: 0.0004990605521015823 | Training Acc: 100.0 | Testing Loss: 0.0011730279074981809 | Testing Acc: 100.0\n",
            "Epoch: 4614 | Training Loss: 0.0004989071749150753 | Training Acc: 100.0 | Testing Loss: 0.0011726085795089602 | Testing Acc: 100.0\n",
            "Epoch: 4615 | Training Loss: 0.0004987592692486942 | Training Acc: 100.0 | Testing Loss: 0.0011725911172106862 | Testing Acc: 100.0\n",
            "Epoch: 4616 | Training Loss: 0.0004986118292436004 | Training Acc: 100.0 | Testing Loss: 0.0011721716728061438 | Testing Acc: 100.0\n",
            "Epoch: 4617 | Training Loss: 0.0004984579863958061 | Training Acc: 100.0 | Testing Loss: 0.0011717581655830145 | Testing Acc: 100.0\n",
            "Epoch: 4618 | Training Loss: 0.000498310080729425 | Training Acc: 100.0 | Testing Loss: 0.001171734882518649 | Testing Acc: 100.0\n",
            "Epoch: 4619 | Training Loss: 0.000498164095915854 | Training Acc: 100.0 | Testing Loss: 0.0011713213752955198 | Testing Acc: 100.0\n",
            "Epoch: 4620 | Training Loss: 0.0004980101948603988 | Training Acc: 100.0 | Testing Loss: 0.0011712921550497413 | Testing Acc: 100.0\n",
            "Epoch: 4621 | Training Loss: 0.0004978597280569375 | Training Acc: 100.0 | Testing Loss: 0.0011708668898791075 | Testing Acc: 100.0\n",
            "Epoch: 4622 | Training Loss: 0.0004977073986083269 | Training Acc: 100.0 | Testing Loss: 0.0011708319652825594 | Testing Acc: 100.0\n",
            "Epoch: 4623 | Training Loss: 0.0004975597839802504 | Training Acc: 100.0 | Testing Loss: 0.0011704068165272474 | Testing Acc: 100.0\n",
            "Epoch: 4624 | Training Loss: 0.000497405999340117 | Training Acc: 100.0 | Testing Loss: 0.0011699988972395658 | Testing Acc: 100.0\n",
            "Epoch: 4625 | Training Loss: 0.0004972580936737359 | Training Acc: 100.0 | Testing Loss: 0.0011699697934091091 | Testing Acc: 100.0\n",
            "Epoch: 4626 | Training Loss: 0.0004971105954609811 | Training Acc: 100.0 | Testing Loss: 0.0011695504654198885 | Testing Acc: 100.0\n",
            "Epoch: 4627 | Training Loss: 0.0004969597794115543 | Training Acc: 100.0 | Testing Loss: 0.0011695271823555231 | Testing Acc: 100.0\n",
            "Epoch: 4628 | Training Loss: 0.0004968136781826615 | Training Acc: 100.0 | Testing Loss: 0.0011691078543663025 | Testing Acc: 100.0\n",
            "Epoch: 4629 | Training Loss: 0.0004966598353348672 | Training Acc: 100.0 | Testing Loss: 0.0011686942307278514 | Testing Acc: 100.0\n",
            "Epoch: 4630 | Training Loss: 0.000496513384860009 | Training Acc: 100.0 | Testing Loss: 0.0011686651268973947 | Testing Acc: 100.0\n",
            "Epoch: 4631 | Training Loss: 0.0004963614628650248 | Training Acc: 100.0 | Testing Loss: 0.0011682456824928522 | Testing Acc: 100.0\n",
            "Epoch: 4632 | Training Loss: 0.0004962121020071208 | Training Acc: 100.0 | Testing Loss: 0.0011682340409606695 | Testing Acc: 100.0\n",
            "Epoch: 4633 | Training Loss: 0.0004960675723850727 | Training Acc: 100.0 | Testing Loss: 0.0011678028386086226 | Testing Acc: 100.0\n",
            "Epoch: 4634 | Training Loss: 0.000495916698127985 | Training Acc: 100.0 | Testing Loss: 0.0011677679140120745 | Testing Acc: 100.0\n",
            "Epoch: 4635 | Training Loss: 0.0004957750788889825 | Training Acc: 100.0 | Testing Loss: 0.001167348469607532 | Testing Acc: 100.0\n",
            "Epoch: 4636 | Training Loss: 0.0004956212942488492 | Training Acc: 100.0 | Testing Loss: 0.0011669292580336332 | Testing Acc: 100.0\n",
            "Epoch: 4637 | Training Loss: 0.0004954793257638812 | Training Acc: 100.0 | Testing Loss: 0.0011669116793200374 | Testing Acc: 100.0\n",
            "Epoch: 4638 | Training Loss: 0.0004953258321620524 | Training Acc: 100.0 | Testing Loss: 0.0011664923513308167 | Testing Acc: 100.0\n",
            "Epoch: 4639 | Training Loss: 0.000495179439894855 | Training Acc: 100.0 | Testing Loss: 0.0011664747726172209 | Testing Acc: 100.0\n",
            "Epoch: 4640 | Training Loss: 0.000495033455081284 | Training Acc: 100.0 | Testing Loss: 0.001166043570265174 | Testing Acc: 100.0\n",
            "Epoch: 4641 | Training Loss: 0.0004948810674250126 | Training Acc: 100.0 | Testing Loss: 0.0011656417045742273 | Testing Acc: 100.0\n",
            "Epoch: 4642 | Training Loss: 0.0004947331035509706 | Training Acc: 100.0 | Testing Loss: 0.0011656124843284488 | Testing Acc: 100.0\n",
            "Epoch: 4643 | Training Loss: 0.0004945810651406646 | Training Acc: 100.0 | Testing Loss: 0.0011651989771053195 | Testing Acc: 100.0\n",
            "Epoch: 4644 | Training Loss: 0.000494436186272651 | Training Acc: 100.0 | Testing Loss: 0.0011651639360934496 | Testing Acc: 100.0\n",
            "Epoch: 4645 | Training Loss: 0.0004942901432514191 | Training Acc: 100.0 | Testing Loss: 0.0011647504288703203 | Testing Acc: 100.0\n",
            "Epoch: 4646 | Training Loss: 0.000494142179377377 | Training Acc: 100.0 | Testing Loss: 0.0011643426259979606 | Testing Acc: 100.0\n",
            "Epoch: 4647 | Training Loss: 0.0004940016660839319 | Training Acc: 100.0 | Testing Loss: 0.0011643368052318692 | Testing Acc: 100.0\n",
            "Epoch: 4648 | Training Loss: 0.0004938453203067183 | Training Acc: 100.0 | Testing Loss: 0.00116392329800874 | Testing Acc: 100.0\n",
            "Epoch: 4649 | Training Loss: 0.0004936974146403372 | Training Acc: 100.0 | Testing Loss: 0.0011639000149443746 | Testing Acc: 100.0\n",
            "Epoch: 4650 | Training Loss: 0.0004935513134114444 | Training Acc: 100.0 | Testing Loss: 0.0011634805705398321 | Testing Acc: 100.0\n",
            "Epoch: 4651 | Training Loss: 0.0004934034077450633 | Training Acc: 100.0 | Testing Loss: 0.0011634455295279622 | Testing Acc: 100.0\n",
            "Epoch: 4652 | Training Loss: 0.0004932588199153543 | Training Acc: 100.0 | Testing Loss: 0.0011630262015387416 | Testing Acc: 100.0\n",
            "Epoch: 4653 | Training Loss: 0.0004931095172651112 | Training Acc: 100.0 | Testing Loss: 0.0011626185150817037 | Testing Acc: 100.0\n",
            "Epoch: 4654 | Training Loss: 0.0004929629503749311 | Training Acc: 100.0 | Testing Loss: 0.0011625951156020164 | Testing Acc: 100.0\n",
            "Epoch: 4655 | Training Loss: 0.0004928169655613601 | Training Acc: 100.0 | Testing Loss: 0.0011621697340160608 | Testing Acc: 100.0\n",
            "Epoch: 4656 | Training Loss: 0.0004926705732941628 | Training Acc: 100.0 | Testing Loss: 0.001162152155302465 | Testing Acc: 100.0\n",
            "Epoch: 4657 | Training Loss: 0.0004925259854644537 | Training Acc: 100.0 | Testing Loss: 0.0011617327108979225 | Testing Acc: 100.0\n",
            "Epoch: 4658 | Training Loss: 0.0004923750529997051 | Training Acc: 100.0 | Testing Loss: 0.0011613250244408846 | Testing Acc: 100.0\n",
            "Epoch: 4659 | Training Loss: 0.0004922330845147371 | Training Acc: 100.0 | Testing Loss: 0.0011613015085458755 | Testing Acc: 100.0\n",
            "Epoch: 4660 | Training Loss: 0.0004920826177112758 | Training Acc: 100.0 | Testing Loss: 0.001160882064141333 | Testing Acc: 100.0\n",
            "Epoch: 4661 | Training Loss: 0.0004919347120448947 | Training Acc: 100.0 | Testing Loss: 0.0011608528438955545 | Testing Acc: 100.0\n",
            "Epoch: 4662 | Training Loss: 0.0004917945479974151 | Training Acc: 100.0 | Testing Loss: 0.0011604393366724253 | Testing Acc: 100.0\n",
            "Epoch: 4663 | Training Loss: 0.0004916422185488045 | Training Acc: 100.0 | Testing Loss: 0.0011604102328419685 | Testing Acc: 100.0\n",
            "Epoch: 4664 | Training Loss: 0.0004915020545013249 | Training Acc: 100.0 | Testing Loss: 0.0011599966092035174 | Testing Acc: 100.0\n",
            "Epoch: 4665 | Training Loss: 0.0004913556622341275 | Training Acc: 100.0 | Testing Loss: 0.001159594627097249 | Testing Acc: 100.0\n",
            "Epoch: 4666 | Training Loss: 0.0004912092117592692 | Training Acc: 100.0 | Testing Loss: 0.0011595654068514705 | Testing Acc: 100.0\n",
            "Epoch: 4667 | Training Loss: 0.000491066079121083 | Training Acc: 100.0 | Testing Loss: 0.0011591577203944325 | Testing Acc: 100.0\n",
            "Epoch: 4668 | Training Loss: 0.0004909167182631791 | Training Acc: 100.0 | Testing Loss: 0.0011591286165639758 | Testing Acc: 100.0\n",
            "Epoch: 4669 | Training Loss: 0.0004907720722258091 | Training Acc: 100.0 | Testing Loss: 0.0011587089393287897 | Testing Acc: 100.0\n",
            "Epoch: 4670 | Training Loss: 0.0004906257381662726 | Training Acc: 100.0 | Testing Loss: 0.00115830113645643 | Testing Acc: 100.0\n",
            "Epoch: 4671 | Training Loss: 0.0004904821398667991 | Training Acc: 100.0 | Testing Loss: 0.0011582777369767427 | Testing Acc: 100.0\n",
            "Epoch: 4672 | Training Loss: 0.0004903346416540444 | Training Acc: 100.0 | Testing Loss: 0.001157869934104383 | Testing Acc: 100.0\n",
            "Epoch: 4673 | Training Loss: 0.0004901881911791861 | Training Acc: 100.0 | Testing Loss: 0.0011578525882214308 | Testing Acc: 100.0\n",
            "Epoch: 4674 | Training Loss: 0.0004900406929664314 | Training Acc: 100.0 | Testing Loss: 0.0011574446689337492 | Testing Acc: 100.0\n",
            "Epoch: 4675 | Training Loss: 0.0004898956976830959 | Training Acc: 100.0 | Testing Loss: 0.0011570426868274808 | Testing Acc: 100.0\n",
            "Epoch: 4676 | Training Loss: 0.000489753729198128 | Training Acc: 100.0 | Testing Loss: 0.0011570134665817022 | Testing Acc: 100.0\n",
            "Epoch: 4677 | Training Loss: 0.0004896031459793448 | Training Acc: 100.0 | Testing Loss: 0.0011566114844754338 | Testing Acc: 100.0\n",
            "Epoch: 4678 | Training Loss: 0.0004894642042927444 | Training Acc: 100.0 | Testing Loss: 0.0011565822642296553 | Testing Acc: 100.0\n",
            "Epoch: 4679 | Training Loss: 0.0004893224686384201 | Training Acc: 100.0 | Testing Loss: 0.001156156649813056 | Testing Acc: 100.0\n",
            "Epoch: 4680 | Training Loss: 0.0004891716525889933 | Training Acc: 100.0 | Testing Loss: 0.001156121725216508 | Testing Acc: 100.0\n",
            "Epoch: 4681 | Training Loss: 0.0004890314303338528 | Training Acc: 100.0 | Testing Loss: 0.0011557082179933786 | Testing Acc: 100.0\n",
            "Epoch: 4682 | Training Loss: 0.0004888835828751326 | Training Acc: 100.0 | Testing Loss: 0.0011553235817700624 | Testing Acc: 100.0\n",
            "Epoch: 4683 | Training Loss: 0.000488738645799458 | Training Acc: 100.0 | Testing Loss: 0.0011552886571735144 | Testing Acc: 100.0\n",
            "Epoch: 4684 | Training Loss: 0.000488594057969749 | Training Acc: 100.0 | Testing Loss: 0.0011548807378858328 | Testing Acc: 100.0\n",
            "Epoch: 4685 | Training Loss: 0.0004884476074948907 | Training Acc: 100.0 | Testing Loss: 0.001154851634055376 | Testing Acc: 100.0\n",
            "Epoch: 4686 | Training Loss: 0.0004883044166490436 | Training Acc: 100.0 | Testing Loss: 0.0011544439475983381 | Testing Acc: 100.0\n",
            "Epoch: 4687 | Training Loss: 0.0004881549975834787 | Training Acc: 100.0 | Testing Loss: 0.001154035795480013 | Testing Acc: 100.0\n",
            "Epoch: 4688 | Training Loss: 0.00048801893717609346 | Training Acc: 100.0 | Testing Loss: 0.0011540299747139215 | Testing Acc: 100.0\n",
            "Epoch: 4689 | Training Loss: 0.0004878714680671692 | Training Acc: 100.0 | Testing Loss: 0.0011536164674907923 | Testing Acc: 100.0\n",
            "Epoch: 4690 | Training Loss: 0.0004877250175923109 | Training Acc: 100.0 | Testing Loss: 0.0011535872472450137 | Testing Acc: 100.0\n",
            "Epoch: 4691 | Training Loss: 0.00048758479533717036 | Training Acc: 100.0 | Testing Loss: 0.0011531852651387453 | Testing Acc: 100.0\n",
            "Epoch: 4692 | Training Loss: 0.0004874369187746197 | Training Acc: 100.0 | Testing Loss: 0.001152783166617155 | Testing Acc: 100.0\n",
            "Epoch: 4693 | Training Loss: 0.0004872978606726974 | Training Acc: 100.0 | Testing Loss: 0.001152759650722146 | Testing Acc: 100.0\n",
            "Epoch: 4694 | Training Loss: 0.0004871517885476351 | Training Acc: 100.0 | Testing Loss: 0.0011523519642651081 | Testing Acc: 100.0\n",
            "Epoch: 4695 | Training Loss: 0.0004870083066634834 | Training Acc: 100.0 | Testing Loss: 0.0011523286812007427 | Testing Acc: 100.0\n",
            "Epoch: 4696 | Training Loss: 0.00048686511581763625 | Training Acc: 100.0 | Testing Loss: 0.0011519265826791525 | Testing Acc: 100.0\n",
            "Epoch: 4697 | Training Loss: 0.00048672169214114547 | Training Acc: 100.0 | Testing Loss: 0.001151897362433374 | Testing Acc: 100.0\n",
            "Epoch: 4698 | Training Loss: 0.00048657861771062016 | Training Acc: 100.0 | Testing Loss: 0.0011514895595610142 | Testing Acc: 100.0\n",
            "Epoch: 4699 | Training Loss: 0.00048643359332345426 | Training Acc: 100.0 | Testing Loss: 0.0011510816402733326 | Testing Acc: 100.0\n",
            "Epoch: 4700 | Training Loss: 0.0004862900823354721 | Training Acc: 100.0 | Testing Loss: 0.0011510583572089672 | Testing Acc: 100.0\n",
            "Epoch: 4701 | Training Loss: 0.00048614543629810214 | Training Acc: 100.0 | Testing Loss: 0.0011506503215059638 | Testing Acc: 100.0\n",
            "Epoch: 4702 | Training Loss: 0.00048600201262161136 | Training Acc: 100.0 | Testing Loss: 0.001150621334090829 | Testing Acc: 100.0\n",
            "Epoch: 4703 | Training Loss: 0.00048586330376565456 | Training Acc: 100.0 | Testing Loss: 0.001150201540440321 | Testing Acc: 100.0\n",
            "Epoch: 4704 | Training Loss: 0.000485715368995443 | Training Acc: 100.0 | Testing Loss: 0.001149805379100144 | Testing Acc: 100.0\n",
            "Epoch: 4705 | Training Loss: 0.0004855733423028141 | Training Acc: 100.0 | Testing Loss: 0.0011497937375679612 | Testing Acc: 100.0\n",
            "Epoch: 4706 | Training Loss: 0.00048542729928158224 | Training Acc: 100.0 | Testing Loss: 0.0011493915226310492 | Testing Acc: 100.0\n",
            "Epoch: 4707 | Training Loss: 0.0004852868150919676 | Training Acc: 100.0 | Testing Loss: 0.0011493624188005924 | Testing Acc: 100.0\n",
            "Epoch: 4708 | Training Loss: 0.0004851465637329966 | Training Acc: 100.0 | Testing Loss: 0.001148960436694324 | Testing Acc: 100.0\n",
            "Epoch: 4709 | Training Loss: 0.00048499865806661546 | Training Acc: 100.0 | Testing Loss: 0.0011485700961202383 | Testing Acc: 100.0\n",
            "Epoch: 4710 | Training Loss: 0.000484861055156216 | Training Acc: 100.0 | Testing Loss: 0.001148546813055873 | Testing Acc: 100.0\n",
            "Epoch: 4711 | Training Loss: 0.0004847165255341679 | Training Acc: 100.0 | Testing Loss: 0.0011481445981189609 | Testing Acc: 100.0\n",
            "Epoch: 4712 | Training Loss: 0.0004845714138355106 | Training Acc: 100.0 | Testing Loss: 0.0011481154942885041 | Testing Acc: 100.0\n",
            "Epoch: 4713 | Training Loss: 0.0004844282811973244 | Training Acc: 100.0 | Testing Loss: 0.0011477135121822357 | Testing Acc: 100.0\n",
            "Epoch: 4714 | Training Loss: 0.0004842863418161869 | Training Acc: 100.0 | Testing Loss: 0.0011476960498839617 | Testing Acc: 100.0\n",
            "Epoch: 4715 | Training Loss: 0.0004841461777687073 | Training Acc: 100.0 | Testing Loss: 0.0011472762562334538 | Testing Acc: 100.0\n",
            "Epoch: 4716 | Training Loss: 0.0004840041510760784 | Training Acc: 100.0 | Testing Loss: 0.0011468857992440462 | Testing Acc: 100.0\n",
            "Epoch: 4717 | Training Loss: 0.00048386064008809626 | Training Acc: 100.0 | Testing Loss: 0.0011468625161796808 | Testing Acc: 100.0\n",
            "Epoch: 4718 | Training Loss: 0.0004837175365537405 | Training Acc: 100.0 | Testing Loss: 0.0011464545968919992 | Testing Acc: 100.0\n",
            "Epoch: 4719 | Training Loss: 0.00048357556806877255 | Training Acc: 100.0 | Testing Loss: 0.0011464195558801293 | Testing Acc: 100.0\n",
            "Epoch: 4720 | Training Loss: 0.0004834338906221092 | Training Acc: 100.0 | Testing Loss: 0.001146011520177126 | Testing Acc: 100.0\n",
            "Epoch: 4721 | Training Loss: 0.000483290379634127 | Training Acc: 100.0 | Testing Loss: 0.0011456210631877184 | Testing Acc: 100.0\n",
            "Epoch: 4722 | Training Loss: 0.0004831512924283743 | Training Acc: 100.0 | Testing Loss: 0.0011456153588369489 | Testing Acc: 100.0\n",
            "Epoch: 4723 | Training Loss: 0.00048300973139703274 | Training Acc: 100.0 | Testing Loss: 0.0011452015023678541 | Testing Acc: 100.0\n",
            "Epoch: 4724 | Training Loss: 0.0004828647361136973 | Training Acc: 100.0 | Testing Loss: 0.0011451721657067537 | Testing Acc: 100.0\n",
            "Epoch: 4725 | Training Loss: 0.00048272599815391004 | Training Acc: 100.0 | Testing Loss: 0.0011447642464190722 | Testing Acc: 100.0\n",
            "Epoch: 4726 | Training Loss: 0.00048257954767905176 | Training Acc: 100.0 | Testing Loss: 0.0011447350261732936 | Testing Acc: 100.0\n",
            "Epoch: 4727 | Training Loss: 0.00048244083882309496 | Training Acc: 100.0 | Testing Loss: 0.0011443212861195207 | Testing Acc: 100.0\n",
            "Epoch: 4728 | Training Loss: 0.0004822943883482367 | Training Acc: 100.0 | Testing Loss: 0.0011439251247793436 | Testing Acc: 100.0\n",
            "Epoch: 4729 | Training Loss: 0.00048215678543783724 | Training Acc: 100.0 | Testing Loss: 0.0011438957881182432 | Testing Acc: 100.0\n",
            "Epoch: 4730 | Training Loss: 0.00048201362369582057 | Training Acc: 100.0 | Testing Loss: 0.0011434995103627443 | Testing Acc: 100.0\n",
            "Epoch: 4731 | Training Loss: 0.0004818701127078384 | Training Acc: 100.0 | Testing Loss: 0.001143476227298379 | Testing Acc: 100.0\n",
            "Epoch: 4732 | Training Loss: 0.00048173294635489583 | Training Acc: 100.0 | Testing Loss: 0.00114307994954288 | Testing Acc: 100.0\n",
            "Epoch: 4733 | Training Loss: 0.0004815879219677299 | Training Acc: 100.0 | Testing Loss: 0.001142683788202703 | Testing Acc: 100.0\n",
            "Epoch: 4734 | Training Loss: 0.0004814503190573305 | Training Acc: 100.0 | Testing Loss: 0.0011426545679569244 | Testing Acc: 100.0\n",
            "Epoch: 4735 | Training Loss: 0.0004813131527043879 | Training Acc: 100.0 | Testing Loss: 0.0011422523530200124 | Testing Acc: 100.0\n",
            "Epoch: 4736 | Training Loss: 0.00048116518883034587 | Training Acc: 100.0 | Testing Loss: 0.0011422231327742338 | Testing Acc: 100.0\n",
            "Epoch: 4737 | Training Loss: 0.0004810309037566185 | Training Acc: 100.0 | Testing Loss: 0.0011418269714340568 | Testing Acc: 100.0\n",
            "Epoch: 4738 | Training Loss: 0.0004808844532817602 | Training Acc: 100.0 | Testing Loss: 0.0011414539767429233 | Testing Acc: 100.0\n",
            "Epoch: 4739 | Training Loss: 0.00048075136146508157 | Training Acc: 100.0 | Testing Loss: 0.0011414188193157315 | Testing Acc: 100.0\n",
            "Epoch: 4740 | Training Loss: 0.00048060822882689536 | Training Acc: 100.0 | Testing Loss: 0.0011410166043788195 | Testing Acc: 100.0\n",
            "Epoch: 4741 | Training Loss: 0.0004804691416211426 | Training Acc: 100.0 | Testing Loss: 0.001140993321314454 | Testing Acc: 100.0\n",
            "Epoch: 4742 | Training Loss: 0.00048032746417447925 | Training Acc: 100.0 | Testing Loss: 0.0011405970435589552 | Testing Acc: 100.0\n",
            "Epoch: 4743 | Training Loss: 0.0004801839531864971 | Training Acc: 100.0 | Testing Loss: 0.0011405679397284985 | Testing Acc: 100.0\n",
            "Epoch: 4744 | Training Loss: 0.000480043760035187 | Training Acc: 100.0 | Testing Loss: 0.0011401657247915864 | Testing Acc: 100.0\n",
            "Epoch: 4745 | Training Loss: 0.00047990019083954394 | Training Acc: 100.0 | Testing Loss: 0.0011397752678021789 | Testing Acc: 100.0\n",
            "Epoch: 4746 | Training Loss: 0.000479765614727512 | Training Acc: 100.0 | Testing Loss: 0.0011397636262699962 | Testing Acc: 100.0\n",
            "Epoch: 4747 | Training Loss: 0.00047961948439478874 | Training Acc: 100.0 | Testing Loss: 0.0011393673485144973 | Testing Acc: 100.0\n",
            "Epoch: 4748 | Training Loss: 0.0004794819396920502 | Training Acc: 100.0 | Testing Loss: 0.0011393323075026274 | Testing Acc: 100.0\n",
            "Epoch: 4749 | Training Loss: 0.00047934771282598376 | Training Acc: 100.0 | Testing Loss: 0.0011389360297471285 | Testing Acc: 100.0\n",
            "Epoch: 4750 | Training Loss: 0.00047920565702952445 | Training Acc: 100.0 | Testing Loss: 0.0011385513935238123 | Testing Acc: 100.0\n",
            "Epoch: 4751 | Training Loss: 0.00047906357212923467 | Training Acc: 100.0 | Testing Loss: 0.0011385222896933556 | Testing Acc: 100.0\n",
            "Epoch: 4752 | Training Loss: 0.00047892489237710834 | Training Acc: 100.0 | Testing Loss: 0.0011381140211597085 | Testing Acc: 100.0\n",
            "Epoch: 4753 | Training Loss: 0.0004787843790836632 | Training Acc: 100.0 | Testing Loss: 0.00113808480091393 | Testing Acc: 100.0\n",
            "Epoch: 4754 | Training Loss: 0.0004786426143255085 | Training Acc: 100.0 | Testing Loss: 0.001137688523158431 | Testing Acc: 100.0\n",
            "Epoch: 4755 | Training Loss: 0.0004785020719282329 | Training Acc: 100.0 | Testing Loss: 0.0011373155284672976 | Testing Acc: 100.0\n",
            "Epoch: 4756 | Training Loss: 0.0004783645272254944 | Training Acc: 100.0 | Testing Loss: 0.0011372922454029322 | Testing Acc: 100.0\n",
            "Epoch: 4757 | Training Loss: 0.0004782198811881244 | Training Acc: 100.0 | Testing Loss: 0.001136907609179616 | Testing Acc: 100.0\n",
            "Epoch: 4758 | Training Loss: 0.000478082278277725 | Training Acc: 100.0 | Testing Loss: 0.0011368724517524242 | Testing Acc: 100.0\n",
            "Epoch: 4759 | Training Loss: 0.0004779465380124748 | Training Acc: 100.0 | Testing Loss: 0.0011364644160494208 | Testing Acc: 100.0\n",
            "Epoch: 4760 | Training Loss: 0.0004778030270244926 | Training Acc: 100.0 | Testing Loss: 0.001136435312218964 | Testing Acc: 100.0\n",
            "Epoch: 4761 | Training Loss: 0.00047766874195076525 | Training Acc: 100.0 | Testing Loss: 0.0011360390344634652 | Testing Acc: 100.0\n",
            "Epoch: 4762 | Training Loss: 0.0004775252309627831 | Training Acc: 100.0 | Testing Loss: 0.0011356541654095054 | Testing Acc: 100.0\n",
            "Epoch: 4763 | Training Loss: 0.00047739208093844354 | Training Acc: 100.0 | Testing Loss: 0.0011356368195265532 | Testing Acc: 100.0\n",
            "Epoch: 4764 | Training Loss: 0.00047725049080327153 | Training Acc: 100.0 | Testing Loss: 0.0011352462461218238 | Testing Acc: 100.0\n",
            "Epoch: 4765 | Training Loss: 0.00047710692160762846 | Training Acc: 100.0 | Testing Loss: 0.0011352113215252757 | Testing Acc: 100.0\n",
            "Epoch: 4766 | Training Loss: 0.00047697266563773155 | Training Acc: 100.0 | Testing Loss: 0.0011348266853019595 | Testing Acc: 100.0\n",
            "Epoch: 4767 | Training Loss: 0.0004768291546497494 | Training Acc: 100.0 | Testing Loss: 0.0011344302911311388 | Testing Acc: 100.0\n",
            "Epoch: 4768 | Training Loss: 0.0004766974598169327 | Training Acc: 100.0 | Testing Loss: 0.0011344009544700384 | Testing Acc: 100.0\n",
            "Epoch: 4769 | Training Loss: 0.00047655878006480634 | Training Acc: 100.0 | Testing Loss: 0.0011340104974806309 | Testing Acc: 100.0\n",
            "Epoch: 4770 | Training Loss: 0.0004764196928590536 | Training Acc: 100.0 | Testing Loss: 0.0011339872144162655 | Testing Acc: 100.0\n",
            "Epoch: 4771 | Training Loss: 0.0004762824100907892 | Training Acc: 100.0 | Testing Loss: 0.0011335849994793534 | Testing Acc: 100.0\n",
            "Epoch: 4772 | Training Loss: 0.00047614038339816034 | Training Acc: 100.0 | Testing Loss: 0.0011335557792335749 | Testing Acc: 100.0\n",
            "Epoch: 4773 | Training Loss: 0.00047600758261978626 | Training Acc: 100.0 | Testing Loss: 0.0011331652058288455 | Testing Acc: 100.0\n",
            "Epoch: 4774 | Training Loss: 0.0004758625873364508 | Training Acc: 100.0 | Testing Loss: 0.0011327805696055293 | Testing Acc: 100.0\n",
            "Epoch: 4775 | Training Loss: 0.00047572943731211126 | Training Acc: 100.0 | Testing Loss: 0.001132757170125842 | Testing Acc: 100.0\n",
            "Epoch: 4776 | Training Loss: 0.00047559518134221435 | Training Acc: 100.0 | Testing Loss: 0.0011323668295517564 | Testing Acc: 100.0\n",
            "Epoch: 4777 | Training Loss: 0.00047545312554575503 | Training Acc: 100.0 | Testing Loss: 0.0011323199141770601 | Testing Acc: 100.0\n",
            "Epoch: 4778 | Training Loss: 0.0004753158427774906 | Training Acc: 100.0 | Testing Loss: 0.001131935277953744 | Testing Acc: 100.0\n",
            "Epoch: 4779 | Training Loss: 0.0004751782398670912 | Training Acc: 100.0 | Testing Loss: 0.0011315622832626104 | Testing Acc: 100.0\n",
            "Epoch: 4780 | Training Loss: 0.0004750422085635364 | Training Acc: 100.0 | Testing Loss: 0.00113153294660151 | Testing Acc: 100.0\n",
            "Epoch: 4781 | Training Loss: 0.0004749034414999187 | Training Acc: 100.0 | Testing Loss: 0.0011311424896121025 | Testing Acc: 100.0\n",
            "Epoch: 4782 | Training Loss: 0.0004747644125018269 | Training Acc: 100.0 | Testing Loss: 0.0011311131529510021 | Testing Acc: 100.0\n",
            "Epoch: 4783 | Training Loss: 0.0004746300692204386 | Training Acc: 100.0 | Testing Loss: 0.0011307226959615946 | Testing Acc: 100.0\n",
            "Epoch: 4784 | Training Loss: 0.0004744820762425661 | Training Acc: 100.0 | Testing Loss: 0.0011303380597382784 | Testing Acc: 100.0\n",
            "Epoch: 4785 | Training Loss: 0.00047435337910428643 | Training Acc: 100.0 | Testing Loss: 0.0011303146602585912 | Testing Acc: 100.0\n",
            "Epoch: 4786 | Training Loss: 0.0004742146411444992 | Training Acc: 100.0 | Testing Loss: 0.0011299181496724486 | Testing Acc: 100.0\n",
            "Epoch: 4787 | Training Loss: 0.00047408006503246725 | Training Acc: 100.0 | Testing Loss: 0.0011299123289063573 | Testing Acc: 100.0\n",
            "Epoch: 4788 | Training Loss: 0.0004739412688650191 | Training Acc: 100.0 | Testing Loss: 0.0011295159347355366 | Testing Acc: 100.0\n",
            "Epoch: 4789 | Training Loss: 0.0004737992712762207 | Training Acc: 100.0 | Testing Loss: 0.0011294868309050798 | Testing Acc: 100.0\n",
            "Epoch: 4790 | Training Loss: 0.0004736664122901857 | Training Acc: 100.0 | Testing Loss: 0.001129090553149581 | Testing Acc: 100.0\n",
            "Epoch: 4791 | Training Loss: 0.0004735258989967406 | Training Acc: 100.0 | Testing Loss: 0.0011287056840956211 | Testing Acc: 100.0\n",
            "Epoch: 4792 | Training Loss: 0.00047339420416392386 | Training Acc: 100.0 | Testing Loss: 0.0011286824010312557 | Testing Acc: 100.0\n",
            "Epoch: 4793 | Training Loss: 0.0004732539819087833 | Training Acc: 100.0 | Testing Loss: 0.0011282919440418482 | Testing Acc: 100.0\n",
            "Epoch: 4794 | Training Loss: 0.00047311195521615446 | Training Acc: 100.0 | Testing Loss: 0.0011282627237960696 | Testing Acc: 100.0\n",
            "Epoch: 4795 | Training Loss: 0.0004729806096293032 | Training Acc: 100.0 | Testing Loss: 0.001127872266806662 | Testing Acc: 100.0\n",
            "Epoch: 4796 | Training Loss: 0.0004728430067189038 | Training Acc: 100.0 | Testing Loss: 0.001127499039284885 | Testing Acc: 100.0\n",
            "Epoch: 4797 | Training Loss: 0.0004727069172076881 | Training Acc: 100.0 | Testing Loss: 0.0011274699354544282 | Testing Acc: 100.0\n",
            "Epoch: 4798 | Training Loss: 0.0004725681501440704 | Training Acc: 100.0 | Testing Loss: 0.0011270851828157902 | Testing Acc: 100.0\n",
            "Epoch: 4799 | Training Loss: 0.00047243060544133186 | Training Acc: 100.0 | Testing Loss: 0.0011270618997514248 | Testing Acc: 100.0\n",
            "Epoch: 4800 | Training Loss: 0.0004722947196569294 | Training Acc: 100.0 | Testing Loss: 0.0011266771471127868 | Testing Acc: 100.0\n",
            "Epoch: 4801 | Training Loss: 0.00047215865924954414 | Training Acc: 100.0 | Testing Loss: 0.001126274699345231 | Testing Acc: 100.0\n",
            "Epoch: 4802 | Training Loss: 0.00047202990390360355 | Training Acc: 100.0 | Testing Loss: 0.0011262571206316352 | Testing Acc: 100.0\n",
            "Epoch: 4803 | Training Loss: 0.0004718912241514772 | Training Acc: 100.0 | Testing Loss: 0.001125866430811584 | Testing Acc: 100.0\n",
            "Epoch: 4804 | Training Loss: 0.0004717490519396961 | Training Acc: 100.0 | Testing Loss: 0.0011258490849286318 | Testing Acc: 100.0\n",
            "Epoch: 4805 | Training Loss: 0.0004716163093689829 | Training Acc: 100.0 | Testing Loss: 0.0011254643322899938 | Testing Acc: 100.0\n",
            "Epoch: 4806 | Training Loss: 0.000471481733256951 | Training Acc: 100.0 | Testing Loss: 0.0011254233540967107 | Testing Acc: 100.0\n",
            "Epoch: 4807 | Training Loss: 0.000471348874270916 | Training Acc: 100.0 | Testing Loss: 0.0011250327806919813 | Testing Acc: 100.0\n",
            "Epoch: 4808 | Training Loss: 0.00047121127136051655 | Training Acc: 100.0 | Testing Loss: 0.0011246539652347565 | Testing Acc: 100.0\n",
            "Epoch: 4809 | Training Loss: 0.00047107660793699324 | Training Acc: 100.0 | Testing Loss: 0.0011246365029364824 | Testing Acc: 100.0\n",
            "Epoch: 4810 | Training Loss: 0.0004709422937594354 | Training Acc: 100.0 | Testing Loss: 0.0011242399923503399 | Testing Acc: 100.0\n",
            "Epoch: 4811 | Training Loss: 0.00047080026706680655 | Training Acc: 100.0 | Testing Loss: 0.0011242108885198832 | Testing Acc: 100.0\n",
            "Epoch: 4812 | Training Loss: 0.0004706718900706619 | Training Acc: 100.0 | Testing Loss: 0.0011238261358812451 | Testing Acc: 100.0\n",
            "Epoch: 4813 | Training Loss: 0.00047052977606654167 | Training Acc: 100.0 | Testing Loss: 0.0011234646663069725 | Testing Acc: 100.0\n",
            "Epoch: 4814 | Training Loss: 0.00047039519995450974 | Training Acc: 100.0 | Testing Loss: 0.0011234413832426071 | Testing Acc: 100.0\n",
            "Epoch: 4815 | Training Loss: 0.00047025643289089203 | Training Acc: 100.0 | Testing Loss: 0.0011230565141886473 | Testing Acc: 100.0\n",
            "Epoch: 4816 | Training Loss: 0.00047012028517201543 | Training Acc: 100.0 | Testing Loss: 0.0011230214731767774 | Testing Acc: 100.0\n",
            "Epoch: 4817 | Training Loss: 0.00046999036567285657 | Training Acc: 100.0 | Testing Loss: 0.0011226367205381393 | Testing Acc: 100.0\n",
            "Epoch: 4818 | Training Loss: 0.00046984985237941146 | Training Acc: 100.0 | Testing Loss: 0.0011222576722502708 | Testing Acc: 100.0\n",
            "Epoch: 4819 | Training Loss: 0.00046972109703347087 | Training Acc: 100.0 | Testing Loss: 0.0011222402099519968 | Testing Acc: 100.0\n",
            "Epoch: 4820 | Training Loss: 0.0004695794195868075 | Training Acc: 100.0 | Testing Loss: 0.001121855340898037 | Testing Acc: 100.0\n",
            "Epoch: 4821 | Training Loss: 0.0004694521485362202 | Training Acc: 100.0 | Testing Loss: 0.0011218376457691193 | Testing Acc: 100.0\n",
            "Epoch: 4822 | Training Loss: 0.0004693193768616766 | Training Acc: 100.0 | Testing Loss: 0.001121446955949068 | Testing Acc: 100.0\n",
            "Epoch: 4823 | Training Loss: 0.0004691758658736944 | Training Acc: 100.0 | Testing Loss: 0.0011214178521186113 | Testing Acc: 100.0\n",
            "Epoch: 4824 | Training Loss: 0.00046904446207918227 | Training Acc: 100.0 | Testing Loss: 0.001121027278713882 | Testing Acc: 100.0\n",
            "Epoch: 4825 | Training Loss: 0.0004689038614742458 | Training Acc: 100.0 | Testing Loss: 0.0011206424096599221 | Testing Acc: 100.0\n",
            "Epoch: 4826 | Training Loss: 0.0004687751643359661 | Training Acc: 100.0 | Testing Loss: 0.0011206191265955567 | Testing Acc: 100.0\n",
            "Epoch: 4827 | Training Loss: 0.00046864236355759203 | Training Acc: 100.0 | Testing Loss: 0.00112024019472301 | Testing Acc: 100.0\n",
            "Epoch: 4828 | Training Loss: 0.0004685062449425459 | Training Acc: 100.0 | Testing Loss: 0.001120222732424736 | Testing Acc: 100.0\n",
            "Epoch: 4829 | Training Loss: 0.00046837786794640124 | Training Acc: 100.0 | Testing Loss: 0.001119831926189363 | Testing Acc: 100.0\n",
            "Epoch: 4830 | Training Loss: 0.0004682416911236942 | Training Acc: 100.0 | Testing Loss: 0.0011194528779014945 | Testing Acc: 100.0\n",
            "Epoch: 4831 | Training Loss: 0.0004681100253947079 | Training Acc: 100.0 | Testing Loss: 0.0011194295948371291 | Testing Acc: 100.0\n",
            "Epoch: 4832 | Training Loss: 0.00046796974493190646 | Training Acc: 100.0 | Testing Loss: 0.0011190506629645824 | Testing Acc: 100.0\n",
            "Epoch: 4833 | Training Loss: 0.00046783816651441157 | Training Acc: 100.0 | Testing Loss: 0.001119027379900217 | Testing Acc: 100.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-181309b577ac>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtest_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0my_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mtesting_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1180\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3053\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3055\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Imm6CG9IRWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check on one manual example"
      ],
      "metadata": {
        "id": "OoIq6quoISE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"My output class is {torch.softmax(model(torch.tensor([[1.0, 2.0]]).to(device)), dim=1).argmax(dim=1).cpu().item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFCDD2HeIUDL",
        "outputId": "ca791ac5-06a0-4c82-bdc2-acb72fd4d92e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My output class is 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Pk9yrAYwIPmr"
      }
    }
  ]
}